{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday, January 25, 2024\n",
    "\n",
    "Trying this again ... but on a newer container.\n",
    "\n",
    "docker container start hfpt_Dec14\n",
    "\n",
    "OK Nice! This still all runs!\n",
    "\n",
    "### Thursday, December 7, 2023\n",
    "\n",
    "Gonna give this another go ... \n",
    "\n",
    "Ok. I was able to download the model, and get this notebook to run. Nice!\n",
    "\n",
    "### Saturday, December 2, 2023\n",
    "\n",
    "https://huggingface.co/Intel/neural-chat-7b-v3-1\n",
    "\n",
    "docker container start hfpt_Oct28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--NousResearch--Llama-2-7b-chat-hf\n",
      "models--bert-base-uncased\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--ehartford--samantha-mistral-7b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mlabonne--NeuralBeagle14-7B\n",
      "models--sentence-transformers--msmarco-MiniLM-L-12-v3\n",
      "models--teknium--OpenHermes-2.5-Mistral-7B\n",
      "models--unsloth--mistral-7b-bnb-4bit\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers\n",
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up 'Intel/neural-chat-7b-v3-1'\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--Intel--neural-chat-7b-v3-1  c8324b70601d://root/.cache/huggingface/hub\n",
    "# Successfully copied 14.5GB to c8324b70601d://root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Intel--neural-chat-7b-v3-1\n",
      "models--NousResearch--Llama-2-7b-chat-hf\n",
      "models--bert-base-uncased\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--ehartford--samantha-mistral-7b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mlabonne--NeuralBeagle14-7B\n",
      "models--sentence-transformers--msmarco-MiniLM-L-12-v3\n",
      "models--teknium--OpenHermes-2.5-Mistral-7B\n",
      "models--unsloth--mistral-7b-bnb-4bit\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backup 'Intel/neural-chat-7b-v3-1'\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--Intel--neural-chat-7b-v3-1 /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 14.5GB to /home/rob/Data3/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model_name = 'Intel/neural-chat-7b-v3-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.35.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.8/dist-packages\n",
      "Editable project location: /usr/local/lib/python3.8/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: auto-gptq, optimum, peft, sentence-transformers, trl\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc86e62235b4c43bbfcf9e2a016d3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0511d75b1194c6da0aee3d3ae297c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this loads it to CPU Ram, not GPU VRAM\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# this too loads it to CPU RAM, but then loads it to GPU VRAM 23990 MiB VRAM\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 8252 MiB ... wayy smaller!\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 8252 MiB ... same as above.\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 5392 MiB ... even smaller!\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "\n",
    "# Time to download the model, then load it to CPU RAM\n",
    "# CPU times: user 1min 49s, sys: 1min 44s, total: 3min 33s\n",
    "# Wall time: 3h 22min 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_input, user_input):\n",
    "\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    return response.split(\"### Assistant:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To calculate the sum of 100, 520, and 60, we will follow these steps:\n",
      "\n",
      "1. Add the first two numbers: 100 + 520\n",
      "2. Add the result from step 1 to the third number: (result from step 1) + 60\n",
      "\n",
      "Step 1: Add 100 and 520\n",
      "100 + 520 = 620\n",
      "\n",
      "Step 2: Add the result from step 1 to the third number (60)\n",
      "(620) + 60 = 680\n",
      "\n",
      "So, the sum of 100, 520, and 60 is 680.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "system_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\n",
    "user_input = \"calculate 100 + 520 + 60\"\n",
    "response = generate_response(system_input, user_input)\n",
    "print(response)\n",
    "\n",
    "# 12238 MiB VRAM\n",
    "\n",
    "# 28.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected response\n",
    "\"\"\"\n",
    "To calculate the sum of 100, 520, and 60, we will follow these steps:\n",
    "\n",
    "1. Add the first two numbers: 100 + 520\n",
    "2. Add the result from step 1 to the third number: (100 + 520) + 60\n",
    "\n",
    "Step 1: Add 100 and 520\n",
    "100 + 520 = 620\n",
    "\n",
    "Step 2: Add the result from step 1 to the third number (60)\n",
    "(620) + 60 = 680\n",
    "\n",
    "So, the sum of 100, 520, and 60 is 680.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
