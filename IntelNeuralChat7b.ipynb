{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday, December 7, 2023\n",
    "\n",
    "Gonna give this another go ... \n",
    "\n",
    "Ok. I was able to download the model, and get this notebook to run. Nice!\n",
    "\n",
    "### Saturday, December 2, 2023\n",
    "\n",
    "https://huggingface.co/Intel/neural-chat-7b-v3-1\n",
    "\n",
    "docker container start hfpt_Oct28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Intel--neural-chat-7b-v3-1\n",
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--TheBloke--Python-Code-33B-GPTQ\n",
      "models--facebook--blenderbot-1B-distill\n",
      "models--google--flan-t5-large\n",
      "models--gpt2-medium\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--model_name\n",
      "tmp0m77pc0x\n",
      "tmpcjh0h7gn\n",
      "tmpfcrmwgx2\n",
      "tmpzafytbf_\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Backup 'Intel/neural-chat-7b-v3-1'\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--Intel--neural-chat-7b-v3-1 /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 14.5GB to /home/rob/Data3/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model_name = 'Intel/neural-chat-7b-v3-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.35.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.8/dist-packages\n",
      "Requires: requests, numpy, safetensors, tqdm, packaging, regex, filelock, tokenizers, pyyaml, huggingface-hub\n",
      "Required-by: trl, sentence-transformers, peft, optimum, auto-gptq\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafba8d283484296b55894661c8e391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.44 s, sys: 2.65 s, total: 11.1 s\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this loads it to CPU Ram, not GPU VRAM\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# this too loads it to CPU RAM, but then loads it to GPU VRAM 23990 MiB VRAM\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 8252 MiB ... wayy smaller!\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 8252 MiB ... same as above.\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "# This loads it right away to GPU VRAM, but takes up way less space 5392 MiB ... even smaller!\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "\n",
    "# Time to download the model, then load it to CPU RAM\n",
    "# CPU times: user 1min 49s, sys: 1min 44s, total: 3min 33s\n",
    "# Wall time: 3h 22min 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_input, user_input):\n",
    "\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the assistant's response\n",
    "    return response.split(\"### Assistant:\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/transformers/src/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To calculate the sum of 100, 520, and 60, we will follow these steps:\n",
      "\n",
      "1. Add the first two numbers: 100 + 520\n",
      "2. Add the result from step 1 to the third number: (100 + 520) + 60\n",
      "\n",
      "Step 1: Add 100 and 520\n",
      "100 + 520 = 620\n",
      "\n",
      "Step 2: Add the result from step 1 to the third number\n",
      "(620) + 60 = 680\n",
      "\n",
      "So, the sum of 100, 520, and 60 is 680.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "system_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\"\n",
    "user_input = \"calculate 100 + 520 + 60\"\n",
    "response = generate_response(system_input, user_input)\n",
    "print(response)\n",
    "\n",
    "# 28.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected response\n",
    "\"\"\"\n",
    "To calculate the sum of 100, 520, and 60, we will follow these steps:\n",
    "\n",
    "1. Add the first two numbers: 100 + 520\n",
    "2. Add the result from step 1 to the third number: (100 + 520) + 60\n",
    "\n",
    "Step 1: Add 100 and 520\n",
    "100 + 520 = 620\n",
    "\n",
    "Step 2: Add the result from step 1 to the third number (60)\n",
    "(620) + 60 = 680\n",
    "\n",
    "So, the sum of 100, 520, and 60 is 680.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
