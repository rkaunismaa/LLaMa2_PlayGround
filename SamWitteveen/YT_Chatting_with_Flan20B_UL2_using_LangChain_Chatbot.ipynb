{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Thursday, December 7, 2023\n",
        "\n",
        "[LangChain Chat with Flan20B](https://www.youtube.com/watch?v=VW5LBavIfY4&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ&index=5)\n",
        "\n",
        "https://colab.research.google.com/drive/138fnzBIH0RNb3W7QclqF1QLMx8QnzCQw?usp=sharing\n",
        "\n",
        "I am going to tweak this code to use a local model instead of the HuggingFaceHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UujGtM2CijN_"
      },
      "outputs": [],
      "source": [
        "#!pip -q install huggingface_hub langchain transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJn113iir0E",
        "outputId": "08583225-cb54-4f4d-b546-26ac4ac4a284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.102\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "#!pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOjEK4AZit_V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# enter your api key\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass(\"Enter your API key: \")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPkGW-opizlk"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sHRrNLci-JM"
      },
      "outputs": [],
      "source": [
        "flan_ul2 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-ul2\",\n",
        "    model_kwargs={\"temperature\":0.1,\n",
        "                  \"max_new_tokens\":256})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTlHXdT6M3Vc"
      },
      "outputs": [],
      "source": [
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":0 }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46qv-pgbjIha"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcSnmWV-q0EP"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we override what was done above and instead use a local model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COvHh7Egq8Yx"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=flan_ul2,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "j7wZs5yvq_1m",
        "outputId": "72703f92-e9c8-4900-d343-8d17a1cc05aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there! I am Sam\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hi Sam, how can I help you?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi there! I am Sam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "msknvw0RrPXo",
        "outputId": "fe1d2eda-a1f0-4761-a8a9-d2dfa01a303b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hi Sam, how can I help you?\n",
            "Human: How are you today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am good, thanks.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversation.predict(input=\"How are you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "0BzCW6dKrZ4U",
        "outputId": "5f05c959-e712-4d87-bc3d-130e25829f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hi Sam, how can I help you?\n",
            "Human: How are you today?\n",
            "AI: I am good, thanks.\n",
            "Human:  Can you help me with some customer support?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I can help you with that.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\" Can you help me with some customer support?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "JLldmw7hTLRD",
        "outputId": "2acf3868-5a62-4c04-e68d-98d05d8772f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hi Sam, how can I help you?\n",
            "Human: How are you today?\n",
            "AI: I am good, thanks.\n",
            "Human:  Can you help me with some customer support?\n",
            "AI: I can help you with that.\n",
            "Human: My TV is broken. can you help fix it?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I can help you with that.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"My TV is broken. can you help fix it?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eETT_I7GeH4f"
      },
      "source": [
        "## Counting the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRx6i3ZzPjZ0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODYI9vJbQClG",
        "outputId": "ae73feb2-dd5a-46e1-ebf7-5db8db1cb110"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['▁Now', '▁Good', '▁Morning', '▁M', 's', '▁Roger', 's']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_input = \"Now Good Morning Ms Rogers\"\n",
        "# tokenizer([test_input])\n",
        "tokenizer.tokenize(test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZR0IEP0aZ3hm",
        "outputId": "f53abd20-5883-48c4-ac4d-b018ddc9639c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nHuman: Hi there! I am Sam\\nAI: Hi Sam, how can I help you?\\nHuman: How are you today?\\nAI: I am good, thanks.\\nHuman:  Can you help me with some customer support?\\nAI: I can help you with that.\\nHuman: My TV is broken. can you help fix it?\\nAI: I can help you with that.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.memory.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "iwyUIWB7X1-u",
        "outputId": "725abc2a-547b-4981-9b13-2e21adb9fa46"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n\\nHuman: Hi there! I am Sam\\nAI: Hi Sam, how can I help you?\\nHuman: How are you today?\\nAI: I am good, thanks.\\nHuman:  Can you help me with some customer support?\\nAI: I can help you with that.\\nHuman: My TV is broken. can you help fix it?\\nAI: I can help you with that.\\nHuman: the next input\\nAI:'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_prompt = conversation.prompt.format(input='the next input',history=memory.buffer)\n",
        "formatted_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjyNnACEaVlj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkTdvaotNvxT"
      },
      "outputs": [],
      "source": [
        "def chat_to_llm(chat_llm ):\n",
        "    conversation_total_tokens = 0\n",
        "    new_conversation = ConversationChain(llm=chat_llm,\n",
        "                                     verbose=False,\n",
        "                                     memory=ConversationBufferMemory()\n",
        "                                     )\n",
        "\n",
        "    while True:\n",
        "        message = input(\"Human: \")\n",
        "        if message=='exit':\n",
        "            print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "            break\n",
        "        if message:\n",
        "            formatted_prompt = conversation.prompt.format(input=message,history=new_conversation.memory.buffer)\n",
        "            num_tokens = len(tokenizer.tokenize(formatted_prompt))\n",
        "            conversation_total_tokens += num_tokens\n",
        "            print(f'tokens sent {num_tokens}')\n",
        "            response = new_conversation.predict(input=message)\n",
        "            response_num_tokens = len(tokenizer.tokenize(response))\n",
        "            conversation_total_tokens += response_num_tokens\n",
        "            print(f\"LLM: {response}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFFJb2itVJfA",
        "outputId": "6c988c6b-1987-4cec-937d-251bfecb37e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Hi how are you today?\n",
            "tokens sent 65\n",
            "LLM: I am fine. How can I help you?\n",
            "Human: Are you male or female?\n",
            "tokens sent 85\n",
            "LLM: I am a female.\n",
            "Human: are you tall or short?\n",
            "tokens sent 101\n",
            "LLM: I am tall.\n",
            "Human: How tall are you?\n",
            "tokens sent 114\n",
            "LLM: I am 5'9\" tall.\n",
            "Human: exit\n",
            "393 tokens used in total in this conversation\n"
          ]
        }
      ],
      "source": [
        "chat_to_llm(flan_ul2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpqGZZ_sXDKP",
        "outputId": "85dcc5ab-a721-4f23-db63-5c45096d51e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: hi how are you?\n",
            "tokens sent 64\n",
            "LLM: I am good thanks.\n",
            "Human: What is your name?\n",
            "tokens sent 78\n",
            "LLM: I am a robot.\n",
            "Human: exit\n",
            "153 tokens used in total in this conversation\n"
          ]
        }
      ],
      "source": [
        "chat_to_llm(flan_t5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfEBOdjH0Kr_"
      },
      "source": [
        "## Using a Summary Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hGxeSLpzDWS"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Bfzaxe0QRT"
      },
      "outputs": [],
      "source": [
        "summary_memory = ConversationSummaryMemory(llm=flan_ul2)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=flan_ul2,\n",
        "    verbose=True,\n",
        "    memory=summary_memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jayeeP7x1MAx"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Hi there! I am Sam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yp2MxnP1RSc"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"How are you today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTzNUXDG1WQG"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"I am good. What is your name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRlmtK6wF_3I"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Thats cool can you help me with customer support?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S41tWBtuGLLt"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"What time is your shop open today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiiUx4DkGRHY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
