{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday, December 8, 2023\n",
    "\n",
    "[TheBloke/Orca-2-13B-AWQ](https://huggingface.co/TheBloke/Orca-2-13B-AWQ)\n",
    "\n",
    "docker container start hfpt_Nov20\n",
    "\n",
    "To load this model I had to run ... 'pip install autoawq'\n",
    "\n",
    "Hmm getting a ' libcudart.so.12: cannot open shared object file: No such file or directory\n",
    "' error ... reading a fix is to run ..\n",
    "\n",
    "pip uninstall torch torchvision torchaudio triton\n",
    "\n",
    "then run ...\n",
    "\n",
    "pip install torch torchvision torchaudio\n",
    "\n",
    "OK Nice! I am now able to step through this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.36.0.dev0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.8/dist-packages\n",
      "Requires: packaging, regex, huggingface-hub, tqdm, filelock, numpy, requests, tokenizers, pyyaml, safetensors\n",
      "Required-by: lm-eval, autoawq\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Salesforce--blip-image-captioning-large  version.txt\n",
      "models--TheBloke--Orca-2-13B-AWQ\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the \"TheBloke/Orca-2-13B-AWQ\" model\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--Orca-2-13B-AWQ 9ef7ecd7959f:/root/.cache/huggingface/hub\n",
    "# Successfully copied 7.25GB to 9ef7ecd7959f:/root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Salesforce--blip-image-captioning-large  version.txt\n",
      "models--TheBloke--Orca-2-13B-AWQ\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers\n",
    "!ls /root/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Orca-2-13B-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.07 s, sys: 1.31 s, total: 5.38 s\n",
      "Wall time: 4.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Nice! This loads to the GPU! 7530 MiB\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True, device_map=\"cuda:0\")\n",
    "\n",
    "# this too takes up CPU RAM\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False, revision=\"main\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False)\n",
    "\n",
    "# Download time ...\n",
    "# 103m 59.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the text streamer to stream output one token at a time\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This system message was copied from ... https://huggingface.co/microsoft/Orca-2-13b\n",
    "system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert prompt to tokens\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_params = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for artificial intelligence, which is the field of computer science that studies how to create machines or systems that can perform tasks that normally require human intelligence, such as reasoning, learning, decision making, perception, natural language processing, etc. AI can be divided into two main branches: machine learning and artificial neural networks. Machine learning is the process of designing algorithms and data structures that can learn from experience and improve their performance on a given task. Artificial neural networks are models inspired by the structure and function of biological neurons that can process and transmit information using mathematical operations. AI has many applications in various domains, such as healthcare, education, entertainment, security, finance, manufacturing, transportation, etc. AI can also pose ethical, social, and legal challenges that need to be addressed by researchers, policymakers, and society at large.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Generate streamed output, visible one token at a time\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    streamer=streamer,\n",
    "    **generation_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation without a streamer, which will include the prompt in the output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    **generation_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.generate output:  <s><|im_start|>system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me about AI<|im_end|>\n",
      "<|im_start|>assistant\n",
      "AI stands for artificial intelligence, which is the field of computer science that studies how to create machines or systems that can perform tasks that normally require human intelligence, such as reasoning, learning, decision making, perception, natural language processing, etc. AI can be divided into two main branches: narrow AI and general AI. Narrow AI refers to systems that can perform specific tasks or domains, such as face recognition, speech synthesis, chess playing, etc. General AI refers to systems that can exhibit human-like intelligence across a wide range of domains, such as understanding natural language, planning, creativity, etc. General AI is still a distant goal for most researchers and developers, and some argue that it may never be achievable.</s>\n"
     ]
    }
   ],
   "source": [
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"model.generate output: \", text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference is also possible via Transformers' pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    **generation_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline output:  <|im_start|>system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me about AI<|im_end|>\n",
      "<|im_start|>assistant\n",
      "AI stands for artificial intelligence, which is the field of computer science that studies how to create machines or systems that can perform tasks that normally require human intelligence, such as reasoning, learning, perception, decision making, and natural language processing.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "pipe_output = pipe(prompt_template)[0]['generated_text']\n",
    "print(\"pipeline output: \", pipe_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
