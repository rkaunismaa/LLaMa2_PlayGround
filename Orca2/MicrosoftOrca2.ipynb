{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monday, January 1, 2023\n",
    "\n",
    "[microsoft/Orca-2-13b](https://huggingface.co/microsoft/Orca-2-13b)\n",
    "\n",
    "docker container start hfpt_Dec14\n",
    "\n",
    "This all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--microsoft--Orca-2-13b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c8324b70601d://root/.cache/huggingface/hub/models--microsoft--Orca-2-13b /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 52.1GB to /home/rob/Data3/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    torch.set_default_device(\"cpu\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"microsoft/Orca-2-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694898e1fa654e1ca53a1fb291efb657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WOW! This actually loads!!\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(modelName, load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "# 43.8s\n",
    "# 13848 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = transformers.AutoModelForCausalLM.from_pretrained(modelName, device_map='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/issues/27132\n",
    "# please use the slow tokenizer since fast and slow tokenizer produces different tokens\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        modelName,\n",
    "        use_fast=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\"\n",
    "user_message = \"How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?\"\n",
    "\n",
    "prompt = f\"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(inputs[\"input_ids\"],)\n",
    "\n",
    "# 1m 47.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokenizer.batch_decode(output_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  <|im_start|> system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|im_end|> \n",
      " <|im_start|> user\n",
      "How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful? <|im_end|> \n",
      " <|im_start|> assistant\n",
      "There are different ways to determine if a restaurant is popular among locals or mainly attracts tourists, such as:\n",
      "\n",
      "- Looking at online reviews and ratings from different sources, such as TripAdvisor, Yelp, Google, etc. If a restaurant has mostly positive reviews from locals and a high rating, it might indicate that it is popular among locals. However, if a restaurant has mostly negative reviews from locals and a low rating, it might indicate that it is not popular among locals. If a restaurant has mostly positive reviews from tourists and a high rating, it might indicate that it is mainly attracts tourists. However, if a restaurant has mostly negative reviews from tourists and a low rating, it might indicate that it is not popular among tourists.\n",
      "- Asking locals for recommendations or advice. Locals might have a better sense of which restaurants are popular and which ones are not, based on their personal preferences and experiences. They might also be able to tell you which restaurants are crowded and which ones are quiet, which might reflect the ratio of locals and tourists.\n",
      "- Observing the atmosphere and the clientele of the restaurant. A restaurant that is popular among locals might have a more relaxed and casual atmosphere, a diverse and mixed crowd, and a menu that reflects the local cuisine and culture. A restaurant that is mainly attracts tourists might have a more formal and fancy atmosphere, a homogeneous and uniform crowd, and a menu that caters to the tourists' expectations and preferences.\n",
      "\n",
      "This information might be useful for you to decide where to eat, depending on your own preferences and goals. For example, if you want to have a authentic and local dining experience, you might prefer a restaurant that is popular among locals. If you want to have a convenient and comfortable dining experience, you might prefer a restaurant that is mainly attracts tourists. If you want to have a mix of both, you might look for a restaurant that has a balance of locals and tourists.</s>\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  <|im_start|> system\n",
      "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|im_end|> \n",
      " <|im_start|> user\n",
      "How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful? <|im_end|> \n",
      " <|im_start|> assistant\n",
      "There are different ways to determine if a restaurant is popular among locals or mainly attracts tourists, such as:\n",
      "\n",
      "- Looking at online reviews and ratings from different sources, such as TripAdvisor, Yelp, Google, etc. If a restaurant has mostly positive reviews from locals and a high rating, it might indicate that it is popular among locals. However, if a restaurant has mostly negative reviews from locals and a low rating, it might indicate that it is not popular among locals. If a restaurant has mostly positive reviews from tourists and a high rating, it might indicate that it is mainly attracts tourists. However, if a restaurant has mostly negative reviews from tourists and a low rating, it might indicate that it is not popular among tourists.\n",
      "- Asking locals for recommendations or advice. Locals might have a better sense of which restaurants are popular and which ones are not, based on their personal preferences and experiences. They might also be able to tell you which restaurants are crowded and which ones are quiet, which might reflect the ratio of locals and tourists.\n",
      "- Observing the atmosphere and the clientele of the restaurant. A restaurant that is popular among locals might have a more relaxed and casual atmosphere, a diverse and mixed crowd, and a menu that reflects the local cuisine and culture. A restaurant that is mainly attracts tourists might have a more formal and fancy atmosphere, a homogeneous and uniform crowd, and a menu that caters to the tourists' expectations and preferences.\n",
      "\n",
      "This information might be useful for you to decide where to eat, depending on your own preferences and goals. For example, if you want to have a authentic and local dining experience, you might prefer a restaurant that is popular among locals. If you want to have a convenient and comfortable dining experience, you might prefer a restaurant that is mainly attracts tourists. If you want to have a mix of both, you might look for a restaurant that has a balance of locals and tourists.</s> \n",
      " <|im_start|> user\n",
      "Give me a list of the key points of your first answer. <|im_end|> \n",
      " <|im_start|> assistant\n",
      "Key points:\n",
      "\n",
      "- Online reviews and ratings can indicate the popularity and quality of a restaurant among locals and tourists\n",
      "- Asking locals for recommendations or advice can give you a more personal and reliable perspective on the restaurant scene\n",
      "- Observing the atmosphere and the clientele of the restaurant can show you the character and the target audience of the restaurant</s>\n"
     ]
    }
   ],
   "source": [
    "# This example continues showing how to add a second turn message by the user to the conversation\n",
    "second_turn_user_message = \"Give me a list of the key points of your first answer.\"\n",
    "\n",
    "# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\n",
    "second_turn_message_in_markup = f\"\\n<|im_start|>user\\n{second_turn_user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "second_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors='pt', add_special_tokens=False)\n",
    "second_turn_input = torch.cat([output_ids, second_turn_tokens['input_ids']], dim=1)\n",
    "\n",
    "output_ids_2 = model.generate(second_turn_input,)\n",
    "second_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n",
    "\n",
    "print(second_turn_answer)\n",
    "\n",
    "# 21.0s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
