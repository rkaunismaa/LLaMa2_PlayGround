{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday, December 8, 2023\n",
    "\n",
    "[TheBloke/Orca-2-13B-AWQ](https://huggingface.co/TheBloke/Orca-2-13B-AWQ)\n",
    "\n",
    "docker container start hfpt_Oct28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--Intel--neural-chat-7b-v3-1\n",
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--TheBloke--Orca-2-13B-AWQ\n",
      "models--TheBloke--Orca-2-13B-GPTQ\n",
      "models--TheBloke--Python-Code-33B-GPTQ\n",
      "models--facebook--blenderbot-1B-distill\n",
      "models--google--flan-t5-large\n",
      "models--gpt2-medium\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--model_name\n",
      "tmp0m77pc0x\n",
      "tmpcjh0h7gn\n",
      "tmpfcrmwgx2\n",
      "tmpzafytbf_\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup \"TheBloke/Orca-2-13B-AWQ\"\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--Orca-2-13B-AWQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 7.25GB to /home/rob/Data3/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Orca-2-13B-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_utils.py:3378\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3368\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3369\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3371\u001b[0m     (\n\u001b[1;32m   3372\u001b[0m         model,\n\u001b[1;32m   3373\u001b[0m         missing_keys,\n\u001b[1;32m   3374\u001b[0m         unexpected_keys,\n\u001b[1;32m   3375\u001b[0m         mismatched_keys,\n\u001b[1;32m   3376\u001b[0m         offload_index,\n\u001b[1;32m   3377\u001b[0m         error_msgs,\n\u001b[0;32m-> 3378\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3379\u001b[0m         model,\n\u001b[1;32m   3380\u001b[0m         state_dict,\n\u001b[1;32m   3381\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3382\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3383\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3384\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3385\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3386\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3387\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3388\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3389\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3390\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3391\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3392\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3393\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3394\u001b[0m     )\n\u001b[1;32m   3396\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3397\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_utils.py:3632\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3630\u001b[0m     set_initialized_submodules(model, _loaded_keys)\n\u001b[1;32m   3631\u001b[0m     \u001b[39m# This will only initialize submodules that are not marked as initialized by the line above.\u001b[39;00m\n\u001b[0;32m-> 3632\u001b[0m     model\u001b[39m.\u001b[39;49mapply(model\u001b[39m.\u001b[39;49m_initialize_weights)\n\u001b[1;32m   3634\u001b[0m \u001b[39m# Set some modules to fp32 if any\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[39mif\u001b[39;00m keep_in_fp32_modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    898\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    898\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.apply at line 897 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m \n\u001b[1;32m    895\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 897\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    898\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:898\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m    897\u001b[0m     module\u001b[39m.\u001b[39mapply(fn)\n\u001b[0;32m--> 898\u001b[0m fn(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    899\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_utils.py:1392\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, \u001b[39m\"\u001b[39m\u001b[39m_is_hf_initialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1391\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_weights(module)\n\u001b[1;32m   1393\u001b[0m module\u001b[39m.\u001b[39m_is_hf_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:846\u001b[0m, in \u001b[0;36mLlamaPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    844\u001b[0m std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39minitializer_range\n\u001b[1;32m    845\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, nn\u001b[39m.\u001b[39mLinear):\n\u001b[0;32m--> 846\u001b[0m     module\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnormal_(mean\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, std\u001b[39m=\u001b[39;49mstd)\n\u001b[1;32m    847\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m         module\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This takes up all CPU RAM, then fills up the swap file ...\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True, device_map=\"cuda:0\")\n",
    "\n",
    "# this too takes up CPU RAM\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False, revision=\"main\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False)\n",
    "\n",
    "# 103m 59.2s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
