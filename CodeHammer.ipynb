{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd871291-1d41-4ab1-8881-cc941f3c9012",
   "metadata": {},
   "source": [
    "November 3 , 2023\n",
    "\n",
    "Running this example on my brand spanking new 4090!\n",
    "\n",
    "[How to Load LLama 13b for Inference on a single RTX 4090](https://codehammer.io/how-to-load-llama-13b-for-inference-on-a-single-rtx-4090/)\n",
    "\n",
    "docker container start hfpt_Oct28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This docker container did not start with the models 'models--meta-llama--Llama-2-7b-hf' or 'models--meta-llama--Llama-2-13b-hf' already downloaded, so what I did was manually copy them from my local '/home/rob/Data2/huggingface/transformers' folder into this container into the folder with the same name.\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f87d6ab-e1f6-410f-b1cc-8edb1e19b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-cased\n",
      "models--decapoda-research--llama-13b-hf\n",
      "models--facebook--bart-large-cnn\n",
      "models--google--pegasus-cnn_dailymail\n",
      "models--gpt2\n",
      "models--gpt2-xl\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--meta-llama--Llama-2-7b-chat-hf\n",
      "models--meta-llama--Llama-2-7b-hf\n",
      "models--openai-gpt\n",
      "models--t5-large\n",
      "models--transformersbook--pegasus-samsum\n",
      "models--xlm-roberta-base\n",
      "tmp560c_s3e\n",
      "tmpea0zbrrj\n",
      "tmpni3ccozw\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33dc1-7ff9-47bc-9c7e-8459ee05f6cb",
   "metadata": {},
   "source": [
    "All of these models load to the 4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516b0911-d38b-4ce3-8eb9-44e3e45e9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'meta-llama/Llama-2-7b'    # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat'    #Nope! \n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b'     # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'  # Works! 14006MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat' # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf' #Works! 14006MiB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK! This now works! Sweet! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a1873a19ca4b00a24ac7e33d78c8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OK! This now works! Sweet! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9282ba-b39a-45dd-b7fd-c929949ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84c004c-5e0f-4670-861b-874ec89f0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db0d2a5-78af-40d1-8adc-7ee0b6f5a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 1. A car is a vehicle with four wheels. 2. A truck is a vehicle with two wheels. 3. A car is a vehicle that runs on gas. 4. A truck is a vehicle that runs on diesel. 5. A car is a vehicle that is'}]\n",
      "CPU times: user 10.7 s, sys: 465 ms, total: 11.1 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecc1a5c-9492-4211-9f7d-7a44446e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba6b42d-c4ce-44de-91c9-2933e4734efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 1. Car = 4 wheels 2. Truck = 2 wheels\\nI am the only person in the world who has 15 brothers and 2 sisters. Who am I?\\nAnswer: A pencil\\nA man is on the second floor of a building. He is wearing a helmet. What is he doing?\\nAnswer: He is painting the ceiling.\\nI have two hands but no arms, two eyes but no head, two feet but no legs, and two wings but no body. Who am I?\\nAnswer: A bird\\nThere are 100 doors, and behind 99 of them are lions that will eat you. Behind the hundredth door is a tiger that will also eat you. What do you do?\\nAnswer: You run through the first 99 doors because those lions are behind you.\\nQuestion: What has four legs but only one foot?\\nAnswer: A bed\\nQuestion: What has a head but no body?\\nAnswer: A penny\\nQuestion: What has a foot but no legs?\\nAnswer: A chair\\nQuestion: What has a face but no head?\\nAnswer: A clock\\nQuestion:'}]\n",
      "CPU times: user 40.5 s, sys: 96 ms, total: 40.6 s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd378a-283f-4e46-a189-5ec77b17f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
