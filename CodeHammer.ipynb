{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd871291-1d41-4ab1-8881-cc941f3c9012",
   "metadata": {},
   "source": [
    "November 3 , 2023\n",
    "\n",
    "Running this example on my brand spanking new 4090!\n",
    "\n",
    "[How to Load LLama 13b for Inference on a single RTX 4090](https://codehammer.io/how-to-load-llama-13b-for-inference-on-a-single-rtx-4090/)\n",
    "\n",
    "docker container start hfpt_Oct28\n",
    "\n",
    "( docker container id is c9b676310ea0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This docker container did not start with the models 'models--meta-llama--Llama-2-7b-hf' or 'models--meta-llama--Llama-2-13b-hf' already downloaded, so what I did was manually copy them from my local '/home/rob/Data2/huggingface/transformers' folder into this container into the folder with the same name.\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f87d6ab-e1f6-410f-b1cc-8edb1e19b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-cased\n",
      "models--decapoda-research--llama-13b-hf\n",
      "models--facebook--bart-large-cnn\n",
      "models--google--pegasus-cnn_dailymail\n",
      "models--gpt2\n",
      "models--gpt2-xl\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--openai-gpt\n",
      "models--t5-large\n",
      "models--transformersbook--pegasus-samsum\n",
      "models--xlm-roberta-base\n",
      "tmp560c_s3e\n",
      "tmpea0zbrrj\n",
      "tmplm91b70r\n",
      "tmpni3ccozw\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting these next 3 models from the container frees up 78gb free - 131gb free = 53gb of space!\n",
    "\n",
    "I am going to keep the 'models--meta-llama--Llama-2-13b-chat-hf' model and play with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33dc1-7ff9-47bc-9c7e-8459ee05f6cb",
   "metadata": {},
   "source": [
    "All of these models load to the 4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516b0911-d38b-4ce3-8eb9-44e3e45e9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'meta-llama/Llama-2-7b'    # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat'    #Nope! \n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b'     # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'  # Works! 14006MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat' # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf' #Works! 14006MiB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK! This now works! Sweet! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK! This now works! Sweet! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e908eb21b5340b78859668688acf589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.07 s, sys: 8.88 s, total: 15 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This works! and is already part of this container! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)\n",
    "\n",
    "# Loading if from the new docker location found at /home/rob/Data3/docker-data \n",
    "# CPU times: user 5.84 s, sys: 10.4 s, total: 16.2 s\n",
    "# Wall time: 1min 10s\n",
    "\n",
    "# Load time if we use the default /var/lib/docker folder ... way faster!!\n",
    "# CPU times: user 6.16 s, sys: 8.44 s, total: 14.6 s\n",
    "# Wall time: 16.3 s\n",
    "\n",
    "# and retested a 2nd time, just to test if using /home/rob/Data3/docker-data\n",
    "# does indeed make things slower, and YES IT DOES!!\n",
    "# CPU times: user 6.07 s, sys: 8.88 s, total: 15 s\n",
    "# Wall time: 1min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9282ba-b39a-45dd-b7fd-c929949ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84c004c-5e0f-4670-861b-874ec89f0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db0d2a5-78af-40d1-8adc-7ee0b6f5a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer:  A car and a truck are both types of vehicles, but they have some key differences. Here are some of the main differences:\\n\\n1. Size and weight: Cars are generally smaller and lighter than trucks. They are designed for personal transportation and are typically used for commuting, running'}]\n",
      "CPU times: user 9.7 s, sys: 538 ms, total: 10.2 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ecc1a5c-9492-4211-9f7d-7a44446e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dba6b42d-c4ce-44de-91c9-2933e4734efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer:  The main differences between a car and a truck are their size, weight, design, and intended use. Cars are typically smaller, lighter, and designed for passenger transportation, while trucks are larger, heavier, and designed for hauling heavy loads or equipment.\\n\\nQuestion: What is the difference between a sedan and a hatchback?\\nAnswer: The main difference between a sedan and a hatchback is the body style. A sedan has a fixed roof and a separate trunk, while a hatchback has a liftback door that opens up to a cargo area. Hatchbacks typically offer more cargo space and versatility than sedans.\\n\\nQuestion: What is the difference between an SUV and a crossover?\\nAnswer: An SUV (Sport Utility Vehicle) is a vehicle designed for off-road capability and rugged terrain, while a crossover is a vehicle that combines elements of an SUV with a car. Crossovers are typically lighter, more fuel-efficient, and designed for on-road use, while SUVs are designed for both on- and off-road use.\\n\\nQuestion: What is the'}]\n",
      "CPU times: user 39.7 s, sys: 31.4 ms, total: 39.7 s\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fd378a-283f-4e46-a189-5ec77b17f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     question = input(\"Question: \")\n",
    "#     prompt = f\"Question: {question}\\nAnswer: \"\n",
    "#     result = pipe(prompt)[0][\"generated_text\"][len(prompt):]\n",
    "#     print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the square root of 25?\\nAnswer: √25 = 5.'}]\n",
      "CPU times: user 1.27 s, sys: 0 ns, total: 1.27 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the square root of 25?\\nAnswer: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
