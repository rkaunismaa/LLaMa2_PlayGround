{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd871291-1d41-4ab1-8881-cc941f3c9012",
   "metadata": {},
   "source": [
    "November 3 , 2023\n",
    "\n",
    "Running this example on my brand spanking new 4090!\n",
    "\n",
    "[How to Load LLama 13b for Inference on a single RTX 4090](https://codehammer.io/how-to-load-llama-13b-for-inference-on-a-single-rtx-4090/)\n",
    "\n",
    "docker container start hfpt_Oct28\n",
    "\n",
    "( docker container id is c9b676310ea0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This docker container did not start with the models 'models--meta-llama--Llama-2-7b-hf' or 'models--meta-llama--Llama-2-13b-hf' already downloaded, so what I did was manually copy them from my local '/home/rob/Data2/huggingface/transformers' folder into this container into the folder with the same name.\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f87d6ab-e1f6-410f-b1cc-8edb1e19b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-cased\n",
      "models--bigscience--bloomz-560m\n",
      "models--decapoda-research--llama-13b-hf\n",
      "models--facebook--bart-large-cnn\n",
      "models--google--pegasus-cnn_dailymail\n",
      "models--google--vit-base-patch16-224-in21k\n",
      "models--gpt2\n",
      "models--gpt2-xl\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--meta-llama--Llama-2-7b-hf\n",
      "models--mistralai--Mistral-7B-Instruct-v0.1\n",
      "models--mistralai--Mistral-7B-v0.1\n",
      "models--nvidia--mit-b0\n",
      "models--openai-gpt\n",
      "models--roberta-large\n",
      "models--robkayinto--vit-base-patch16-224-in21k-finetuned-lora-food101\n",
      "models--t5-large\n",
      "models--transformersbook--pegasus-samsum\n",
      "models--xlm-roberta-base\n",
      "tmp3i5avt2i\n",
      "tmp560c_s3e\n",
      "tmp8is86yg8\n",
      "tmpea0zbrrj\n",
      "tmplm91b70r\n",
      "tmpni3ccozw\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting these next 3 models from the container frees up 78gb free - 131gb free = 53gb of space!\n",
    "\n",
    "I am going to keep the 'models--meta-llama--Llama-2-13b-chat-hf' model and play with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf\n",
    "# docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 13.5GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf\n",
    "# docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf\n",
    "# docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 26GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf\n",
    "# docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33dc1-7ff9-47bc-9c7e-8459ee05f6cb",
   "metadata": {},
   "source": [
    "All of these models load to the 4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516b0911-d38b-4ce3-8eb9-44e3e45e9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'meta-llama/Llama-2-7b'    # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat'    #Nope! \n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b'     # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'  # Works! 14006MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat' # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf' #Works! 14006MiB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbbfd03332a4bbd9849b0011b4126e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Does this work for mistralai/Mistral-7B-v0.1 ?? Yup! It's downloading! Nice!\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--mistralai--Mistral-7B-v0.1 /home/rob/Data2/huggingface/transformers/\n",
    "checkpoint = 'mistralai/Mistral-7B-v0.1'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True) # 8236 VRAM\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "# 207m 33.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aa41e221144d5fa8fb8a21e887467d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OK! This now works! Sweet! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK! This now works! Sweet! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2b0818e6ba4395943c6e7b1df4952f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.11 s, sys: 7.26 s, total: 12.4 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This works! and is already part of this container! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)\n",
    "\n",
    "# Loading if from the new docker location found at /home/rob/Data3/docker-data \n",
    "# CPU times: user 5.84 s, sys: 10.4 s, total: 16.2 s\n",
    "# Wall time: 1min 10s\n",
    "\n",
    "# Load time if we use the default /var/lib/docker folder ... way faster!!\n",
    "# CPU times: user 6.16 s, sys: 8.44 s, total: 14.6 s\n",
    "# Wall time: 16.3 s\n",
    "\n",
    "# and retested a 2nd time, just to test if using /home/rob/Data3/docker-data\n",
    "# does indeed make things slower, and YES IT DOES!!\n",
    "# CPU times: user 6.07 s, sys: 8.88 s, total: 15 s\n",
    "# Wall time: 1min\n",
    "\n",
    "# Excellent! Moved the docker location to /home/rob/Data3/docker-data ... and now it's fast again! Nice!\n",
    "# CPU times: user 5.11 s, sys: 7.26 s, total: 12.4 s\n",
    "# Wall time: 14.9 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9282ba-b39a-45dd-b7fd-c929949ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84c004c-5e0f-4670-861b-874ec89f0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db0d2a5-78af-40d1-8adc-7ee0b6f5a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 18-wheelers.\\nThis was the last one, and I was done.\\nI’d like to thank everyone who participated in the contest, especially those who tried their hand at writing. I was amazed at the creativity and talent that was displayed, and I hope that everyone learned something about'}]\n",
      "CPU times: user 8.86 s, sys: 485 ms, total: 9.34 s\n",
      "Wall time: 9.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecc1a5c-9492-4211-9f7d-7a44446e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba6b42d-c4ce-44de-91c9-2933e4734efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 20 mpg!\\n\\n## 5.\\n\\nQuestion: What can you do to a car that’s never been made?\\nAnswer: Win a race!\\n\\n## 6.\\n\\nQuestion: Can you name a car that has an air conditioner?\\nAnswer: An F1!\\n\\n## 7.\\n\\nQuestion: How can you tell a car to go faster?\\nAnswer: Press the accelerator pedal!\\n\\n## 8.\\n\\nQuestion: What do you call a car that eats people?\\nAnswer: A cannibal!\\n\\n## 9.\\n\\nQuestion: Can you name a car that has no engine?\\nAnswer: A taxi!\\n\\n## 10.\\n\\nQuestion: Why did the man cross the road?\\nAnswer: Because it was a chicken!\\n\\n## 11.\\n\\nQuestion: What do you call a car with no doors?\\nAnswer: A car!\\n\\n## 12.\\n\\nQuestion: What do you call a car with no doors?\\nAnswer: A car!\\n\\n## 13.\\n\\nQuestion: What do you call'}]\n",
      "CPU times: user 40.9 s, sys: 181 ms, total: 41.1 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28fd378a-283f-4e46-a189-5ec77b17f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     question = input(\"Question: \")\n",
    "#     prompt = f\"Question: {question}\\nAnswer: \"\n",
    "#     result = pipe(prompt)[0][\"generated_text\"][len(prompt):]\n",
    "#     print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the square root of 25?\\nAnswer: √25 = 5'}]\n",
      "CPU times: user 1.14 s, sys: 3.43 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the square root of 25?\\nAnswer: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
