{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd871291-1d41-4ab1-8881-cc941f3c9012",
   "metadata": {},
   "source": [
    "November 3 , 2023\n",
    "\n",
    "Running this example on my brand spanking new 4090!\n",
    "\n",
    "[How to Load LLama 13b for Inference on a single RTX 4090](https://codehammer.io/how-to-load-llama-13b-for-inference-on-a-single-rtx-4090/)\n",
    "\n",
    "docker container start hfpt_Oct28\n",
    "\n",
    "( docker container id is c9b676310ea0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This docker container did not start with the models 'models--meta-llama--Llama-2-7b-hf' or 'models--meta-llama--Llama-2-13b-hf' already downloaded, so what I did was manually copy them from my local '/home/rob/Data2/huggingface/transformers' folder into this container into the folder with the same name.\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "docker cp /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f87d6ab-e1f6-410f-b1cc-8edb1e19b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-cased\n",
      "models--decapoda-research--llama-13b-hf\n",
      "models--facebook--bart-large-cnn\n",
      "models--google--pegasus-cnn_dailymail\n",
      "models--gpt2\n",
      "models--gpt2-xl\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--meta-llama--Llama-2-7b-chat-hf\n",
      "models--meta-llama--Llama-2-7b-hf\n",
      "models--openai-gpt\n",
      "models--t5-large\n",
      "models--transformersbook--pegasus-samsum\n",
      "models--xlm-roberta-base\n",
      "tmp560c_s3e\n",
      "tmpea0zbrrj\n",
      "tmpni3ccozw\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting these next 3 models from the container frees up 78gb free - 131gb free = 53gb of space!\n",
    "\n",
    "I am going to keep the 'models--meta-llama--Llama-2-13b-chat-hf' model and play with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b33dc1-7ff9-47bc-9c7e-8459ee05f6cb",
   "metadata": {},
   "source": [
    "All of these models load to the 4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516b0911-d38b-4ce3-8eb9-44e3e45e9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'meta-llama/Llama-2-7b'    # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat'    #Nope! \n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf' # Works! 7718MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b'     # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'  # Works! 14006MiB\n",
    "\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat' # Nope!\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf' #Works! 14006MiB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK! This now works! Sweet! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 7660MiB\n",
    "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f728f1a23d394486ba0cd0738b7ee94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OK! This now works! Sweet! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-hf'\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works! and is already part of this container! 13948MiB\n",
    "checkpoint = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9282ba-b39a-45dd-b7fd-c929949ff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84c004c-5e0f-4670-861b-874ec89f0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db0d2a5-78af-40d1-8adc-7ee0b6f5a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 1. A car is a vehicle that is used to transport people. A truck is a vehicle that is used to transport goods. 2. Cars are typically smaller than trucks and have a higher speed limit. 3. Cars are typically more expensive than trucks. 4. C'}]\n",
      "CPU times: user 10.6 s, sys: 427 ms, total: 11.1 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecc1a5c-9492-4211-9f7d-7a44446e05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba6b42d-c4ce-44de-91c9-2933e4734efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the difference between a car and a truck?\\nAnswer: 4 wheel drive and 4 wheel steer.\\nQuestion: What is the difference between a 2x4 and a 4x4?\\nAnswer: One goes in the front, the other goes in the back.\\nQuestion: What is the difference between a 4x4 and a 4x2?\\nAnswer: One goes in the front, the other goes in the back, and one goes under the front, the other goes under the back.\\nQuestion: What is the difference between a 4x4 and a 4x2x4?\\nAnswer: One goes in the front, the other goes in the back, and one goes under the front, the other goes under the back, and one goes on top of the other.\\nQuestion: What is the difference between a 4x4 and a 4x2x4x4?\\nAnswer: One goes in the front, the other goes in the back, and one goes under the front, the other goes under the back, and one goes on top of the other, and one goes in the front, the other goes in the back, and one goes under the front, the other goes under the back, and one goes on top of the other'}]\n",
      "CPU times: user 42 s, sys: 0 ns, total: 42 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the difference between a car and a truck?\\nAnswer: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd378a-283f-4e46-a189-5ec77b17f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     question = input(\"Question: \")\n",
    "#     prompt = f\"Question: {question}\\nAnswer: \"\n",
    "#     result = pipe(prompt)[0][\"generated_text\"][len(prompt):]\n",
    "#     print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Question: What is the square root of 25?\\nAnswer: 5 is the square root of 25.\\nQuestion: What is the square root of 26?\\nAnswer: 5.13 is the square root of 26.\\nQuestion: What is the square root of 27?\\nAnswer: 5.22 is the square root of 27.\\nQuestion: What is the square root of 28?\\nAnswer: 5.31 is the square root of 28.\\nQuestion: What is the square root of 29?\\nAnswer: 5.40 is the square root of 29.\\nQuestion: What is the square root of 30?\\nAnswer: 5.50 is the square root of 30.\\nQuestion: What is the square root of 31?\\nAnswer: 5.60 is the square root of 31.\\nQuestion: What is the square root of 32?\\nAnswer: 5.70 is the square root of 32.\\nQuestion: What is the square root of 33?\\nAnswer: 5.80 is the square root of 33.\\nQuestion: What is the'}]\n",
      "CPU times: user 43.4 s, sys: 26 ms, total: 43.5 s\n",
      "Wall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(pipe(\"Question: What is the square root of 25?\\nAnswer: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
