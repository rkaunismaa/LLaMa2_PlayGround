{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday, January 23, 2024\n",
    "\n",
    "Trying this again because I just noticed this morning that this model was updated 5 days ago.\n",
    "\n",
    "So I copied in my existing copy of this model from Data2, and when I ran the load, I got an error when loading the model ...\n",
    "\n",
    "ModuleNotFoundError: No module named 'transformers.cache_utils'\n",
    "\n",
    "So I will delete the copy, then start the load again ... Nope! Still getting this error! WTF?\n",
    "\n",
    "Hmm Gonna try to download the newest version of this model later today ...\n",
    "\n",
    "#### Sunday, January 7, 2024\n",
    "\n",
    "Downloaded the newest version of this model.\n",
    "\n",
    "#### Sunday, December 24, 2023\n",
    "\n",
    "Download microsoft/phi-2 and play with it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--NousResearch--Llama-2-7b-chat-hf\n",
      "models--bert-base-uncased\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--ehartford--samantha-mistral-7b\n",
      "models--microsoft--phi-2\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mlabonne--NeuralBeagle14-7B\n",
      "models--sentence-transformers--msmarco-MiniLM-L-12-v3\n",
      "models--teknium--OpenHermes-2.5-Mistral-7B\n",
      "models--unsloth--mistral-7b-bnb-4bit\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--microsoft--phi-2  c8324b70601d://root/.cache/huggingface/hub\n",
    "# Successfully copied 5.56GB to c8324b70601d://root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/.cache/huggingface/hub/models--microsoft--phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--NousResearch--Llama-2-7b-chat-hf\n",
      "models--bert-base-uncased\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--ehartford--samantha-mistral-7b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mlabonne--NeuralBeagle14-7B\n",
      "models--sentence-transformers--msmarco-MiniLM-L-12-v3\n",
      "models--teknium--OpenHermes-2.5-Mistral-7B\n",
      "models--unsloth--mistral-7b-bnb-4bit\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6e76febfe34ef78114438184634870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e51d6e0f6c41e090ea4e284b92b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a053ba13e24561911e7750e6f91bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.cache_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/phi-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 83m 26.6s\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 78m 12.1s\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py:553\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    552\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 553\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/dynamic_module_utils.py:499\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    487\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    488\u001b[0m     repo_id,\n\u001b[1;32m    489\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    498\u001b[0m )\n\u001b[0;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/dynamic_module_utils.py:199\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mImport a module on the cache directory for modules and extract a class from it.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    `typing.Type`: The class looked for.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m module_path \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mreplace(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:848\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-2/85d00b03fee509307549d823fdd095473ba5197c/modeling_phi.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_attn_mask_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prepare_4d_causal_attention_mask\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BaseModelOutputWithPast,\n\u001b[1;32m     33\u001b[0m     CausalLMOutputWithPast,\n\u001b[1;32m     34\u001b[0m     SequenceClassifierOutputWithPast,\n\u001b[1;32m     35\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.cache_utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "\n",
    "# 83m 26.6s\n",
    "# 78m 12.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--CompVis--stable-diffusion-v1-4\n",
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--anton-l--ddpm-butterflies-128\n",
      "models--google--ddpm-cat-256\n",
      "models--google--ddpm-celebahq-256\n",
      "models--google--ddpm-church-256\n",
      "models--kandinsky-community--kandinsky-2-2-decoder\n",
      "models--kandinsky-community--kandinsky-2-2-prior\n",
      "models--kohbanye--pixel-art-style\n",
      "models--lllyasviel--control_v11p_sd15_openpose\n",
      "models--microsoft--Orca-2-13b\n",
      "models--microsoft--phi-2\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--nitrosocke--Ghibli-Diffusion\n",
      "models--ogkalu--Comic-Diffusion\n",
      "models--runwayml--stable-diffusion-v1-5\n",
      "models--stabilityai--sd-vae-ft-mse\n",
      "models--stabilityai--sd-x2-latent-upscaler\n",
      "models--stabilityai--stable-diffusion-xl-base-1.0\n",
      "models--stabilityai--stable-diffusion-xl-refiner-1.0\n",
      "models--stabilityai--stable-video-diffusion-img2vid-xt\n",
      "tmplfhsdhsz\n",
      "tmpnhzefb_1\n",
      "tmpqyiqy8vy\n",
      "tmpt0cjd006\n",
      "tmpw_a6f9l2\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c8324b70601d://root/.cache/huggingface/hub/models--microsoft--phi-2 /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 5.56GB to /home/rob/Data3/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "\n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "## Exercises\n",
      "\n",
      "1. Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "    \"\"\"\n",
      "    Returns the sum of all even numbers in the list\n",
      "    \"\"\"\n",
      "    return sum(filter(lambda x: x % 2 == 0, numbers))\n",
      "\n",
      "print(sum_even([1, 2, 3, 4, 5, 6])) # Output: 12\n",
      "```\n",
      "\n",
      "2. Write a Python function that takes\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
