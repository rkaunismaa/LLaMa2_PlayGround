{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wednesday, November 8, 2023\n",
    "\n",
    "https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral (published November 7, 2023)\n",
    "\n",
    "https://github.com/mehdiir/Roberta-Llama-Mistral/\n",
    "\n",
    "This notebook was generated from the 'Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md' using jupytext ...\n",
    "\n",
    "jupytext --to ipynb Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e93560fc",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora\" \n",
    "thumbnail: /blog/assets/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/Thumbnail.png\n",
    "authors:\n",
    "- user: mehdiiraqui \n",
    "  guest: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a139bd4",
   "metadata": {},
   "source": [
    "# Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora\n",
    "<!-- TOC -->\n",
    "\n",
    "- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with LoRA](#comparing-the-performance-of-llms-a-deep-dive-into-roberta-llama-2-and-mistral-for-disaster-tweets-analysis-with-lora)\n",
    "    - [Introduction](#introduction)\n",
    "    - [Hardware Used](#hardware-used)\n",
    "    - [Goals](#goals)\n",
    "    - [Dependencies](#dependencies)\n",
    "    - [Pre-trained Models](#pre-trained-models)\n",
    "        - [RoBERTa](#roberta)\n",
    "        - [Llama 2](#llama-2)\n",
    "        - [Mistral 7B](#mistral-7b)\n",
    "    - [LoRA](#lora)\n",
    "    - [Setup](#setup)\n",
    "    - [Data preparation](#data-preparation)\n",
    "        - [Data loading](#data-loading)\n",
    "        - [Data Processing](#data-processing)\n",
    "    - [Models](#models)\n",
    "        - [RoBERTa](#roberta)\n",
    "            - [Load RoBERTA Checkpoints for the Classification Task](#load-roberta-checkpoints-for-the-classification-task)\n",
    "            - [LoRA setup for RoBERTa classifier](#lora-setup-for-roberta-classifier)\n",
    "        - [Mistral](#mistral)\n",
    "            - [Load checkpoints for the classfication model](#load-checkpoints-for-the-classfication-model)\n",
    "            - [LoRA setup for Mistral 7B classifier](#lora-setup-for-mistral-7b-classifier)\n",
    "        - [Llama 2](#llama-2)\n",
    "            - [Load checkpoints for the classification mode](#load-checkpoints-for-the-classfication-mode)\n",
    "            - [LoRA setup for Llama 2 classifier](#lora-setup-for-llama-2-classifier)\n",
    "    - [Setup the trainer](#setup-the-trainer)\n",
    "        - [Evaluation Metrics](#evaluation-metrics)\n",
    "        - [Custom Trainer for Weighted Loss](#custom-trainer-for-weighted-loss)\n",
    "        - [Trainer Setup](#trainer-setup)\n",
    "            - [RoBERTa](#roberta)\n",
    "            - [Mistral-7B](#mistral-7b)\n",
    "            - [Llama 2](#llama-2)\n",
    "    - [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "    - [Results](#results)\n",
    "    - [Conclusion](#conclusion)\n",
    "    - [Resources](#resources)\n",
    "\n",
    "<!-- /TOC -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c986a1",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In the fast-moving world of Natural Language Processing (NLP), we often find ourselves comparing different language models to see which one works best for specific tasks. This blog post is all about comparing three models: RoBERTa, Mistral-7b, and Llama-2-7b. We used them to tackle a common problem - classifying tweets about disasters. It is important to note that Mistral and Llama 2 are large models with 7 billion parameters. In contrast, RoBERTa-large (355M parameters) is a relatively smaller model used as a baseline for the comparison study.\n",
    "\n",
    "In this blog, we used PEFT (Parameter-Efficient Fine-Tuning) technique: LoRA (Low-Rank Adaptation of Large Language Models) for fine-tuning the pre-trained model on the sequence classification task. LoRa is designed to significantly reduce the number of trainable parameters while maintaining strong downstream task performance. \n",
    "\n",
    "The main objective of this blog post is to implement LoRA fine-tuning for sequence classification tasks using three pre-trained models from Hugging Face: [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1), and [roberta-large](https://huggingface.co/roberta-large)\n",
    "\n",
    "## Hardware Used \n",
    "\n",
    "- Number of nodes: 1 \n",
    "- Number of GPUs per node: 1\n",
    "- GPU type: A6000 \n",
    "- GPU memory: 48GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358537b",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- Implement fine-tuning of pre-trained LLMs using LoRA PEFT methods.\n",
    "- Learn how to use the HuggingFace APIs ([transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index), and [datasets](https://huggingface.co/docs/datasets/index)).\n",
    "- Setup the hyperparameter tuning and experiment logging using [Weights & Biases](https://wandb.ai).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0a91e",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6110b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "* datasets\n",
    "* evaluate\n",
    "* peft\n",
    "* scikit-learn\n",
    "* torch\n",
    "* transformers\n",
    "* wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd2f06",
   "metadata": {},
   "source": [
    "Note: For reproducing the reported results, please check the pinned versions in the [wandb reports](#resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29789e8",
   "metadata": {},
   "source": [
    "## Pre-trained Models\n",
    "\n",
    "### [RoBERTa](https://arxiv.org/abs/1907.11692)\n",
    "\n",
    "RoBERTa (Robustly Optimized BERT Approach) is an advanced variant of the BERT model proposed by Meta AI research team. BERT is a transformer-based language model using self-attention mechanisms for contextual word representations and trained with a masked language model objective. Note that BERT is an encoder only model used for natural language understanding tasks (such as sequence classification and token classification).\n",
    "\n",
    "RoBERTa is a popular model to fine-tune and appropriate as a baseline for our experiments. For more information, you can check the Hugging Face model [card](https://huggingface.co/docs/transformers/model_doc/roberta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348b6a0",
   "metadata": {},
   "source": [
    "### [Llama 2](https://arxiv.org/abs/2307.09288)\n",
    "\n",
    "Llama 2 models, which stands for Large Language Model Meta AI, belong to the family of large language models (LLMs) introduced by Meta AI. The Llama 2 models vary in size, with parameter counts ranging from 7 billion to 65 billion.\n",
    "\n",
    "Llama 2 is an auto-regressive language model, based on the transformer decoder architecture. To generate text, Llama 2 processes a sequence of words as input and iteratively predicts the next token using a sliding window.\n",
    "Llama 2 architecture is slightly different from models like GPT-3. For instance, Llama 2 employs the SwiGLU activation function rather than ReLU and opts for rotary positional embeddings in place of absolute learnable positional embeddings. \n",
    " \n",
    "The recently released Llama 2 introduced architectural refinements to better leverage very long sequences by extending the context length to up to 4096 tokens, and using grouped-query attention (GQA) decoding. \n",
    "\n",
    "### [Mistral 7B](https://arxiv.org/abs/2310.06825)\n",
    "\n",
    "Mistral 7B v0.1, with 7.3 billion parameters, is the first LLM introduced by Mistral AI.\n",
    "The main novel techniques used in Mistral 7B's architecture are: \n",
    "- Sliding Window Attention: Replace the full attention (square compute cost) with a sliding window based attention where each token can attend to at most 4,096 tokens from the previous layer (linear compute cost). This mechanism enables Mistral 7B to handle longer sequences, where higher layers can access historical information beyond the window size of 4,096 tokens. \n",
    "- Grouped-query Attention: used in Llama 2 as well, the technique optimizes the inference process (reduce processing time) by caching the key and value vectors for previously decoded tokens in the sequence.  \n",
    "\n",
    "## [LoRA](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "PEFT, Parameter Efficient Fine-Tuning, is a collection of techniques (p-tuning, prefix-tuning, IA3, Adapters, and LoRa) designed to fine-tune large models using a much smaller set of training parameters while preserving the performance levels typically achieved through full fine-tuning. \n",
    "\n",
    "LoRA, Low-Rank Adaptation, is a PEFT method that shares similarities with Adapter layers. Its primary objective is to reduce the model's trainable parameters. LoRA's operation involves \n",
    "learning a low rank update matrix while keeping the pre-trained weights frozen.\n",
    "\n",
    "![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral/lora.png)\n",
    "\n",
    "## Setup\n",
    "\n",
    "RoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` for all models to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ddf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MAX_LEN = 512 \n",
    "roberta_checkpoint = \"roberta-large\"\n",
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "# override the above ...\n",
    "# mistral_checkpoint = '/tf/All/Data3/mistralai/mistral-7B-v0.1/mistral-7B-v0.1' \n",
    "llama_checkpoint = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n",
      "4.35.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5fa0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Data preparation\n",
    "### Data loading\n",
    "\n",
    "We will load the dataset from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f63e9d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mehdiiraqui/twitter_disaster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a2ea9",
   "metadata": {},
   "source": [
    " Now, let's split the dataset into training and validation datasets. Then add the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc79c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Split the dataset into training and validation datasets\n",
    "data = dataset['train'].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a97f8",
   "metadata": {},
   "source": [
    "Here's an overview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba7d28df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'target'],\n",
       "        num_rows: 6090\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'target'],\n",
       "        num_rows: 1523\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'target'],\n",
       "        num_rows: 3263\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91efd50",
   "metadata": {},
   "source": [
    "Let's check the data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605e6e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6090 entries, 0 to 6089\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        6090 non-null   int64 \n",
      " 1   keyword   6037 non-null   object\n",
      " 2   location  4064 non-null   object\n",
      " 3   text      6090 non-null   object\n",
      " 4   target    6090 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 238.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      " 4   target    3263 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 127.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data['train'].to_pandas().info()\n",
    "data['test'].to_pandas().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f347909",
   "metadata": {},
   "source": [
    "- Train dataset\n",
    "\n",
    "```<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 7613 entries, 0 to 7612\n",
    "Data columns (total 5 columns):\n",
    " #   Column    Non-Null Count  Dtype \n",
    "---  ------    --------------  ----- \n",
    " 0   id        7613 non-null   int64 \n",
    " 1   keyword   7552 non-null   object\n",
    " 2   location  5080 non-null   object\n",
    " 3   text      7613 non-null   object\n",
    " 4   target    7613 non-null   int64 \n",
    "dtypes: int64(2), object(3)\n",
    "memory usage: 297.5+ KB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ec99e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- Test dataset\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 3263 entries, 0 to 3262\n",
    "Data columns (total 5 columns):\n",
    " #   Column    Non-Null Count  Dtype \n",
    "---  ------    --------------  ----- \n",
    " 0   id        3263 non-null   int64 \n",
    " 1   keyword   3237 non-null   object\n",
    " 2   location  2158 non-null   object\n",
    " 3   text      3263 non-null   object\n",
    " 4   target    3263 non-null   int64 \n",
    "dtypes: int64(2), object(3)\n",
    "memory usage: 127.6+ KB\n",
    "```\n",
    "\n",
    "**Target distribution in the train dataset**\n",
    "```\n",
    "target\n",
    "0    4342\n",
    "1    3271\n",
    "Name: count, dtype: int64\n",
    "```\n",
    "\n",
    "As the classes are not balanced, we will compute the positive and negative weights and use them for loss calculation later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65279a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\n",
    "neg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1622137404580153\n",
      "0.877521613832853\n"
     ]
    }
   ],
   "source": [
    "print(pos_weights)\n",
    "print(neg_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5f665",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The final weights are: \n",
    "```\n",
    "POS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\n",
    "```\n",
    "\n",
    "Then, we compute the maximum length of the column text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6264ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Characters\n",
    "max_char = data['train'].to_pandas()['text'].str.len().max()\n",
    "# Number of Words\n",
    "max_words = data['train'].to_pandas()['text'].str.split().str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(max_char)\n",
    "print(max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9190cc",
   "metadata": {},
   "source": [
    "```\n",
    "The maximum number of characters is 152.\n",
    "The maximum number of words is 31.\n",
    "```\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Let's take a look to one row example of training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05973d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5285,\n",
       " 'keyword': 'fear',\n",
       " 'location': 'Thibodaux, LA',\n",
       " 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n",
       " 'target': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c30f1c",
   "metadata": {},
   "source": [
    "```\n",
    "{'id': 5285,\n",
    " 'keyword': 'fear',\n",
    " 'location': 'Thibodaux, LA',\n",
    " 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n",
    " 'target': 0}\n",
    "```\n",
    "\n",
    "The data comprises a keyword, a location and the text of the tweet. For the sake of simplicity, we select the `text` feature as the only input to the LLM. \n",
    "\n",
    "At this stage, we prepared the train, validation, and test sets in the HuggingFace format expected by the pre-trained LLMs. The next step is to define the tokenized dataset for training using the appropriate tokenizer to transform the `text` feature into two Tensors of sequence of token ids and attention masks. As each model has its specific tokenizer, we will need to define three different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ea1f3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We start by defining the RoBERTa dataloader: \n",
    "\n",
    "-  Load the tokenizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6698380",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e03a67",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add_prefix_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to 'True' for Llama 2 and Mistral 7b.\n",
    "\n",
    "- Define the preprocessing function for converting one row of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fefa46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_preprocessing_function(examples):\n",
    "    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee395c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "By applying the preprocessing function to the first example of our training dataset, we have the tokenized inputs (`input_ids`) and the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89929f7c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_preprocessing_function(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c0161",
   "metadata": {},
   "source": [
    "```\n",
    "{'input_ids': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "```\n",
    "\n",
    "- Now, let's  apply the preprocessing function to the entire dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922fafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_delete = ['id', 'keyword','location', 'text']\n",
    "# Apply the preprocessing function and remove the undesired columns\n",
    "roberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "# Rename the target to label as for HugginFace standards\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "# Set to torch format\n",
    "roberta_tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb056f",
   "metadata": {},
   "source": [
    "**Note:** we deleted the undesired columns from our data: id, keyword, location and text. We have deleted the text because we have already converted it into the inputs ids and the attention mask:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34781d8c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can have a look into our tokenized training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aeab95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([    0,   127,  2373,  2490,     4,  1205,   640,    90,     4,   876,\n",
       "            73,   118,   725,   398, 13083,   329,   398,   119,  1343,   246,\n",
       "             2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eff54b",
   "metadata": {},
   "source": [
    "```\n",
    "{'label': tensor(0),\n",
    " 'input_ids': tensor([    0,   127,  2373,  2490,     4,  1205,   640,    90,     4,   876,\n",
    "            73,   118,   725,   398, 13083,   329,   398,   119,  1343,   246,\n",
    "             2]),\n",
    " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726723d6",
   "metadata": {},
   "source": [
    "- For generating the training batches, we also need to pad the rows of a given batch to the maximum length found in the batch. For that, we will use the `DataCollatorWithPadding` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b4d3d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b819f04",
   "metadata": {},
   "source": [
    "You can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \n",
    "\n",
    "**Note** that Llama 2 and Mistral 7B don't have a default `pad_token_id`. So, we use the `eos_token_id` for padding as well.\n",
    "\n",
    "- Mistral 7B: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e15065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mistral 7B Tokenizer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True)\n",
    "mistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "def mistral_preprocessing_function(examples):\n",
    "    return mistral_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "mistral_tokenized_datasets = data.map(mistral_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "mistral_tokenized_datasets = mistral_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "mistral_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "mistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2837e2c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- Llama 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e51308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama 2 Tokenizer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, add_prefix_space=True)\n",
    "llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return llama_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "llama_tokenized_datasets = data.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "llama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "llama_tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "llama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439252f",
   "metadata": {},
   "source": [
    "Now that we have prepared the tokenized datasets, the next section will showcase how to load the pre-trained LLMs checkpoints and how to set the LoRa weights. \n",
    "\n",
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part I added to allow only one model to train at a time, cuz I only got 24gb of VRAM! The boolean controls if we load and train the model or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "runRoberta = False\n",
    "runMistral = True\n",
    "runLLama2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "\n",
    "#### Load RoBERTa Checkpoints for the Classification Task\n",
    "\n",
    "We load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72b17206",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    "\n",
    "if runRoberta:\n",
    "    \n",
    "    roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8928da",
   "metadata": {},
   "source": [
    "####  LoRA setup for RoBERTa classifier\n",
    "\n",
    "We import LoRa configuration and set some parameters for RoBERTa classifier:\n",
    "- TaskType: Sequence classification\n",
    "- r(rank): Rank for our decomposition matrices\n",
    "- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n",
    "- lora_dropout: Dropout probability of the LoRA layers\n",
    "- bias: Whether to add bias term to LoRa layers\n",
    "\n",
    "The code below uses the values recommended by the [Lora paper](https://arxiv.org/abs/2106.09685). [Later in this post](#hyperparameter-tuning) we will perform hyperparameter tuning of these parameters using `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c662264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "if runRoberta:\n",
    "    \n",
    "    roberta_peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    "    )\n",
    "    roberta_model = get_peft_model(roberta_model, roberta_peft_config)\n",
    "    roberta_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e2c5b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can see that the number of trainable parameters represents only 0.64% of the RoBERTa model parameters:\n",
    "\n",
    "trainable params: 2,299,908 || all params: 356,610,052 || trainable%: 0.6449363911929212\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df1382",
   "metadata": {},
   "source": [
    "### Mistral\n",
    "\n",
    "#### Load checkpoints for the classfication model\n",
    "\n",
    "Let's load the pre-trained Mistral-7B model with a sequence classification head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1315c7f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9efe4566045499eed6471e8066703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "if runMistral:\n",
    "  \n",
    "  mistral_model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=mistral_checkpoint,\n",
    "    num_labels=2,\n",
    "    device_map=\"auto\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553979e",
   "metadata": {},
   "source": [
    "For Mistral 7B, we have to add the padding token id as it is not defined by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83c2d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runMistral:\n",
    "    \n",
    "    mistral_model.config.pad_token_id = mistral_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9900bef",
   "metadata": {},
   "source": [
    "####  LoRa setup for Mistral 7B classifier\n",
    "\n",
    "For Mistral 7B model, we need to specify the `target_modules` (the query and value vectors from the attention modules):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a52979a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 860,160 || all params: 7,111,528,448 || trainable%: 0.012095290151611583\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "if runMistral:\n",
    "\n",
    "    mistral_peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    mistral_model = get_peft_model(mistral_model, mistral_peft_config)\n",
    "    mistral_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f958ba",
   "metadata": {},
   "source": [
    "The number of trainable parameters reprents only 0.024% of the Mistral model parameters:\n",
    "```\n",
    "trainable params: 1,720,320 || all params: 7,112,380,416 || trainable%: 0.02418768259540745\n",
    "```\n",
    "\n",
    "### Llama 2\n",
    "\n",
    "#### Load checkpoints for the classfication mode\n",
    "\n",
    "Let's load pre-trained Llama 2 model with a sequence classification header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "681f1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# model =LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, torch_dtype=torch.float16)\n",
    "\n",
    "if runLLama2:\n",
    "  \n",
    "  llama_model =  AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=llama_checkpoint,\n",
    "    num_labels=2,\n",
    "    # device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_8bit=True, torch_dtype=torch.float16\n",
    "  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc71b3",
   "metadata": {},
   "source": [
    "For Llama 2, we have to add the padding token id as it is not defined by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63125510",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLLama2:\n",
    "    \n",
    "    llama_model.config.pad_token_id = llama_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d92a7",
   "metadata": {},
   "source": [
    "#### LoRa setup for Llama 2 classifier\n",
    "\n",
    "We define LoRa for Llama 2 with the same parameters as for Mistral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "454f8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "if runLLama2:\n",
    "    \n",
    "    llama_peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS, r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", \n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",  \n",
    "        ],\n",
    "    )\n",
    "\n",
    "    llama_model = get_peft_model(llama_model, llama_peft_config)\n",
    "    llama_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5df50",
   "metadata": {},
   "source": [
    "The number of trainable parameters reprents only 0.12% of the Llama 2 model parameters:\n",
    "```\n",
    "trainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\n",
    "```\n",
    "\n",
    "At this point, we defined the tokenized dataset for training as well as the LLMs setup with LoRa layers. The following section will introduce how to launch training using the HuggingFace `Trainer` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a99ea",
   "metadata": {},
   "source": [
    "## Setup the trainer\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "First, we define the performance metrics we will use to compare the three models: F1 score, recall, precision and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdae70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418dd90",
   "metadata": {},
   "source": [
    "### Custom Trainer for Weighted Loss \n",
    "As mentioned at the beginning of this post, we have an imbalanced distribution between positive and negative classes. We need to train our models with a weighted cross-entropy loss to account for that. The `Trainer` class doesn't support providing a custom loss as it expects to get the loss directly from the model's outputs. \n",
    "\n",
    "So, we need to define our custom `WeightedCELossTrainer` that overrides the `compute_loss` method to calculate the weighted cross-entropy loss based on the model's predictions and the input labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91e7ff23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440dacc9",
   "metadata": {},
   "source": [
    "### Trainer Setup\n",
    "\n",
    "Let's set the training arguments and the trainer for the three models.\n",
    "\n",
    "#### RoBERTa \n",
    "First important step is to move the models to the GPU device for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "959cbb45",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if runRoberta:\n",
    "    \n",
    "    roberta_model = roberta_model.cuda()\n",
    "# roberta_model.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad570d9",
   "metadata": {},
   "source": [
    "It will print the following: \n",
    "```\n",
    "device(type='cuda', index=0)\n",
    "```\n",
    "\n",
    "Then, we set the training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1f0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "if runRoberta:\n",
    "\n",
    "    lr = 1e-4\n",
    "    batch_size = 8\n",
    "    num_epochs = 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"roberta-large-lora-token-classification\",\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type= \"constant\",\n",
    "        warmup_ratio= 0.1,\n",
    "        max_grad_norm= 0.3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.001,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "        fp16=False,\n",
    "        gradient_checkpointing=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba27e18",
   "metadata": {},
   "source": [
    "Finally, we define the RoBERTa trainer by providing the model, the training arguments and the tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f06c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runRoberta:\n",
    "    \n",
    "    roberta_trainer = WeightedCELossTrainer(\n",
    "        model=mistral_model,\n",
    "        args=training_args,\n",
    "        train_dataset=mistral_tokenized_datasets['train'],\n",
    "        eval_dataset=mistral_tokenized_datasets[\"val\"],\n",
    "        data_collator=mistral_data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is not part of the original notebook. I added it to validate all the previous roberta code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runRoberta:\n",
    "    \n",
    "    roberta_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe8913",
   "metadata": {},
   "source": [
    "#### Mistral-7B\n",
    "\n",
    "Similar to RoBERTa, we initialize the `WeightedCELossTrainer` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aeece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# this throws an error ...\n",
    "# mistral_model = mistral_model.cuda()\n",
    "\n",
    "# lr = 1e-4\n",
    "# batch_size = 8\n",
    "# num_epochs = 5\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"mistral-lora-token-classification\",\n",
    "#     learning_rate=lr,\n",
    "#     lr_scheduler_type= \"constant\",\n",
    "#     warmup_ratio= 0.1,\n",
    "#     max_grad_norm= 0.3,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=num_epochs,\n",
    "#     weight_decay=0.001,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     report_to=\"wandb\",\n",
    "#     fp16=True,\n",
    "#     gradient_checkpointing=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# mistral_trainer = WeightedCELossTrainer(\n",
    "#     model=mistral_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=mistral_tokenized_datasets['train'],\n",
    "#     eval_dataset=mistral_tokenized_datasets[\"val\"],\n",
    "#     data_collator=mistral_data_collator,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runMistral:\n",
    "\n",
    "    lr = 1e-4\n",
    "    batch_size = 8\n",
    "    num_epochs = 5\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"mistral-lora-token-classification\",\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type= \"constant\",\n",
    "        warmup_ratio= 0.1,\n",
    "        max_grad_norm= 0.3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.001,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "if runMistral:\n",
    "    \n",
    "    mistral_trainer = WeightedCELossTrainer(\n",
    "        model=mistral_model,\n",
    "        args=training_args,\n",
    "        train_dataset=mistral_tokenized_datasets['train'],\n",
    "        eval_dataset=mistral_tokenized_datasets[\"val\"],\n",
    "        data_collator=mistral_data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobkayinto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/PEFT/wandb/run-20231109_003358-2u9c0ynw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/robkayinto/huggingface/runs/2u9c0ynw' target=\"_blank\">revived-waterfall-24</a></strong> to <a href='https://wandb.ai/robkayinto/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/robkayinto/huggingface' target=\"_blank\">https://wandb.ai/robkayinto/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/robkayinto/huggingface/runs/2u9c0ynw' target=\"_blank\">https://wandb.ai/robkayinto/huggingface/runs/2u9c0ynw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/PEFT/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.ipynb Cell 87\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/PEFT/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m runMistral:\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/PEFT/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.ipynb#Y152sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     mistral_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/transformers/src/transformers/trainer.py:1511\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1512\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1513\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1514\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1515\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1516\u001b[0m     )\n",
      "File \u001b[0;32m/transformers/src/transformers/trainer.py:1867\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1862\u001b[0m             model\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m   1863\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1864\u001b[0m         )\n\u001b[1;32m   1866\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 1867\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1868\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1869\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1870\u001b[0m     \u001b[39m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py:412\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m OptState\u001b[39m.\u001b[39mREADY:\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m--> 412\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n",
      "\u001b[0;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "if runMistral:\n",
    "\n",
    "    mistral_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b19a2",
   "metadata": {},
   "source": [
    "**Note** that we needed to enable half-precision training by setting `fp16` to `True`. The main reason is that Mistral-7B is large, and its weights cannot fit into one GPU memory (48GB) with full float32 precision. \n",
    "\n",
    "#### Llama 2\n",
    "\n",
    "Similar to Mistral 7B, we define the trainer as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928eb3e7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "llama_model = llama_model.cuda()\n",
    "\n",
    "# lr = 1e-4\n",
    "# batch_size = 8\n",
    "# num_epochs = 5\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"llama-lora-token-classification\",\n",
    "#     learning_rate=lr,\n",
    "#     lr_scheduler_type= \"constant\",\n",
    "#     warmup_ratio= 0.1,\n",
    "#     max_grad_norm= 0.3,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=num_epochs,\n",
    "#     weight_decay=0.001,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     report_to=\"wandb\",\n",
    "#     fp16=True,\n",
    "#     gradient_checkpointing=True,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# llama_trainer = WeightedCELossTrainer(\n",
    "#     model=llama_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=llama_tokenized_datasets['train'],\n",
    "#     eval_dataset=llama_tokenized_datasets[\"val\"],\n",
    "#     data_collator=llama_data_collator,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLLama2:\n",
    "\n",
    "    lr = 1e-4\n",
    "    batch_size = 8\n",
    "    num_epochs = 5\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"llama-lora-token-classification\",\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type= \"constant\",\n",
    "        warmup_ratio= 0.1,\n",
    "        max_grad_norm= 0.3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.001,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if runLLama2:\n",
    "\n",
    "    llama_trainer = WeightedCELossTrainer(\n",
    "        model=llama_model,\n",
    "        args=training_args,\n",
    "        train_dataset=llama_tokenized_datasets['train'],\n",
    "        eval_dataset=llama_tokenized_datasets[\"val\"],\n",
    "        data_collator=llama_data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19204dc",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a359f16",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We have used Wandb Sweep API to run hyperparameter tunning with Bayesian search strategy (30 runs). The hyperparameters tuned are the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83819f",
   "metadata": {},
   "source": [
    "| method | metric              | lora_alpha                                | lora_bias                 | lora_dropout            | lora_rank                                          | lr                          | max_length                |\n",
    "|--------|---------------------|-------------------------------------------|---------------------------|-------------------------|----------------------------------------------------|-----------------------------|---------------------------|\n",
    "| bayes  | goal: maximize      | distribution: categorical                 | distribution: categorical | distribution: uniform   | distribution: categorical                          | distribution: uniform       | distribution: categorical |\n",
    "|        | name: eval/f1-score | values:   <br>-16     <br>-32     <br>-64 | values: None              | -max: 0.1   <br>-min: 0 | values:     <br>-4   <br>-8    <br>-16     <br>-32 | -max: 2e-04<br>-min: 1e-05 | values: 512               |           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c956ac",
   "metadata": {},
   "source": [
    " For more information, you can check the Wandb experiment report in the [resources sections](#resources).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815a936",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Models  | F1 score | Training time  | Memory consumption           | Number of trainable parameters |\n",
    "|---------|----------|----------------|------------------------------|--------------------------------|\n",
    "| RoBERTa | 0.8077   | 538 seconds    | GPU1: 9.1 Gb<br>GPU2: 8.3 Gb | 0.64%                          |\n",
    "| Mistral 7B | 0.7364   | 2030 seconds   | GPU1: 29.6 Gb<br>GPU2: 29.5 Gb | 0.024%                         |\n",
    "| Llama 2  | 0.7638   | 2052 seconds   | GPU1: 35 Gb <br>GPU2: 33.9 Gb | 0.12%                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506a47e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this blog post, we compared the performance of three large language models (LLMs) - RoBERTa, Mistral 7b, and Llama 2 - for disaster tweet classification using LoRa. From the performance results, we can see that RoBERTa is outperforming Mistral 7B and Llama 2 by a large margin. This raises the question about whether we really need a complex and large LLM for tasks like short-sequence binary classification? \n",
    "\n",
    "One learning we can draw from this study is that one should account for the specific project requirements, available resources, and performance needs to choose the LLMs model to use. \n",
    "\n",
    "Also, for relatively *simple* prediction tasks with short sequences base models such as RoBERTa remain competitive. \n",
    "\n",
    "Finally, we showcase that LoRa method can be applied to both encoder (RoBERTa) and decoder (Llama 2 and Mistral 7B) models. \n",
    "\n",
    "## Resources \n",
    "\n",
    "1. You can find the code script in the following [Github project](https://github.com/mehdiir/Roberta-Llama-Mistral/).\n",
    "\n",
    "2. You can check the hyper-param search results in the following Weight&Bias reports:\n",
    "    - [RoBERTa](https://api.wandb.ai/links/mehdi-iraqui/505c22j1)\n",
    "    - [Mistral 7B](https://api.wandb.ai/links/mehdi-iraqui/24vveyxp)\n",
    "    - [Llama 2](https://api.wandb.ai/links/mehdi-iraqui/qq8beod0)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
