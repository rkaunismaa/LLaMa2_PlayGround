{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday, January 10, 2024\n",
    "\n",
    "Did some cleanup of /root/.cache/huggingface/hub and wanted to make sure this still runs.\n",
    "\n",
    "### Monday, January 8, 2024\n",
    "\n",
    "I'm trying to run this again on the hfpt_Dec14 container ... yup! It still runs! Nice!\n",
    "\n",
    "### Tueasday, January 2, 2024\n",
    "\n",
    "Run this again to ensure it still runs ... Yup! It still runs! Nice!\n",
    "\n",
    "### Wednesday, December 27, 2023\n",
    "\n",
    "I'm trying to run this again on the hfpt_Dec14 container using 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "1) pip install playwright\n",
    "2) playwright install-deps (again, this installed a ton of stuff!)\n",
    "3) playwright install (this install chromium, firefox)\n",
    "4) pip install html2text\n",
    "\n",
    "Nice! Now it all runs!\n",
    "\n",
    "### Thursday, December 14, 2023\n",
    "\n",
    "Gonna give this another go, to see if things now magically work ...\n",
    "\n",
    "I manually created this notebook from the medium blog post, but I just saw the author DOES include a [notebook link](https://github.com/madhavthaker1/llm/blob/main/rag/e2e_rag.ipynb) ... and the blog post does NOT show all of the code. I added in the missing code, and now this all runs! Nice!\n",
    "\n",
    "### Thursday, November 30, 2023\n",
    "\n",
    "Attempting to run this again ...\n",
    "\n",
    "docker container start hfpt_Oct28\n",
    "\n",
    "### Sunday, November 26, 2023\n",
    "\n",
    "[Build your own RAG with Mistral-7B and LangChain](https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch datasets\n",
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--NousResearch--Llama-2-7b-chat-hf     version.txt\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2  version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers\n",
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.2 c8324b70601d:///root/.cache/huggingface/hub\n",
    "# Successfully copied 14.5GB to c8324b70601d:///root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bits and Bytes parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f37cf8607c6471b9c2527eb2cbc4514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4add92d77e49728fac70331a38c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This loads the model directly into the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# 8.7s   5392 MiB VRAM\n",
    "# 10.3s\n",
    "# 15.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Mistral text generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and chunk documents. Load chunked documents into FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Articles to index\n",
    "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
    "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firing this for the first time produced an error ...\n",
    "# ImportError: playwright is required for AsyncChromiumLoader. Please install it with `pip install playwright`.\n",
    "# pip install playwright\n",
    "\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not run ... sigh.\n",
    "# The error message suggests running the following command from the terminal\n",
    "# playwright install-deps\n",
    "# I did this and it appeared to install a ton of stuff ...\n",
    "# OK Nice! This now runs!\n",
    "\n",
    "# This does the work of pulling the docs and loading them ... \n",
    "docs = loader.load()\n",
    "\n",
    "# 1m 41.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this for the first time generates an error ... solution is to ..\n",
    "# pip install html2text\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 148, which is longer than the specified 100\n",
      "Created a chunk of size 4288, which is longer than the specified 100\n",
      "Created a chunk of size 131, which is longer than the specified 100\n",
      "Created a chunk of size 230, which is longer than the specified 100\n",
      "Created a chunk of size 500, which is longer than the specified 100\n",
      "Created a chunk of size 207, which is longer than the specified 100\n",
      "Created a chunk of size 365, which is longer than the specified 100\n",
      "Created a chunk of size 312, which is longer than the specified 100\n",
      "Created a chunk of size 515, which is longer than the specified 100\n",
      "Created a chunk of size 584, which is longer than the specified 100\n",
      "Created a chunk of size 1119, which is longer than the specified 100\n",
      "Created a chunk of size 257, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 136, which is longer than the specified 100\n",
      "Created a chunk of size 230, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 134, which is longer than the specified 100\n",
      "Created a chunk of size 224, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 142, which is longer than the specified 100\n",
      "Created a chunk of size 293, which is longer than the specified 100\n",
      "Created a chunk of size 148, which is longer than the specified 100\n",
      "Created a chunk of size 4288, which is longer than the specified 100\n",
      "Created a chunk of size 450, which is longer than the specified 100\n",
      "Created a chunk of size 125, which is longer than the specified 100\n",
      "Created a chunk of size 133, which is longer than the specified 100\n",
      "Created a chunk of size 462, which is longer than the specified 100\n",
      "Created a chunk of size 178, which is longer than the specified 100\n",
      "Created a chunk of size 469, which is longer than the specified 100\n",
      "Created a chunk of size 432, which is longer than the specified 100\n",
      "Created a chunk of size 380, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 134, which is longer than the specified 100\n",
      "Created a chunk of size 132, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 142, which is longer than the specified 100\n",
      "Created a chunk of size 293, which is longer than the specified 100\n",
      "Created a chunk of size 148, which is longer than the specified 100\n",
      "Created a chunk of size 4288, which is longer than the specified 100\n",
      "Created a chunk of size 403, which is longer than the specified 100\n",
      "Created a chunk of size 146, which is longer than the specified 100\n",
      "Created a chunk of size 136, which is longer than the specified 100\n",
      "Created a chunk of size 267, which is longer than the specified 100\n",
      "Created a chunk of size 203, which is longer than the specified 100\n",
      "Created a chunk of size 541, which is longer than the specified 100\n",
      "Created a chunk of size 120, which is longer than the specified 100\n",
      "Created a chunk of size 300, which is longer than the specified 100\n",
      "Created a chunk of size 324, which is longer than the specified 100\n",
      "Created a chunk of size 281, which is longer than the specified 100\n",
      "Created a chunk of size 305, which is longer than the specified 100\n",
      "Created a chunk of size 333, which is longer than the specified 100\n",
      "Created a chunk of size 457, which is longer than the specified 100\n",
      "Created a chunk of size 421, which is longer than the specified 100\n",
      "Created a chunk of size 528, which is longer than the specified 100\n",
      "Created a chunk of size 154, which is longer than the specified 100\n",
      "Created a chunk of size 592, which is longer than the specified 100\n",
      "Created a chunk of size 633, which is longer than the specified 100\n",
      "Created a chunk of size 128, which is longer than the specified 100\n",
      "Created a chunk of size 235, which is longer than the specified 100\n",
      "Created a chunk of size 303, which is longer than the specified 100\n",
      "Created a chunk of size 658, which is longer than the specified 100\n",
      "Created a chunk of size 155, which is longer than the specified 100\n",
      "Created a chunk of size 236, which is longer than the specified 100\n",
      "Created a chunk of size 520, which is longer than the specified 100\n",
      "Created a chunk of size 172, which is longer than the specified 100\n",
      "Created a chunk of size 162, which is longer than the specified 100\n",
      "Created a chunk of size 438, which is longer than the specified 100\n",
      "Created a chunk of size 333, which is longer than the specified 100\n",
      "Created a chunk of size 581, which is longer than the specified 100\n",
      "Created a chunk of size 183, which is longer than the specified 100\n",
      "Created a chunk of size 432, which is longer than the specified 100\n",
      "Created a chunk of size 208, which is longer than the specified 100\n",
      "Created a chunk of size 242, which is longer than the specified 100\n",
      "Created a chunk of size 574, which is longer than the specified 100\n",
      "Created a chunk of size 204, which is longer than the specified 100\n",
      "Created a chunk of size 105, which is longer than the specified 100\n",
      "Created a chunk of size 741, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n",
      "Created a chunk of size 232, which is longer than the specified 100\n",
      "Created a chunk of size 118, which is longer than the specified 100\n",
      "Created a chunk of size 142, which is longer than the specified 100\n",
      "Created a chunk of size 293, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI_bge-large-en-v1.5\n",
      "BAAI_bge-small-en-v1.5\n",
      "hkunlp_instructor-xl\n",
      "sentence-transformers_all-MiniLM-L6-v2\n",
      "sentence-transformers_all-mpnet-base-v2\n",
      "sentence-transformers_clip-ViT-B-32\n",
      "sentence-transformers_multi-qa-mpnet-base-dot-v1\n",
      "thenlper_gte-large\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/torch/sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c8324b70601d://root/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2 /home/rob/Data3/sentence_transformers\n",
    "# Successfully copied 439MB to c8324b70601d:///root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PromptTemplate and LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='could start cutting into his workload. Furthermore, his rest of the season\\nschedule isn’t fantasy-friendly. Try to flip Edwards and a WR3 for Kenneth\\nWalker or Tony Pollard', metadata={'source': 'https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/'}),\n",
       "  Document(page_content='“ **Gus Edwards** has been on fire lately. He is the RB1 over the past three\\nweeks, averaging 22.2 half-point PPR fantasy points and two rushing touchdowns\\nper game. However, over 54% of his fantasy production came from the six\\nrushing touchdowns. Meanwhile, the veteran averaged only 6.5 fantasy points\\nper game over the first six contests. He had more than six fantasy points only\\nonce, in the Week 2 matchup where Edwards found the end zone. The veteran\\nrunning back is a touchdown-or-bust player, and Keaton Mitchell', metadata={'source': 'https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/'}),\n",
       "  Document(page_content='* Raheem Mostert or James Cook\\n  * Jahmyr Gibbs or Kyren Williams\\n  * Devin Singletary or Jerome Ford', metadata={'source': 'https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/'}),\n",
       "  Document(page_content='* Raheem Mostert or James Cook\\n  * Jahmyr Gibbs or Kyren Williams\\n  * Devin Singletary or Jerome Ford', metadata={'source': 'https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/'})],\n",
       " 'question': 'Should I start Gibbs next week for fantasy?',\n",
       " 'text': \" Based on the provided context, there is no clear answer regarding starting Jahmyr Gibbs next week for fantasy as there is no mention of his upcoming schedule or recent performance in the documents. However, the documents do suggest considering trades involving Gus Edwards and other running backs such as Raheem Mostert, James Cook, Devin Singletary, and Jerome Ford. If you have the opportunity to trade Edwards for one of these players, it may be worth considering if you believe their upcoming schedules and recent performances make them better options for your fantasy team. Ultimately, the decision to start Gibbs should be based on your current roster, upcoming matchups, and personal assessment of each player's potential performance.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Should I start Gibbs next week for fantasy?\")\n",
    "\n",
    "# 9.3s\n",
    "# 6690 MiB VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
