{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monday, February 5, 2024\n",
    "\n",
    "Had some CUDA Toolkit headaches, and NVIDIA driver issues, so I am running this just to see if anything is now broke because of this.\n",
    "\n",
    "Yup, now we have problems with 'AutoModelForCausalLM.from_pretrained' ... sigh ...\n",
    "\n",
    "Nice! It's fixed ... at least the loading part is now working ... have not tested the remainder of this notebook.\n",
    "\n",
    "\n",
    "#### Saturday, January 13, 2024\n",
    "\n",
    "[Samantha and Mistral 7B: A Powerful and Versatile Language Model Duo](https://huggingface.co/blog/Andyrasika/samantha-and-mistral-7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monday, February 5, 2024 ... added this code to address model load problems ...\n",
    "\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses the model \"ehartford/samantha-mistral-7b\" and yet when you try to go to that model page on hugging face, you get redirected to \"cognitivecomputations/samantha-mistral-7b\" ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--HuggingFaceH4--zephyr-7b-beta\n",
      "models--Intel--neural-chat-7b-v3-1\n",
      "models--NousResearch--Llama-2-7b-chat-hf\n",
      "models--bert-base-uncased\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--ehartford--samantha-mistral-7b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mlabonne--NeuralBeagle14-7B\n",
      "models--sentence-transformers--msmarco-MiniLM-L-12-v3\n",
      "models--teknium--OpenHermes-2.5-Mistral-7B\n",
      "models--unsloth--mistral-7b-bnb-4bit\n",
      "version.txt\n",
      "version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c8324b70601d://root/.cache/huggingface/hub/models--ehartford--samantha-mistral-7b /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 15GB to /home/rob/Data3/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Name = \"ehartford/samantha-mistral-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd38ddbd334fabbe5d06e7fa86de64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7404095ee67f4df1a47e51f2f74d0082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monday, February 5, 2024 ... this started failing today ...\n",
    "#    these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
    "#                         `device_map` to `from_pretrained`. Check\n",
    "#                         https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
    "#                         for more details.\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_Name,\n",
    "#                                               load_in_8bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                               torch_dtype=torch.float16,\n",
    "#                                               low_cpu_mem_usage=True,\n",
    "#                                               )\n",
    "\n",
    "# Hmm nice! That one tweak worked ... change 'auto' to device\n",
    "model = AutoModelForCausalLM.from_pretrained(model_Name,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map=device,\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True,\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
