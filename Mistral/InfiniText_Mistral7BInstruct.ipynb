{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thursday, November 9, 2023\n",
    "\n",
    "[InfiniText: Empowering Conversations & Content with Mistral-7B-Instruct-v0.1](https://huggingface.co/blog/Andyrasika/mistral-7b-empowering-conversation) (Published October 13, 2023)\n",
    "\n",
    "This all runs in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoProcessor\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac5740e4461408392d4c9391e847dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--mistralai--Mistral-7B-Instruct-v0.1 /home/rob/Data3/huggingface/transformers/\n",
    "# Successfully copied 15GB to /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import textwrap\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "  lines = text.split('\\n')\n",
    "  wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "  wrapped_text = '\\n'.join(wrapped_lines)\n",
    "  return wrapped_text\n",
    "\n",
    "def multimodal_prompt(input_text:str, system_prompt=\"\",max_length=512) -> str:\n",
    "  \"\"\"few-shot text-to-text prompting\n",
    "\n",
    "  Generates text using a large language model, given a prompt and a device.\n",
    "\n",
    "  Args:\n",
    "    model: An AutoModelForCausalLM instance.\n",
    "    tokenizer: An AutoTokenizer instance.\n",
    "    prompt: The prompt to use for generation.\n",
    "    device: The device to use for generation.\n",
    "\n",
    "  Returns:\n",
    "    A string containing the generated text.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"<s>[INST]{input_text}[/INST]\"\"\"\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "  output = model.generate(**model_inputs,\n",
    "                                max_length=max_length,\n",
    "                                use_cache=True,\n",
    "                                early_stopping=True,\n",
    "                                bos_token_id=model.config.bos_token_id,\n",
    "                                eos_token_id=model.config.eos_token_id,\n",
    "                                pad_token_id=model.config.eos_token_id,\n",
    "                                temperature=0.1,\n",
    "                               do_sample=True)\n",
    "\n",
    "# Randomly select one of the generated outputs\n",
    "  response = random.choice(tokenizer.batch_decode(output))\n",
    "\n",
    "  # Wrap the response text to a width of 90 characters\n",
    "  wrapped_response = wrap_text(response)\n",
    "  print(wrapped_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]Write a detailed analogy between mathematics and a lighthouse.[/INST]\n",
      "Mathematics can be compared to a lighthouse in many ways. Just as a lighthouse guides\n",
      "ships safely to shore by emitting a steady light and warning of dangerous waters,\n",
      "mathematics provides a framework for understanding and navigating the complexities of the\n",
      "world around us.\n",
      "\n",
      "The light emitted by a lighthouse can be seen for miles, guiding ships even in the darkest\n",
      "of conditions. Similarly, the principles of mathematics can be applied in a wide range of\n",
      "situations, from simple calculations to complex scientific and technological problems.\n",
      "\n",
      "Just as a lighthouse must be carefully maintained and updated to remain effective,\n",
      "mathematics must also be constantly refined and expanded to keep pace with new discoveries\n",
      "and technologies.\n",
      "\n",
      "Furthermore, just as a lighthouse serves as a beacon of hope and guidance for sailors,\n",
      "mathematics can provide a sense of order and predictability in an often chaotic world. By\n",
      "using mathematical models and equations, we can make predictions and plan for the future\n",
      "with greater accuracy and confidence.\n",
      "\n",
      "In short, mathematics and a lighthouse are both essential tools for navigating and\n"
     ]
    }
   ],
   "source": [
    "multimodal_prompt('Write a detailed analogy between mathematics and a lighthouse.',\n",
    "         max_length=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]Alice: I don't know why, I'm struggling to maintain focus while studying. Any\n",
      "suggestion?\n",
      "\n",
      " Bob:[/INST] Bob: Have you tried breaking your study sessions into smaller, more focused\n",
      "intervals? This technique is called the Pomodoro Technique and it can help you maintain\n",
      "focus and increase productivity. You can also try taking short breaks between study\n",
      "sessions to give your brain a break and recharge. Additionally, make sure you're studying\n",
      "in a comfortable and distraction-free environment.</s>\n"
     ]
    }
   ],
   "source": [
    "multimodal_prompt(\"\"\"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion? \\n\\n Bob:\"\"\",max_length=128)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
