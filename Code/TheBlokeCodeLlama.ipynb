{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday, November 17, 2023\n",
    "\n",
    "Been doing some model cleanup on the hfpt_Oct28 container, and want to re-run this just to ensure all is still well ...\n",
    "\n",
    "Yup! This still works! Nice!\n",
    "\n",
    "### Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "\n",
    "# (base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# (base) rob@KAUWITB:~$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ  version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4d4b8cb4b44e8881ebf57b26eab3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    }
   ],
   "source": [
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Hello! I'd be happy to help you with that.\n",
      "\n",
      "To solve this problem, we can use a combination of natural language processing (NLP) and machine learning techniques. Here's an example of how we can approach this problem:\n",
      "\n",
      "1. First, we need to preprocess the text data. We can use a library like NLTK or spaCy to tokenize the text, remove stop words, and stem the words.\n",
      "2. Next, we need to extract the key phrases from the text. We can use a technique like Latent Dirichlet Allocation (LDA) to extract the topics from the text.\n",
      "3. Once we have the key phrases, we can use a machine learning algorithm like Naive Bayes or Logistic Regression to classify the phrases into categories like \"AI\", \"Machine Learning\", \"Deep Learning\", etc.\n",
      "4. Finally, we can use a library like Plotly to visualize the results. We can create a bar chart or a pie chart to show the distribution of the phrases in each category.\n",
      "\n",
      "Here's some sample code to illustrate this approach:\n",
      "```python\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import LatentDirichletAllocation\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.metrics import accuracy_score\n",
      "import plotly.graph_objs as go\n",
      "\n",
      "# Load the data\n",
      "with open(\"data.txt\", \"r\") as f:\n",
      "    text = f.read()\n",
      "\n",
      "# Preprocess the text data\n",
      "tokens = word_tokenize(text)\n",
      "tokens = [word.lower() for word in tokens if word.isalpha() and not word.isnumeric()]\n",
      "tokens = [word for word in tokens if word not in set(stopwords.words(\"english\"))]\n",
      "tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
      "\n",
      "# Create a TF-IDF vectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "\n",
      "# Vectorize the text data\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here's one possible solution:\n",
      "```\n",
      "def tell_me_about_ai(input_string):\n",
      "    if \"AI\" in input_string:\n",
      "        return \"Artificial intelligence (AI) refers to the ability of machines to perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems.\"\n",
      "    else:\n",
      "        return \"I'm not sure what you mean by 'AI'. Could you explain?\"\n",
      "```\n",
      "This function checks if the string \"AI\" is present in the input string. If it is, it returns a definition of artificial intelligence. Otherwise, it asks for further clarification.\n",
      "\n",
      "Example test cases:\n",
      "```\n",
      "print(tell_me_about_ai(\"What is AI?\"))\n",
      "# Output: Artificial intelligence (AI) refers to the ability of machines to perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems.\n",
      "\n",
      "print(tell_me_about_ai(\"Can you explain AI to me?\"))\n",
      "# Output: Sure! Artificial intelligence (AI) refers to the ability of machines to perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems.\n",
      "\n",
      "print(tell_me_about_ai(\"What do you know about AI?\"))\n",
      "# Output: I'm not sure what you mean by 'AI'. Could you explain?\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
