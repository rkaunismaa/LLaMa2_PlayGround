{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    }
   ],
   "source": [
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here is one possible solution to the coding problem:\n",
      "```\n",
      "def tell_me_about_ai(prompt):\n",
      "    # Initialize an empty list to store the answers\n",
      "    answers = []\n",
      "\n",
      "    # Tokenize the prompt into individual words\n",
      "    words = prompt.split()\n",
      "\n",
      "    # Iterate over the words and check if they are in the list of AI-related words\n",
      "    for word in words:\n",
      "        if word in AI_WORDS:\n",
      "            # If the word is in the list, add it to the list of answers\n",
      "            answers.append(word)\n",
      "\n",
      "    # Return the list of answers\n",
      "    return answers\n",
      "\n",
      "# Define a list of AI-related words\n",
      "AI_WORDS = [\"artificial\", \"intelligence\", \"machine\", \"learning\", \"deep\", \"learning\", \"neural\", \"networks\", \"computer\", \"vision\", \"natural\", \"language\", \"processing\"]\n",
      "\n",
      "# Test the function with some example prompts\n",
      "print(tell_me_about_ai(\"What is artificial intelligence?\"))\n",
      "# Output: [\"artificial\", \"intelligence\"]\n",
      "\n",
      "print(tell_me_about_ai(\"What is machine learning?\"))\n",
      "# Output: [\"machine\", \"learning\"]\n",
      "\n",
      "print(tell_me_about_ai(\"What is deep learning?\"))\n",
      "# Output: [\"deep\", \"learning\"]\n",
      "\n",
      "print(tell_me_about_ai(\"What is computer vision?\"))\n",
      "# Output: [\"computer\", \"vision\"]\n",
      "\n",
      "print(tell_me_about_ai(\"What is natural language processing?\"))\n",
      "# Output: [\"natural\", \"language\", \"processing\"]\n",
      "```\n",
      "This code defines a function called `tell_me_about_ai` that takes a prompt as input and returns a list of words that are related to AI. The function first tokenizes the prompt into individual words, and then iterates over the words to check if they are in the list of AI-related words. If a word is found in the list, it is added to the list of answers. Finally, the function returns the list of answers.\n",
      "\n",
      "The code also defines a list of AI-related words called `AI_WORDS`, which is used by the `tell_me_about_ai` function to check if a word is related to AI\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here is a possible solution in Python 3:\n",
      "```python\n",
      "def tell_me_about_ai(query):\n",
      "    # Check if the query contains any of the keywords for AI\n",
      "    keywords = [\"artificial intelligence\", \"machine learning\", \"deep learning\", \"neural networks\"]\n",
      "    for keyword in keywords:\n",
      "        if keyword in query:\n",
      "            return True\n",
      "    return False\n",
      "```\n",
      "This function takes a string `query` as input, and checks if it contains any of the keywords for AI. If it does, it returns `True`, otherwise it returns `False`.\n",
      "\n",
      "To pass the example test cases, we can call the function with different queries:\n",
      "```python\n",
      "assert tell_me_about_ai(\"What is artificial intelligence?\") == True\n",
      "assert tell_me_about_ai(\"Explain machine learning.\") == True\n",
      "assert tell_me_about_ai(\"Define deep learning.\") == True\n",
      "assert tell_me_about_ai(\"Describe neural networks.\") == True\n",
      "assert tell_me_about_ai(\"What are some applications of artificial intelligence?\") == False\n",
      "assert tell_me_about_ai(\"How do you build a chatbot?\") == False\n",
      "```\n",
      "Note that the first four assertions should return `True`, while the last two should return `False`.\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
