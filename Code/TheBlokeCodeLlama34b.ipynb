{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuesday, January 2, 2024\n",
    "\n",
    "Trying this again ... using the hfpt_Dec14 container.\n",
    "\n",
    "Yup! It still works!\n",
    "\n",
    "### Wednesday, December 27, 2023\n",
    "\n",
    "Trying this again ... using the hfpt_Dec14 container. \n",
    "\n",
    "I am going to torch all the models in the /root/.cache/huggingface/hub directory to see how much space gets freed. \n",
    "\n",
    "Data2 is at 355.3 GB currently ... now let's clear away all the current models and see what we gain ... hmm odd, nothing seems to have been gained ... sigh.\n",
    "\n",
    "Right .... have to run \n",
    "\n",
    "1) pip install auto-gptq \n",
    "2) pip install optimum\n",
    "\n",
    "\n",
    "### Friday, November 17, 2023\n",
    "\n",
    "Been doing some model cleanup on the hfpt_Oct28 container, and want to re-run this just to ensure all is still well ...\n",
    "\n",
    "Yup! This still works! Nice!\n",
    "\n",
    "### Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--microsoft--Orca-2-13b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c8324b70601d://root/.cache/huggingface/hub\n",
    "# Successfully copied 18.3GB to c8324b70601d://root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--microsoft--Orca-2-13b\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "\n",
    "# (base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# (base) rob@KAUWITB:~$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ  version.txt\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 15.1s 18740 MiB VRAM ... why less than before??\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "The prompt is asking for a description of AI, so here is a possible answer:\n",
      "\n",
      "AI stands for artificial intelligence, which refers to the ability of a computer or machine to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision making, and language translation. AI systems can analyze data, identify patterns, and make predictions or decisions based on that data. They can also learn and improve their performance over time, much like humans do.\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as playing chess or recognizing faces.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can, such as reasoning, problem-solving, and learning.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds.\n",
      "\n",
      "The development of AI has many applications, such as:\n",
      "\n",
      "1. Healthcare: AI can help doctors and hospitals to analyze medical data, diagnose diseases, and develop personalized treatment plans.\n",
      "2. Finance: AI can help financial institutions to detect fraud, analyze market trends, and make investment decisions.\n",
      "3. Transportation: AI can improve the safety and efficiency of transportation systems, such as by developing autonomous vehicles.\n",
      "\n",
      "However, AI also raises ethical and societal concerns, such as:\n",
      "\n",
      "1. Bias: AI systems can perpetuate biases and discrimination if they are trained on biased data or designed with a particular worldview.\n",
      "2. Job displacement: AI has the potential to automate many jobs currently performed by humans, which could lead to job displacement and economic disruption.\n",
      "3. Privacy: AI systems often require access to large amounts of personal data, which raises concerns about privacy and data protection.\n",
      "\n",
      "Overall, AI has the potential to bring about significant benefits, but it also requires careful consideration of its ethical and societal implications.</s>\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 20.0s 18974 MiB VRAM\n",
    "\n",
    "# 6.1s 18856 MiB VRAM\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "The first line of this problem is a classic trick question. The correct answer is \"AI is not AI\". Here's why:\n",
      "\n",
      "\"AI\" stands for \"Artificial Intelligence\", which refers to the ability of a computer or machine to perform tasks that typically require human intelligence, such as learning, reasoning, and decision-making. However, the term \"AI\" does not refer to any specific algorithm or technique used to achieve these tasks. It is simply a label that encompasses a wide range of approaches and techniques.\n",
      "\n",
      "Therefore, when asked to write code that solves the problem \"Tell me about AI\", it is impossible to do so without being vague or unspecific. The question is too broad and open-ended, and there are many different ways to approach it.\n",
      "\n",
      "One possible solution would be to write a program that generates text based on a set of predefined rules or heuristics. For example, the program could start with a list of keywords related to AI, such as \"machine learning,\" \"natural language processing,\" \"computer vision,\" etc., and then generate random sentences or paragraphs based on those keywords. However, the output would likely be nonsensical or irrelevant, and would not provide any meaningful information about AI.\n",
      "\n",
      "Another possible solution would be to use natural language processing (NLP) techniques to analyze and understand the nuances of the question itself, and then generate an appropriate response. For example, the program could identify the keyword \"me\" in the question, and realize that the user is asking for personalized information about AI. The program could then generate a response that is tailored to the user's interests and background, such as explaining the basics of machine learning or discussing the current state of AI research in a particular field.\n",
      "\n",
      "Overall, the question \"Tell me about AI\" is too vague and open-ended to be solved by any specific piece of code. It requires a more general and flexible approach, such as generating text based on predefined rules or analyzing the nuances of the question using NLP techniques.\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 18.6s 18974 MiB VRAM\n",
    "\n",
    "# 7.2s 18878 MiB VRAM\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
