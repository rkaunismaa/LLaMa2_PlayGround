{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday, December 27, 2023\n",
    "\n",
    "Trying this again ... using the hfpt_Dec14 container. \n",
    "\n",
    "I am going to torch all the models in the /root/.cache/huggingface/hub directory to see how much space gets freed. \n",
    "\n",
    "Data2 is at 355.3 GB currently ... now let's clear away all the current models and see what we gain ... hmm odd, nothing seems to have been gained ... sigh.\n",
    "\n",
    "Right .... have to run \n",
    "\n",
    "1) pip install auto-gptq \n",
    "2) pip install optimum\n",
    "\n",
    "\n",
    "### Friday, November 17, 2023\n",
    "\n",
    "Been doing some model cleanup on the hfpt_Oct28 container, and want to re-run this just to ensure all is still well ...\n",
    "\n",
    "Yup! This still works! Nice!\n",
    "\n",
    "### Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c8324b70601d://root/.cache/huggingface/hub\n",
    "# Successfully copied 18.3GB to c8324b70601d://root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ  version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "\n",
    "# (base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# (base) rob@KAUWITB:~$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ  version.txt\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 15.1s 18740 MiB VRAM ... why less than before??\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "```\n",
      "def solve(n):\n",
      "    # Write your code here\n",
      "    return \"AI is a rapidly growing field that holds great promise for the future. It has many applications in various industries such as healthcare, finance, education, and transportation.\"\n",
      "\n",
      "\n",
      "# Test cases\n",
      "def test_solve():\n",
      "    assert solve(1) == \"AI is a rapidly growing field that holds great promise for the future. It has many applications in various industries such as healthcare, finance, education, and transportation.\"\n",
      "\n",
      "\n",
      "# Run test cases\n",
      "test_solve()\n",
      "```</s>\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 6.1s 18856 MiB VRAM\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "```\n",
      "def tell_me_about_AI(topic):\n",
      "    if topic == \"\":\n",
      "        return \"I can tell you about many topics related to AI, such as machine learning, natural language processing, computer vision, and robotics. If you have a specific question or topic in mind, please let me know and I'll do my best to provide a detailed response.\"\n",
      "    else:\n",
      "        return f\"I'm not able to provide information on the topic of '{topic}'. However, I can try to help you find more information on this topic by providing suggestions for books, articles, or other resources that may be relevant.\"\n",
      "\n",
      "# Example usage:\n",
      "print(tell_me_about_AI(\"\"))\n",
      "print(tell_me_about_AI(\"machine learning\"))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 7.2s 18878 MiB VRAM\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
