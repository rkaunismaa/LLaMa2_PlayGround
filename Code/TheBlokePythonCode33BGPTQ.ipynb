{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monday, December 4, 2023\n",
    "\n",
    "Trying it again ...\n",
    "\n",
    "pip install exllamav2\n",
    "\n",
    "docker container start hfpt_Oct28\n",
    "\n",
    "### Sunday, December 3, 2023\n",
    "\n",
    "Downloaded the \"TheBloke/Python-Code-33B-GPTQ\" model yesterday ... ran a quick test this morning. It works!\n",
    "\n",
    "### Friday, November 17, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/Python-Code-33B-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Python-Code-33B-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ  tmpcjh0h7gn\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ        tmpzafytbf_\n",
      "models--TheBloke--Python-Code-33B-GPTQ\t       version.txt\n",
      "models--meta-llama--Llama-2-13b-hf\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# This loads the model into CPU ram for inference \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# hf = HuggingFacePipeline.from_model_id(\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     model_id=model_name_or_path,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# This takes up all CPU ram, then fill up ALL swap space ... yeah, this does not work!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Also, I stopped it at 6 minutes cuz it was gonna fill up the swap space ... and that never ends well.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m hf \u001b[39m=\u001b[39m HuggingFacePipeline\u001b[39m.\u001b[39;49mfrom_model_id(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     pipeline_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m512\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/Code/TheBlokePythonCode33BGPTQ.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/langchain/llms/huggingface_pipeline.py:99\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m         model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_model_kwargs)\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m task \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mtext2text-generation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    101\u001b[0m         model \u001b[39m=\u001b[39m AutoModelForSeq2SeqLM\u001b[39m.\u001b[39mfrom_pretrained(model_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_model_kwargs)\n",
      "File \u001b[0;32m/transformers/src/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m/transformers/src/transformers/modeling_utils.py:3152\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3149\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[39m=\u001b[39mtorch_dtype, device_map\u001b[39m=\u001b[39mdevice_map)\n\u001b[1;32m   3151\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 3152\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   3154\u001b[0m \u001b[39m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:1088\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[1;32m   1087\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m LlamaModel(config)\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m   1090\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:944\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask_converter \u001b[39m=\u001b[39m AttnMaskConverter(is_causal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 944\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    945\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    947\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:944\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask_converter \u001b[39m=\u001b[39m AttnMaskConverter(is_causal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[0;32m--> 944\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([LlamaDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    945\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    947\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:748\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[1;32m    743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m (\n\u001b[1;32m    744\u001b[0m     LlamaAttention(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m    745\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39m_flash_attn_2_enabled\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    746\u001b[0m     \u001b[39melse\u001b[39;00m LlamaFlashAttention2(config\u001b[39m=\u001b[39mconfig)\n\u001b[1;32m    747\u001b[0m )\n\u001b[0;32m--> 748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m LlamaMLP(config)\n\u001b[1;32m    749\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    750\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm \u001b[39m=\u001b[39m LlamaRMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/transformers/src/transformers/models/llama/modeling_llama.py:349\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[1;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mintermediate_size\n\u001b[0;32m--> 349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_size, bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/init.py:419\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    417\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This loads the model into CPU ram for inference \n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_name_or_path,\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 512},\n",
    "# )\n",
    "\n",
    "# This too loads it to CPU ram \n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_name_or_path,\n",
    "#     task=\"text-generation\",\n",
    "#     device=0,\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 512},\n",
    "# )\n",
    "\n",
    "# This too loads it to CPU ram \n",
    "# hf = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=model_name_or_path,\n",
    "#     task=\"text-generation\",\n",
    "#     device_map=\"auto\",\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 512},\n",
    "# )\n",
    "\n",
    "# This takes up all CPU ram, then fill up ALL swap space ... yeah, this does not work!\n",
    "# Also, I stopped it at 6 minutes cuz it was gonna fill up the swap space ... and that never ends well.\n",
    "# I also tried this without running CUDA_VISIBLE_DEVICES ... same problem.\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name_or_path,\n",
    "    task=\"text-generation\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 11.2 s, total: 31.9 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             # use_exllama_v2=True, ... this does not work\n",
    "                                             revision=\"main\")\n",
    "\n",
    "\n",
    "# Load time\n",
    "# CPU times: user 19.9 s, sys: 9.32 s, total: 29.3 s\n",
    "# Wall time: 12.4 s\n",
    "# 18424MiB VRAM\n",
    "# Memory goes from 4.9GB to 10.8GB ... why??\n",
    "\n",
    "# Download time\n",
    "# CPU times: user 1min 32s, sys: 1min 39s, total: 3min 12s\n",
    "# Wall time: 4h 37s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--Python-Code-33B-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 16.9GB to /home/rob/Data3/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''This is a conversation with your helpful AI assistant. AI assistant can generate Python Code along with necessary explanation.\n",
    "\n",
    "Context\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "'''\n",
    "\n",
    "# print(\"\\n\\n*** Generate:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "<s> This is a conversation with your helpful AI assistant. AI assistant can generate Python Code along with necessary explanation.\n",
      "\n",
      "Context\n",
      "You are a helpful AI assistant.\n",
      "\n",
      "USER: Tell me about AI\n",
      "ASSISTANT:\n",
      "\n",
      "Artificial Intelligence (AI) is a subfield of computer science that studies intelligent agents. An intelligent agent is a program that can make decisions and take actions based on its environment. \n",
      "\n",
      "AI researchers study how to make machines intelligent by replicating the processes of the brain. They use techniques such as machine learning, pattern recognition, and natural language processing to achieve this. \n",
      "\n",
      "AI is a vast field with many applications. Some common applications include:\n",
      "\n",
      "1. Machine Learning: This subfield of AI studies how machines can learn from data and improve their performance. It is used in many practical applications such as spam filtering, recommendation systems, and self-driving cars.\n",
      "\n",
      "2. Pattern Recognition: This subfield studies how machines can recognize patterns in data. It is used in applications such as facial recognition, spam filtering, and medical imaging.\n",
      "\n",
      "3. Natural Language Processing: This subfield studies how machines can understand and generate natural language. It is used in applications such as language translation, text analysis, and chatbots.\n",
      "\n",
      "4. Reinforcement Learning: This subfield studies how machines can learn to behave in an environment by trial and error. It is used in applications such as robotics, drones, and video games.\n",
      "\n",
      "5. Neural Networks: This subfield studies how machines can be trained to perform tasks using a network of artificial neurons. It is used in applications such as image recognition, language processing, and autonomous driving.\n",
      "\n",
      "AI is a rapidly evolving field with many exciting developments. Some examples include:\n",
      "\n",
      "1. Deep Learning: This subfield uses deep neural networks to achieve state-of<s>the-art performance in tasks such as image recognition and language processing.\n",
      "\n",
      "2. Genetic Algorithms: These algorithms use a population-based approach to optimize solutions to problems. They are used in applications such as optimization, genetics, and machine learning.\n",
      "\n",
      "3. Logic Programming: This field studies how to represent and reason with knowledge using logic. It is used in applications such as natural language processing and expert systems.\n",
      "\n",
      "4. Convolutional Neural Networks: These networks use a layered architecture to process data. They are used in applications such as image recognition and language processing.\n",
      "\n",
      "5. Graphical Models: These models use a graph-based approach to represent and reason with data. They are used in applications such as machine learning, computer vision, and natural language processing.\n",
      "\n",
      "AI is a fascinating field that is transforming the way we interact with machines and how machines interact with each other. It is already having a significant impact on our lives, and its potential is only just beginning to be realized.</s>\n",
      "CPU times: user 27.8 s, sys: 483 ms, total: 28.3 s\n",
      "Wall time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxTokens = 2048\n",
    "output2048 = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=maxTokens)\n",
    "output = output2048[0]\n",
    "print(len(output))\n",
    "print(tokenizer.decode(output))\n",
    "\n",
    "# This cell starts at 18424 MiB VRAM ... goes up to 19524 MiB\n",
    "# 9.9s ... WHY does this time vary by so much?? This was fast!\n",
    "# It varies based on how many tokens get generated for the output ...\n",
    "# 708 ... 32.9s ...\n",
    "# 371 ... 16.1s ...\n",
    "# 595 ... 26.1s ...\n",
    "# 405 ... 17.0s ...\n",
    "# 632 ... 28.4s ...\n",
    "\n",
    "\n",
    "# 38.4s\n",
    "# 20.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "tfPipeLine = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a conversation with your helpful AI assistant. AI assistant can generate Python Code along with necessary explanation.\n",
      "\n",
      "Context\n",
      "You are a helpful AI assistant.\n",
      "\n",
      "USER: Tell me about AI\n",
      "ASSISTANT:\n",
      "Artificial Intelligence (AI) is a field of computer science that studies intelligent agents. An intelligent agent is a program that can make decisions and take actions on its own, without human intervention. \n",
      "\n",
      "AI assistants, such as Siri, Alexa, and Google Assistant, have become common in recent years. These assistants use natural language processing (NLP) to understand human speech and respond accordingly. They can perform tasks like searching the internet, playing music, and controlling other devices in the home.\n",
      "\n",
      "In addition to consumer-level AI assistants, there are also advanced AI systems being developed for use in industries like finance, healthcare, and transportation. These systems use machine learning algorithms to analyze large amounts of data and make predictions about future outcomes. For example, in healthcare, AI systems can analyze patient data and identify patterns that humans may miss, leading to more accurate diagnoses and treatments.\n",
      "\n",
      "Overall, AI technology is advancing rapidly, and it is becoming more and more integrated into our everyday lives.\n"
     ]
    }
   ],
   "source": [
    "print(tfPipeLine(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 21106 MiB VRAM\n",
    "# 20324 MiB VRAM\n",
    "# 19906 MiB VRAM\n",
    "\n",
    "# 18.7s\n",
    "# 15.0s\n",
    "# 12.9s \n",
    "# 15.2s\n",
    "# 10.9s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try this with [LangChain](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines), shall we ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline as LCHuggingFacePipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the transformers pipeline we defined above ... \n",
    "lchfpl = LCHuggingFacePipeLine(pipeline=tfPipeLine, verbose=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the replicated code from a previous cell ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "\n",
    "prompt_template=f'''This is a conversation with your helpful AI assistant. AI assistant can generate Python Code along with necessary explanation.\n",
    "\n",
    "Context\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "USER: {prompt}\n",
    "ASSISTANT:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate as LCPromptTemplate\n",
    "\n",
    "lcPromptTemplate = LCPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = lcPromptTemplate | lchfpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = chain.invoke({\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) is a subfield of computer science that studies intelligent agents. An intelligent agent is a system that perceives its environment and takes actions to achieve specific goals. \n",
      "\n",
      "In the past few decades, AI has made significant progress in various fields such as speech recognition, image processing, and machine learning. These advances have enabled AI systems to perform tasks that were previously considered to be the domain of humans, such as playing chess or identifying faces.\n",
      "\n",
      "However, there are still many challenges to overcome before AI systems can match the full range of human capabilities. One such challenge is understanding language, which is essential for interacting with humans. Another challenge is improving the efficiency of the algorithms used by AI systems, so that they can handle larger amounts of data and make more accurate predictions.\n",
      "\n",
      "Overall, the field of AI is growing rapidly, and it is expected to have a significant impact on many aspects of our lives in the future.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
