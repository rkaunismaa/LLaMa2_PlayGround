{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday, December 2, 2023\n",
    "\n",
    "Re ran this with the \"TheBloke/CodeLlama-34B-Instruct-GPTQ\" model, and it works! Nice! 20298MiB VRAM!\n",
    "\n",
    "### Friday, November 17, 2023\n",
    "\n",
    "Been doing some model cleanup on the hfpt_Oct28 container, and want to re-run this just to ensure all is still well ...\n",
    "\n",
    "Yup! This still works! Nice!\n",
    "\n",
    "So THIS notebook was just copied from TheBlokeCodeLlama.ipynb and I want to use this to download the 13B model and compare it against the 34B model ...\n",
    "\n",
    "So the 13B model has been downloaded and works just fine! NOTE THIS NOTEBOOK DOES NOT USE LANGCHAIN!\n",
    "\n",
    "### Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-13B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 7.26GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "# (base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# (base) rob@KAUWITB:~$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started with all of these models below ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--EleutherAI--gpt-neox-20b\t     models--mosaicml--mpt-7b-instruct\n",
      "models--codellama--CodeLlama-7b-hf\t     tmp_35pwa_q\n",
      "models--meta-llama--Llama-2-13b-chat-hf      tmphxgsvifp\n",
      "models--meta-llama--Llama-2-13b-hf\t     tmpjiz9wrho\n",
      "models--meta-llama--Llama-2-7b-chat-hf\t     tmppceppekl\n",
      "models--meta-llama--Llama-2-7b-hf\t     tmpt7cv3q8c\n",
      "models--mistralai--Mistral-7B-Instruct-v0.1  version.txt\n",
      "models--mistralai--Mistral-7B-v0.1\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I am going to whack them all ... The 2TB disk, where we store the docker images and containers, is currently at 1.3TB free (34.9% full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/rob/Data2/huggingface/transformers': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the transformers directory ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are at 1.4TB free (27.6% free)\n",
    "\n",
    "Now copy in just the model we want to work with, \"TheBloke/CodeLlama-34B-Instruct-GPTQ\" ...\n",
    "\n",
    "(base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now at 1.4TB free, 28.5% full ... I am really glad I cloned that 1TB drive to this 2TB drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "\n",
    "# Let's go with the smaller model, shall we ... \n",
    "# model_name_or_path = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
    "\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 12.5 s, total: 39 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM\n",
    "\n",
    "# First time to download ...\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
    "# CPU times: user 43.8 s, sys: 43.2 s, total: 1min 26s\n",
    "# Wall time: 1h 42min 31s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "The task is to write a program that generates a description of AI.\n",
      "\n",
      "Here's an example of how the program might work:\n",
      "\n",
      "```\n",
      "#include <iostream>\n",
      "#include <string>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "int main() {\n",
      "  string description = \"Artificial intelligence (AI) refers to the ability of a computer or machine to perform tasks that typically require human intelligence, such as understanding natural language, recognizing images, making decisions, and solving problems. AI systems use a combination of algorithms and statistical models to analyze data and make predictions or decisions. The field of AI is rapidly growing, with applications in industries such as healthcare, finance, and transportation.\";\n",
      "\n",
      "  cout << description << endl;\n",
      "\n",
      "  return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "This program generates a description of AI using a string variable. The description is then printed to the console using the `cout` statement.\n",
      "\n",
      "To test this program, you can compile and run it on your computer. If the program runs successfully and produces the expected output, it has passed the test case.\n",
      "\n",
      "Note that this program is just one possible way to solve the problem. There may be other approaches that could also be used to generate a description of AI.</s>\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 17.3s 10128MiB VRAM\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Hello! I'm just an AI, my purpose is to assist users with information and tasks. I am trained on a wide range of texts and can generate human-like responses to questions and prompts. My capabilities include understanding natural language, generating text, and answering questions. I can be used in a variety of applications such as customer service, language translation, and content generation. However, it's important to note that I am not perfect and may make mistakes or not fully understand the context of certain questions or prompts.\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 10.1s 10128MiB VRAM\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
