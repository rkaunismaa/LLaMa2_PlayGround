{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friday, November 17, 2023\n",
    "\n",
    "Been doing some model cleanup on the hfpt_Oct28 container, and want to re-run this just to ensure all is still well ...\n",
    "\n",
    "Yup! This still works! Nice!\n",
    "\n",
    "So THIS notebook was just copied from TheBlokeCodeLlama.ipynb and I want to use this to download the 13B model and compare it against the 34B model ...\n",
    "\n",
    "So the 13B model has been downloaded and works just fine! NOTE THIS NOTEBOOK DOES NOT USE LANGCHAIN!\n",
    "\n",
    "### Thursday, November 16, 2023\n",
    "\n",
    "https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "\n",
    "Let's see if we can get this to run locally, shall we ... \n",
    "\n",
    "I am trying this model because it has the most downloads of all his models, AND of all the huggingface models that are 'code' related.\n",
    "\n",
    "Wow! This actually works on the 4090! I am stunned! 20524MiB of VRAM!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 18.3GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "# docker cp c9b676310ea0://home/rob/Data2/huggingface/transformers/models--TheBloke--CodeLlama-13B-Instruct-GPTQ /home/rob/Data3/huggingface/transformers\n",
    "# Successfully copied 7.26GB to /home/rob/Data3/huggingface/transformers\n",
    "\n",
    "# (base) rob@KAUWITB:~$ docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--CodeLlama-34B-Instruct-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 18.3GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# (base) rob@KAUWITB:~$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this to just target the 4090, othewise the 34B model does NOT load.\n",
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-13B-Instruct-GPTQ  version.txt\n",
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775f31769b4e4deb82b010a2e5270246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94af05d5f3084869940041ea24bed9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9217844125c4754a7fc77b5734e4187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6975f5fb0d4c31a91325955a4b3c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "\n",
    "# Let's go with the smaller model, shall we ... \n",
    "model_name_or_path = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
    "\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01134535159d439a82cb0722c349c95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5732839274374a1e8d7679a0688ddb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddff0ce751349578be190c73887f260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 s, sys: 43.2 s, total: 1min 26s\n",
      "Wall time: 1h 42min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "# 16.7s to load, takes up 20132Mib VRAM\n",
    "\n",
    "# First time to download ...\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
    "# CPU times: user 43.8 s, sys: 43.2 s, total: 1min 26s\n",
    "# Wall time: 1h 42min 31s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here is a possible solution to the coding problem:\n",
      "```\n",
      "import re\n",
      "\n",
      "def tell_me_about_ai(text):\n",
      "    \"\"\"\n",
      "    Return a string describing the topic of the text.\n",
      "\n",
      "    :param text: The text to analyze.\n",
      "    :type text: str\n",
      "    :return: A string describing the topic of the text.\n",
      "    :rtype: str\n",
      "    \"\"\"\n",
      "\n",
      "    # Remove punctuation and convert to lowercase\n",
      "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
      "\n",
      "    # Tokenize the text into individual words\n",
      "    words = text.split()\n",
      "\n",
      "    # Create a dictionary to store the word frequencies\n",
      "    word_freq = {}\n",
      "\n",
      "    # Iterate over the words and update the word frequencies\n",
      "    for word in words:\n",
      "        if word in word_freq:\n",
      "            word_freq[word] += 1\n",
      "        else:\n",
      "            word_freq[word] = 1\n",
      "\n",
      "    # Find the most common word\n",
      "    most_common_word = max(word_freq, key=word_freq.get)\n",
      "\n",
      "    # Return a string describing the topic of the text\n",
      "    return f\"The text is about {most_common_word}.\"\n",
      "\n",
      "# Example test cases\n",
      "assert tell_me_about_ai(\"The quick brown fox jumps over the lazy dog.\") == \"The text is about brown.\"\n",
      "assert tell_me_about_ai(\"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\") == \"The text is about brown.\"\n",
      "assert tell_me_about_ai(\"The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\") == \"The text is about brown.\"\n",
      "```\n",
      "This code uses the `re` module to remove punctuation and convert the text to lowercase, and then uses the `split()` method to tokenize the text into individual words. It then creates a dictionary to store the word frequencies and iterates over the words to update the word frequencies. Finally, it finds the most common word and returns a string describing the topic of the text.\n",
      "\n",
      "The example test cases are included to demonstrate the functionality of the code.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "# # To use a different branch, change revision\n",
    "# # For example: revision=\"main\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "#                                              device_map=\"auto\",\n",
    "#                                              trust_remote_code=False,\n",
    "#                                              revision=\"main\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# # Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 17.3s 10128MiB VRAM\n",
    "\n",
    "# 22.5s 20524Mib VRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n",
      "[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
      "Tell me about AI\n",
      "[/INST]\n",
      "\n",
      "Here's a possible solution:\n",
      "```\n",
      "def tell_me_about_AI(message):\n",
      "    if message == \"What is AI?\":\n",
      "        return \"Artificial intelligence (AI) refers to the ability of machines and computers to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\"\n",
      "    elif message == \"What are some examples of AI?\":\n",
      "        return \"Examples of AI include virtual assistants like Siri or Alexa, self-driving cars, and language translation software.\"\n",
      "    else:\n",
      "        return \"Sorry, I don't know what you mean by '{}'. Could you rephrase your question?\".format(message)\n",
      "```\n",
      "This function takes in a string `message` representing the user's input, and returns a string with the appropriate response. The function uses an `if...elif...else` statement to check the value of `message`, and returns a different response depending on whether it matches one of the three possible inputs.\n",
      "\n",
      "Note that this function assumes that the user will only ask one question at a time, and that the questions will be asked in a specific format (e.g., \"What is X?\" or \"What are Y?\"). If you want to handle more complex user input or multiple questions at once, you can modify the function accordingly.\n"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n",
    "# 10.1s 10128MiB VRAM\n",
    "\n",
    "# 11.9s 20524Mib VRAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
