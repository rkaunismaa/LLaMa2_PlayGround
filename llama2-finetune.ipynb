{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sunday, November 19, 2023\n",
    "\n",
    "I am going to look into the datasets found at [LLMDataHub: Awesome Datasets for LLM Training](https://github.com/Zjh-819/LLMDataHub) \n",
    "\n",
    "Let's go with [this](https://huggingface.co/datasets/THUDM/webglm-qa) dataset. This data has been loaded into the data/webglm-qa folder.\n",
    "\n",
    "Ok Nice! I got this to run!\n",
    "\n",
    "Let's try this on \"meta-llama/Llama-2-13b-hf\" shall we ... Nice! It also works!\n",
    "\n",
    "### Friday, November 17, 2023\n",
    "\n",
    "[How to fine-tune Llama 2 on your own data](https://brev.dev/blog/fine-tuning-llama-2-your-own-data)\n",
    "\n",
    "https://github.com/brevdev/notebooks/blob/main/llama2-finetune.ipynb\n",
    "\n",
    "This notebook uses \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "Wow Really!? I need to create my own dataset!? ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--meta-llama--Llama-2-7b-chat-hf\n",
      "models--meta-llama--Llama-2-7b-hf\n",
      "models--mistralai--Mistral-7B-Instruct-v0.1\n",
      "models--mistralai--Mistral-7B-v0.1\n",
      "tmpjiz9wrho\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 13.5GB to c9b676310ea0://home/rob/Data2/huggingface/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ... \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> •\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> •\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> •\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tuning Llama 2 7B on your own data 🤙\n",
    "\n",
    "Welcome!\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune Meta's [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b).\n",
    "\n",
    "## Watch the accompanying video walk-through (but for Mistral) [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)! If you'd like to see that notebook instead, click [here](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb).\n",
    "\n",
    "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
    "\n",
    "In this notebook, we will load the large model in 4bit using `bitsandbytes` and use LoRA to train using the PEFT library from Hugging Face 🤗.\n",
    "\n",
    "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
    "\n",
    "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/NVDyv7TUgJ) or on [X](https://x.com/harperscarroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we begin: A note on OOM errors\n",
    "\n",
    "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/NVDyv7TUgJ) or on [X](https://x.com/harperscarroll).\n",
    "\n",
    "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin!\n",
    "I used a GPU and dev environment from [brev.dev](https://brev.dev). Provision a pre-configured GPU in one click [here](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&name=llama2-7b-finetune) (a single A10G or L4 should be enough for this dataset; anything with >= 24GB GPU Memory. You may need more GPUs and/or Memory if your sequence max_length is larger than 512). Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10** and CUDA 11.7) and click the \"Build\" button to build your Verb container. Give this a few minutes.\n",
    "\n",
    "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
    "\n",
    "\n",
    "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuXIFTFapAMI"
   },
   "outputs": [],
   "source": [
    "# # You only need to run this once per machine\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### 0. Accelerator\n",
    "\n",
    "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcE4NTeFyRgd"
   },
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCc64bfnmd3j"
   },
   "source": [
    "Here's where you load your own data. You want the data formatted in a `.jsonl` file, structured something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data\n",
    "\n",
    "To prepare your dataset for loading, all you need is a `.jsonl` file structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will combine all your features into one input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataFolder = 'data/webglm-qa/'\n",
    "\n",
    "# train_dataset = load_dataset('json', data_files='notes.jsonl', split='train')  \n",
    "# eval_dataset = load_dataset('json', data_files='notes_validation.jsonl', split='train')\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=dataFolder + 'train.jsonl', split='train')  \n",
    "eval_dataset = load_dataset('json', data_files=dataFolder + 'validation.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what measures do film makers take to protect child actors in horror films or emotionally distressing scenes?',\n",
       " 'answer': 'Directors must be mindful of a child actor’s well-being and take care to shield the children they are working with from some darker themes of the material[1]. They usually try and avoid exposing the very young actors to the violence of such scenes, e.g. by clever dubbing and editing[3]. There are laws that require them to do so, such as if a child is not comfortable with what they will be exposed with (such as mock rape and murder scenes)[4], the director can not under law be allowed to use that child for that particular scene. Laws also protect children from certain flavors of exploitation, particularly of a sexual nature.',\n",
       " 'references': ['Not all stories about childhood are suitable for children. It is not uncommon for filmmakers to explore darker aspects of childhood in their films. Although telling these stories is important, if adequate protection measures aren’t taken, child actors can sustain psychological harm. Directors must be mindful of a child actor’s well-being and take care to shield the children they are working with from some darker themes of the material.',\n",
       "  'Linda has said in numerous interviews that she had no idea what she was doing at the time. But it begs the question: can starring in a horror movie as a child actor have an emotional effect during filming or even once the cameras stop rolling?',\n",
       "  'Do the directors usually try and avoid exposing the very young actors to the violence of such scenes, e.g. by clever dubbing and editing? Are there laws that require them to do so, just like there are ratings that prevent children from seeing some movies?',\n",
       "  'If a child is not comfortable with what they will be exposed with (such as mock rape and murder scenes), the director can not under law be allowed to use that child for that particular scene.',\n",
       "  \"That isn't to say that a kid can do anything on a movie set. Laws protect children from certain flavors of exploitation, particularly of a sexual nature.\"]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting prompts\n",
    "Then create a formatting_func to structure training examples as prompts. In my case, my data was just notes like this:\n",
    "\n",
    "```json\n",
    "{\"note\": \"note-for-model-to-predict\"}\n",
    "{\"note\": \"note-for-model-to-predict-1\"}\n",
    "{\"note\": \"note-for-model-to-predict-2\"}\n",
    "```\n",
    "So the formatting_func I used was:\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    #text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Llama 2 7B - `meta-llama/Llama-2-7b-hf` - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E0Nl5mWL0k2T"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718df96dbdfe4b85a4093c70baa96456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Let's try the bigger model ...\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "haSUDD9HyRgf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d088e8a351af4ea98458d56d5bdfb454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3436dd8bf204012a1941c3c1109aef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44579\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs. \n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512 # This was an appropriate max length for my dataset\n",
    "\n",
    "max_length = 300 # Looking at the graph, I am going to choose ...\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4191e11742644141a4c92abbb14b67fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2314efd679964bd1a7fa0935d61ad421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 894, 29901, 3750, 526, 1422, 260, 4285, 313, 15227, 529, 7145, 529, 5188, 1974, 29897, 310, 10489, 29915, 26094, 4359, 2337, 29871, 29896, 29900, 274, 1237, 1422, 29973, 13, 835, 673, 29901, 450, 29871, 29896, 29900, 1644, 4328, 1546, 278, 1422, 260, 4285, 310, 10489, 26094, 338, 5517, 2861, 304, 263, 15687, 7841, 491, 10489, 16355, 29892, 607, 338, 5517, 2309, 304, 1207, 278, 26094, 6775, 363, 20330, 304, 2274, 322, 6456, 29889, 450, 29871, 29896, 29900, 1644, 4328, 947, 451, 12695, 9432, 278, 3935, 4328, 297, 11029, 1546, 278, 1422, 4072, 310, 10489, 29892, 408, 5188, 1974, 10489, 338, 6133, 19468, 26413, 393, 508, 3438, 701, 304, 29871, 29945, 29900, 274, 1237, 901, 639, 11798, 265, 29892, 1550, 10489, 393, 28103, 7488, 29088, 20801, 338, 901, 19390, 491, 871, 29871, 29941, 274, 1237, 639, 11798, 265, 29961, 29941, 1822, 7579, 304, 4797, 4759, 1179, 29892, 278, 8666, 4328, 1546, 4943, 322, 5188, 1974, 10489, 26496, 338, 14235, 29871, 29906, 29900, 304, 29871, 29906, 29945, 10151, 29892, 470, 29871, 29945, 29900, 274, 1237, 639, 11798, 265, 29961, 29946, 1402, 322, 278, 17261, 1546, 5188, 1974, 322, 4943, 10489, 756, 1063, 28325, 2354, 20493, 1951, 29871, 29906, 29900, 29900, 29929, 29892, 411, 278, 1556, 8541, 2454, 7910, 13920, 292, 297, 278, 1833, 1023, 2440, 29961, 29945, 1822, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44579\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Llama 2 7B does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\" Given the following biometric data, score the users' health, from 0-100.\n",
    "\n",
    "### Biometric Data:\n",
    "Temperature=98.2,\n",
    "Sex=F,\n",
    "Age=29,\n",
    "Height=69 inches,\n",
    "Weight=160 lbs,\n",
    "V02_Max=55,\n",
    "HRV=55\n",
    "\n",
    "### Health Score:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `eval_prompt` I used was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \" The following is a note by Eevee the Dog: # \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NidIuFXMyRgi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The following is a note by Eevee the Dog: #  # 维瑟罗\n",
      "\n",
      "维瑟罗（法語：，法语发音：[vizɛʁo]；奧克語：）是法国埃罗省的一个市镇，属于蒙彼利埃区。\n",
      "\n",
      "## 地理\n",
      "\n",
      "维瑟罗（43°32'15\"N, 3°47'14\"E）面积10.93 平方千米，位于法国奧克西塔尼大區埃罗省，该省份为法国南部沿海省份，南濒地中海，西南接奥德省，西北接塔恩省和阿韦龙省，东北与加尔省接壤。\n",
      "\n",
      "与维瑟罗接壤的市镇（或旧市镇、城区）包括：��\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ybeyl20n3dYH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 126361600 || all params: 6798341120 || trainable%: 1.8587122618524914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 5120)\n",
      "        (layers): ModuleList(\n",
      "          (0-39): 40 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (o_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              )\n",
      "              (up_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=13824, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              )\n",
      "              (down_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=13824, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "              )\n",
      "              (act_fn): SiLUActivation()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=5120, out_features=32000, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobkayinto\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples, so I used only 500 steps. I found that the end product worked well.\n",
    "\n",
    "A note on training. You can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting - the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`llama2-7b-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"llama2-7b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "       # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/wandb/run-20231119_131717-i74gm6fk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/robkayinto/journal-finetune/runs/i74gm6fk' target=\"_blank\">llama2-7b-journal-finetune-2023-11-19-13-16</a></strong> to <a href='https://wandb.ai/robkayinto/journal-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/robkayinto/journal-finetune' target=\"_blank\">https://wandb.ai/robkayinto/journal-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/robkayinto/journal-finetune/runs/i74gm6fk' target=\"_blank\">https://wandb.ai/robkayinto/journal-finetune/runs/i74gm6fk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 25:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.637317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.616449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.609803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.606593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.602623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.601166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.598701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.597555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.596501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.622300</td>\n",
       "      <td>1.596344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run time of job \"EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-11-19 13:20:03 UTC)\" was missed by 0:00:01.257845\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Run time of job \"EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-11-19 13:22:33 UTC)\" was missed by 0:00:01.833954\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Run time of job \"EmissionsTracker._measure_power (trigger: interval[0:00:15], next run at: 2023-11-19 13:32:48 UTC)\" was missed by 0:00:01.113827\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 48s, sys: 7min 3s, total: 24min 52s\n",
      "Wall time: 25min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.6223255615234375, metrics={'train_runtime': 1528.6316, 'train_samples_per_second': 0.654, 'train_steps_per_second': 0.327, 'total_flos': 2.3361094656e+16, 'train_loss': 1.6223255615234375, 'epoch': 0.02})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 25m 31.8s on 13B model\n",
    "# 14m 15.6s on 7B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM ... \n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base Llama 2 7B model from the Huggingface Hub:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SKSnF016yRgp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03df31bc61a41f38a4822729c07c298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7B, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"llama2-7b-journal-finetune/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lMkVNEUvyRgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The following is a note by Eevee the Dog, which doesn't share anything too personal: # 1: I love my family very much. # 2: I love my friends very much. # 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2: I love my friends very much.\n",
      "# 3: I love my dog very much.\n",
      "# 1: I love my family very much.\n",
      "# 2:\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" The following is a note by Eevee the Dog, which doesn't share anything too personal: # \"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Llama 2 on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/NVDyv7TUgJ).\n",
    "\n",
    "🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
