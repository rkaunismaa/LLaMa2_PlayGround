{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday, November 7, 2023\n",
    "\n",
    "Ran this with the \"meta-llama/Llama-2-13b-hf\" model ...\n",
    "\n",
    "#### Sunday, November 5, 2023\n",
    "\n",
    "[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)\n",
    "\n",
    "The current blog post does not run here without a lot of changes ... with the below changes, it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 13.5GB to c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 26GB to c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf\n",
    "# rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-cased\n",
      "models--decapoda-research--llama-13b-hf\n",
      "models--facebook--bart-large-cnn\n",
      "models--google--pegasus-cnn_dailymail\n",
      "models--gpt2\n",
      "models--gpt2-xl\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "models--meta-llama--Llama-2-7b-hf\n",
      "models--mistralai--Mistral-7B-v0.1\n",
      "models--openai-gpt\n",
      "models--t5-large\n",
      "models--transformersbook--pegasus-samsum\n",
      "models--xlm-roberta-base\n",
      "tmp560c_s3e\n",
      "tmp8is86yg8\n",
      "tmpea0zbrrj\n",
      "tmplm91b70r\n",
      "tmpni3ccozw\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added this next line that was not part of the blog post code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf80a37c181a43b28798798d70077f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# \"meta-llama/Llama-2-13b-hf\" => 13,948Mib VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(model, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from blog ...\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 392 ms, total: 21.6 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "\n",
    "# model = \"meta-llama/Llama-2-7b-hf\"\n",
    "# CPU times: user 21.2 s, sys: 392 ms, total: 21.6 s\n",
    "# Wall time: 21.7 s\n",
    "\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "# CPU times: user 28.8 s, sys: 938 ms, total: 29.8 s\n",
    "# Wall time: 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "I have never seen \"Breaking Bad\" but I have heard good things about it. I haven't seen \"Band of Brothers\" yet, but I'm sure it is good.\n",
      "I'm going to be honest and say that I don't know many other shows. I'm not really a TV person. I'm more of a movie person. But I have heard good things about \"Friday Night Lights\". I've also heard that \"Mad Men\" is good.\n",
      "I'm going to check out \"Breaking Bad\" and \"Band of Brothers\" and see if I like them. If I like them, I'll let you know.\n",
      "I've been thinking about starting to watch \"Breaking Bad\" and \"Band of Brothers\" myself. I've\n"
     ]
    }
   ],
   "source": [
    "# model = \"meta-llama/Llama-2-7b-hf\"\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "I don't know if you'd like \"The Wire\" or not. It's a cop show, but it's very different than most cop shows. It's more of a social commentary, and the characters are very realistic.\n",
      "I've heard \"The Wire\" is very good. I'll have to check it out.\n",
      "I've been watching \"The Wire\" and it's really good. I'm on season 2 now.\n",
      "I've heard \"The Wire\" is very good. I'll have to check it out. I've been watching \"The Wire\" and it's really good. I'm on season 2 now.\n",
      "I'm not sure if you'd like \"The Wire\" or not.\n"
     ]
    }
   ],
   "source": [
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
