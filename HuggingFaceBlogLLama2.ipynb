{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friday, December 15, 2023\n",
    "\n",
    "Trying this notebook in the hfpt_Dec14 container. \n",
    "\n",
    "Yup it still works.\n",
    "\n",
    "A more current version of this blog post is availalbe [here](https://github.com/huggingface/blog/blob/main/llama2.md)\n",
    "\n",
    "#### Tuesday, November 7, 2023\n",
    "\n",
    "Ran this with the \"meta-llama/Llama-2-13b-hf\" model ...\n",
    "\n",
    "#### Sunday, November 5, 2023\n",
    "\n",
    "[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)\n",
    "\n",
    "The current blog post does not run here without a lot of changes ... with the below changes, it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-7b-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 13.5GB to c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-7b-chat-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-13b-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 26GB to c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf c9b676310ea0:/home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-7b-hf\n",
    "# rm -rf /home/rob/Data2/huggingface/transformers/models--meta-llama--Llama-2-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--bert-base-uncased  models--meta-llama--Llama-2-13b-hf  version.txt\n"
     ]
    }
   ],
   "source": [
    "# !ls /home/rob/Data2/huggingface/transformers/\n",
    "!ls /root/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added this next line that was not part of the blog post code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecc78607cbf42ea8844b1a86865f709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# \"meta-llama/Llama-2-13b-hf\" => 13,948Mib VRAM\n",
    "model = AutoModelForCausalLM.from_pretrained(model, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from blog ...\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 586 ms, total: 29.2 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "# CPU times: user 28.6 s, sys: 586 ms, total: 29.2 s\n",
    "# Wall time: 29.4 s\n",
    "\n",
    "# model = \"meta-llama/Llama-2-7b-hf\"\n",
    "# CPU times: user 21.2 s, sys: 392 ms, total: 21.6 s\n",
    "# Wall time: 21.7 s\n",
    "\n",
    "# model = \"meta-llama/Llama-2-13b-hf\"\n",
    "# CPU times: user 28.8 s, sys: 938 ms, total: 29.8 s\n",
    "# Wall time: 30 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "I've been watching \"The Sopranos\" and \"The Wire\". I also like \"The Office\" and \"It's Always Sunny in Philadelphia\".\n",
      "I'm watching \"The Wire\" and \"The Sopranos\" now. I'm also watching \"The Office\" and \"It's Always Sunny in Philadelphia\".\n",
      "I'm watching \"The Sopranos\" and \"The Wire\" now. I'm also watching \"The Office\" and \"It's Always Sunny in Philadelphia\".\n",
      "I'm watching \"The Wire\" and \"The Sopranos\" now. I'm also watching \"The Office\" and \"It's Always Sunny in Philadelphia\".\n",
      "I'm watching \"The Sopranos\" and \"The W\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
