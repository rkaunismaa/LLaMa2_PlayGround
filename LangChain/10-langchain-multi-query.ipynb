{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wednesday, November 22, 2023\n",
        "\n",
        "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb\n",
        "\n",
        "docker container start hfpt_Nov20\n",
        "\n",
        "Start => OpenAI usage is at $1.47\n",
        "\n",
        "Finish => OpenAI usage is at $1.63\n",
        "\n",
        "This all runs.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/08-langchain-multi-query.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XDGL6Oi6h4"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# LangChain Multi-Query for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qi8B1fgywJzE"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU \\\n",
        "#   pinecone-client==2.2.4 \\\n",
        "#   langchain==0.0.321 \\\n",
        "#   datasets==2.14.6 \\\n",
        "#   openai==0.28.1 \\\n",
        "#   tiktoken==0.5.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CPmfrdJ9_2YA"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Py-rVqx-I0"
      },
      "source": [
        "We will download an existing dataset from Hugging Face Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iatOGmKgz8NE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 41584\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data\n",
        "\n",
        "# 2m 10.3s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P7E6JYtb0cW7"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = []\n",
        "\n",
        "for row in data:\n",
        "    doc = Document(\n",
        "        page_content=row[\"chunk\"],\n",
        "        metadata={\n",
        "            \"title\": row[\"title\"],\n",
        "            \"source\": row[\"source\"],\n",
        "            \"id\": row[\"id\"],\n",
        "            \"chunk-id\": row[\"chunk-id\"],\n",
        "            \"text\": row[\"chunk\"]\n",
        "        }\n",
        "    )\n",
        "    docs.append(doc)\n",
        "\n",
        "    # 7.9s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yb540kEs_6PZ"
      },
      "source": [
        "## Embedding and Vector DB Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKEmBZMBxtd"
      },
      "source": [
        "Initialize our embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qZ6vTiDPBznz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = \"text-embedding-ada-002\"\n",
        "\n",
        "# get openai api key from platform.openai.com\n",
        "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
        "# OPENAI_API_KEY = getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# embed = OpenAIEmbeddings(\n",
        "#     model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get openai api key from platform.openai.com\n",
        "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IurEkeeI-IYl"
      },
      "source": [
        "Create our Pinecone index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nL3KFF9E9Qb_"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "import time\n",
        "\n",
        "index_name = \"langchain-multi-query-demo\"\n",
        "\n",
        "# # find API key in console at app.pinecone.io\n",
        "# YOUR_API_KEY = os.getenv('PINECONE_API_KEY') or getpass(\"Pinecone API Key: \")\n",
        "# # find ENV (cloud region) next to API key in console\n",
        "# YOUR_ENV = os.getenv('PINECONE_ENVIRONMENT') or input(\"Pinecone Env: \")\n",
        "\n",
        "# find API key in console at app.pinecone.io\n",
        "YOUR_API_KEY = getpass(\"Pinecone API Key: \")\n",
        "# find ENV (cloud region) next to API key in console\n",
        "YOUR_ENV = input(\"Pinecone Env: \")\n",
        "\n",
        "# pinecone.init(\n",
        "#     api_key=YOUR_API_KEY,\n",
        "#     environment=YOUR_ENV\n",
        "# )\n",
        "\n",
        "# if index_name not in pinecone.list_indexes():\n",
        "#     # we create a new index\n",
        "#     pinecone.create_index(\n",
        "#         name=index_name,\n",
        "#         metric='cosine',\n",
        "#         dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "#     )\n",
        "#     # wait for index to be initialized\n",
        "#     while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
        "#         time.sleep(1)\n",
        "\n",
        "# # now connect to index\n",
        "# index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "pinecone.init(\n",
        "    api_key=YOUR_API_KEY,\n",
        "    environment=YOUR_ENV\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pinecone.describe_index(index_name).status[\"ready\"]:\n",
        "        time.sleep(1)\n",
        "\n",
        "# 15.8s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now connect to index\n",
        "index = pinecone.Index(index_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A3B7dHsd6QcP"
      },
      "source": [
        "Populate our index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B7Yi2YGBpTWf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41584"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "thfCYHuSpW4H"
      },
      "outputs": [],
      "source": [
        "# Yeah, we want to do this ... !\n",
        "# if you want to speed things up to follow along\n",
        "docs = docs[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HXVVU97C6SwT"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b41ea28c353b47f9acf70ba8f14ca59a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 10.3 s, sys: 253 ms, total: 10.6 s\n",
            "Wall time: 27min 15s\n"
          ]
        }
      ],
      "source": [
        "%%time \n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from uuid import uuid4\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(docs), batch_size)):\n",
        "    i_end = min(len(docs), i+batch_size)\n",
        "    docs_batch = docs[i:i_end]\n",
        "    # get IDs\n",
        "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
        "    # get text and embed\n",
        "    texts = [d.page_content for d in docs_batch]\n",
        "    embeds = embed.embed_documents(texts=texts)\n",
        "    # get metadata\n",
        "    metadata = [d.metadata for d in docs_batch]\n",
        "    to_upsert = zip(ids, embeds, metadata)\n",
        "    index.upsert(vectors=to_upsert)\n",
        "\n",
        "# CPU times: user 10.3 s, sys: 253 ms, total: 10.6 s\n",
        "# Wall time: 27min 15s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8FbngTBzAAU-"
      },
      "source": [
        "## Multi-Query with LangChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YVVYr13n_Ot2"
      },
      "source": [
        "Now we switch across to using our populated index as a vectorstore in Langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ETs0emsAh-K",
        "outputId": "0b1de24b-2f9f-48a6-d8ca-bd3d6aa007e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/langchain/vectorstores/pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "vectorstore = Pinecone(index, embed.embed_query, text_field)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nW_GCB6a3_N_"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1iptBAriANrD"
      },
      "source": [
        "We initialize the `MultiQueryRetriever`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yYjztBp2ANHC"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8qZCd1TAMAn"
      },
      "source": [
        "We set logging so that we can see the queries as they're generated by our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rgV1eYU6FgX7"
      },
      "outputs": [],
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jrjwkpJWAaAn"
      },
      "source": [
        "To query with our multi-query retriever we call the `get_relevant_documents` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJ4cSJXFinV",
        "outputId": "265900d1-6aa7-4d28-cbbe-e2e95b7df7b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What information can you provide about llama 2?', '2. Could you give me some details about llama 2?', '3. I would like to learn more about llama 2. Can you help me with that?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 44.6 ms, sys: 0 ns, total: 44.6 ms\n",
            "Wall time: 7.37 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "question = \"tell me about llama 2?\"\n",
        "\n",
        "docs = retriever.get_relevant_documents(query=question)\n",
        "len(docs)\n",
        "\n",
        "# CPU times: user 44.6 ms, sys: 0 ns, total: 44.6 ms\n",
        "# Wall time: 7.37 s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kSu1GsFfAqCd"
      },
      "source": [
        "From this we get a variety of docs retrieved by each of our queries independently. By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return `6` unique docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce5WBh6MFltP",
        "outputId": "f7b06949-e2a6-472e-eaf9-e712dc4bcca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Q:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6\\nmonths. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.\\nQ:Yes or no: Would a pear sink in water?\\nA:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,\\napear would ﬂoat. So the answer is no.\\nTable 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.\\nPROMPT FOR DATE UNDERSTANDING\\nQ:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\\nA:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday\\nis12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.', metadata={'chunk-id': '137', 'id': datetime.date(2201, 11, 22), 'source': 'http://arxiv.org/pdf/2201.11903', 'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'}),\n",
              " Document(page_content='knowledge.\\nINVARIANT LAMA We create I NVARIANT LAMA, a subset of the LAMA (Petroni et al., 2019)\\ntask for measuring time-invariant knowledge which might be forgotten during CKL. Among the\\n41 relations of the T-REx (Elsahar et al., 2018) subset of LAMA, we manually select 28 relation\\ntypes that probe for time-invariant instances (a full list of time-invariant relations are provided in\\nAppendix B.1). We also remove instances where the answer overlapped with the subject following\\nPoerner et al. (2019) since the answers for these instances can be inferred from the cloze statement\\nitself. Lastly, we remove instances where the answer was a non-entity to leave only the instances\\nthat require world knowledge for prediction on their answers (Guu et al., 2020).\\nUPDATED LAMA and N EWLAMA We construct U PDATED LAMA and N EWLAMA for measuring the update of outdated knowledge and acquisition of new knowledge during CKL. The challenge of constructing U PDATED LAMA is that a knowledge instance can be only considered as the\\nknowledge that requires update only if it is present in both D0andD1with changed details, and\\nthe challenge of constructing N EWLAMA is that the knowledge can be considered new only if it is', metadata={'chunk-id': '59', 'id': datetime.date(2110, 11, 22), 'source': 'http://arxiv.org/pdf/2110.03215', 'title': 'Towards Continual Knowledge Learning of Language Models'}),\n",
              " Document(page_content='PROMPT FOR STRATEGY QA\\nQ:Do hamsters provide food for any animals?\\nA:Ham sters areprey animals. Prey arefood forpreda tors. Thus, hamsters provide food forsome animals. So\\nthe answer is yes.\\nQ:Could Brooke Shields succeed at University of Pennsylvania?\\nA:Brooke Shields went toPrince tonUniversity. Prince tonUniversityisabout asacademically rigorousasthe\\nUniversityofPenn sylvania.Thus, Brooke Shields could alsosucceed attheUniversityofPenn sylvania. So the\\nanswer is yes.\\nQ:Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nA:Hydrogenhasanatomic numberof1.1squared is1.There are5Spice Girls. Thus, Hydrogen’s atomic\\nnumbersquared islessthan 5. So the answer is no.\\nQ:Yes or no: Is it common to see frost during some college commencements?\\nA:College commence ment ceremonies canhappeninDecember,May, andJune. Decemberisinthewinter,so\\nthere canbefrost. Thus, there could befrost atsome commence ments. So the answer is yes.\\nQ:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?', metadata={'chunk-id': '136', 'id': datetime.date(2201, 11, 22), 'source': 'http://arxiv.org/pdf/2201.11903', 'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'}),\n",
              " Document(page_content='and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:\\n//github.com/tatsu-lab/stanford_alpaca .\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\\nDhanasekaran, Atharva Naik, David Stap, Eshaan\\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, IshanPurohit,IshaniMondal,JacobAnderson,Kirby\\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\\nPulkitVerma,RavsehajSinghPuri,RushangKaria,\\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.', metadata={'chunk-id': '54', 'id': datetime.date(2212, 11, 22), 'source': 'http://arxiv.org/pdf/2212.10560', 'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions'}),\n",
              " Document(page_content='UPDATED LAMAis the prime minister of England. Theresa May →\\nBoris Johnson\\nhas the most passing yards in the NFL. Brady Quinn →\\nJalen Guyton\\nBale has champions league titles with3→4Real Madrid.\\nNEWLAMAAlicia Braga plays in the New Mutant. Cecilia Reyes\\nowns the rights to the Falcon and theDisneyWinter Soldier.\\nTesla invested in the digital currency bitcoin. 1.5 billion\\nNEWLAMA-E ASYThe decision of the two volleyball stars Bria and Cimone\\nHoward University Woodard to withdraw from the Power 5 School to study\\nat has become a national story.\\nAllen Lazard is ofﬁcially listed as questionable with asixnuclear injury after missing the last games.\\nC E XPERIMENTAL CONFIGURATION\\nPretraining Congifuration We utilize the T5 initially pretrained on C4 (April 2019) and continually pretrained with salient span masking (Guu et al., 2020) on Wikipedia (May 2020) as initialization. We use the checkpoints from Wolf et al. (2020). We also perform the SSM objective during\\nCKL because it was shown to help LMs “focus on problems that require world knowledge” (Guu\\net al., 2020; Roberts et al., 2020).', metadata={'chunk-id': '70', 'id': datetime.date(2110, 11, 22), 'source': 'http://arxiv.org/pdf/2110.03215', 'title': 'Towards Continual Knowledge Learning of Language Models'}),\n",
              " Document(page_content='Variant I: Fact selection. The ﬁrst variant of\\nour approach involves selecting facts from the outputs of LLMs based on external knowledge. For\\nexample, consider the running example of “ Did\\nAristotle use a laptop? ”, where we only have access to the ﬁrst two reasoning paths, R1andR2.\\nIn this case, the ﬁrst sentence in R2and the second sentence in R1are supported by knowledge\\nK1andK2, respectively. Therefore, the ﬁrst variant would output the ﬁrst sentence in R2and the\\nsecond sentence in R1as the supporting facts.\\nVariant II: Fact generation. The second variant of our approach involves generating facts\\nbased on both the outputs of LLMs and external\\nknowledge. For example, consider the running example of “ Did Aristotle use a laptop? ”, where we\\nonly have access to the ﬁrst reasoning path R1.\\nThe second sentence in R1is supported by the second knowledge paragraph K2. However, the ﬁrst\\nsentence is not supported by any evidence paragraphs. We can generate questions about the ﬁrst\\nsentence, such as “When did Aristotle die?” and\\nuse the ﬁrst knowledge paragraph K1to generate', metadata={'chunk-id': '25', 'id': datetime.date(2301, 11, 22), 'source': 'http://arxiv.org/pdf/2301.00303', 'title': 'Rethinking with Retrieval: Faithful Large Language Model Inference'}),\n",
              " Document(page_content='45\\nUser : Nice to meet you, I’m Joe\\nLaMDA : Hello Joe. How can I best assist you?\\nUser : Can you suggest a good dessert?\\nLaMDA : If you’re like me and like to try something new, I recommend a good old fashioned, traditional\\npie. They’re easy to make, very tasty, and a deﬁnite crowd pleaser.\\nUser : What kind of pie is easy to make?\\nLaMDA : Key lime pie and coconut cream pie are both very easy to make. They’re also extremely delicious.\\nUser : How do I make a key lime pie?\\nLaMDA : You can get an easy pie crust at any supermarket. Mix the following ingredients together: 5 egg\\nyolks 3/4 cup sugar 1 14 oz can of sweetened condensed milk 1 teaspoon of lime zest Mix in the\\nfood processor, pour into the pie crust, bake on 350F for 30- 40 mins.\\nUser : How many giraffes do I need for that recipe?\\nLaMDA : You’re the chef, so I’d think you’d know best. Good luck in making your new pie, Joe.\\nUser : What do I do if my washing machine doesn’t spin?', metadata={'chunk-id': '152', 'id': datetime.date(2201, 11, 22), 'source': 'http://arxiv.org/pdf/2201.08239', 'title': 'LaMDA: Language Models for Dialog Applications'})]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KLMwfZPfBF89"
      },
      "source": [
        "## Adding the Generation in RAG"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X79eNNL_BM4G"
      },
      "source": [
        "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jNnXYOtqypiz"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"contexts\"],\n",
        "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
        "    contexts provided. If the question cannot be answered using the information\n",
        "    provided say \"I don't know\".\n",
        "\n",
        "    Contexts:\n",
        "    {contexts}\n",
        "\n",
        "    Question: {query}\"\"\",\n",
        ")\n",
        "\n",
        "# Chain\n",
        "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "h6GVEZkhytdM",
        "outputId": "f03086b8-8d30-4d6e-a723-833ffecbcf8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I don\\'t have any information about \"llama 2\" in the provided contexts.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = qa_chain(\n",
        "    inputs={\n",
        "        \"query\": question,\n",
        "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
        "    }\n",
        ")\n",
        "out[\"text\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KemgDCg8DkgE"
      },
      "source": [
        "## Chaining Everything with a SequentialChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbLlWgEEII1"
      },
      "source": [
        "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the _retrieval_ component within a `TransformChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BpFmiRtYDpHp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import TransformChain\n",
        "\n",
        "def retrieval_transform(inputs: dict) -> dict:\n",
        "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
        "    docs = [d.page_content for d in docs]\n",
        "    docs_dict = {\n",
        "        \"query\": inputs[\"question\"],\n",
        "        \"contexts\": \"\\n---\\n\".join(docs)\n",
        "    }\n",
        "    return docs_dict\n",
        "\n",
        "retrieval_chain = TransformChain(\n",
        "    input_variables=[\"question\"],\n",
        "    output_variables=[\"query\", \"contexts\"],\n",
        "    transform=retrieval_transform\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SoD45Au1Eg-r"
      },
      "source": [
        "Now we chain this with our generation step using the `SequentialChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "azqCwDwXEkDT"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "rag_chain = SequentialChain(\n",
        "    chains=[retrieval_chain, qa_chain],\n",
        "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
        "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xpB2aWV4ESzf"
      },
      "source": [
        "Then we perform the full RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "JvJbUaLqFRG2",
        "outputId": "582caa21-777a-4a01-a618-9db64185ad5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What information can you provide about llama 2?', '2. Could you give me some details about llama 2?', '3. I would like to learn more about llama 2. Can you help me with that?']\n",
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 64.4 ms, sys: 646 µs, total: 65.1 ms\n",
            "Wall time: 10min 18s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I don\\'t have any information about \"llama 2\" in the provided contexts.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "out = rag_chain({\"question\": question})\n",
        "out[\"text\"]\n",
        "\n",
        "# CPU times: user 64.4 ms, sys: 646 µs, total: 65.1 ms\n",
        "# Wall time: 10min 18s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bLmv01geK-ZS"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZVPhHzLDQQ"
      },
      "source": [
        "## Custom Multiquery"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rI-KVO6zjJZw"
      },
      "source": [
        "We'll try this with two prompts, both encourage more variety in search queries.\n",
        "\n",
        "**Prompt A**\n",
        "```\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives.\n",
        "Each query MUST tackle the question from a different viewpoint,\n",
        "we want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "```\n",
        "\n",
        "\n",
        "**Prompt B**\n",
        "```\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4IlEnYeKLFzh"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain.chains import LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "template = \"\"\"\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CgduNJWLBez",
        "outputId": "7ffee6c2-27b4-4bdf-8c79-7effd27e3cd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and capabilities of Large Language Model Llama 2?', '2. How does Llama 2 compare to other Large Language Models in terms of performance and efficiency?', '3. What are the applications and use cases of Llama 2 in the field of Machine Learning and Natural Language Processing?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 69.5 ms, sys: 266 µs, total: 69.8 ms\n",
            "Wall time: 10min 14s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
        ")  # \"lines\" is the key (attribute name) of the parsed output\n",
        "\n",
        "# Results\n",
        "docs = retriever.get_relevant_documents(\n",
        "    query=question\n",
        ")\n",
        "len(docs)\n",
        "\n",
        "# CPU times: user 69.5 ms, sys: 266 µs, total: 69.8 ms\n",
        "# Wall time: 10min 14s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSySsaDKMK1i",
        "outputId": "e6f95abd-99fc-4576-d1f4-5fd4c21c70ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Gall², Suzana Ili\\x01 c, Yacine Jernite, Younes Belkada, Thomas Wolf\\nAbstract\\nLarge language models (LLMs) have been shown to be able to perform new tasks based on\\na few demonstrations or natural language instructions. While these capabilities have led to\\nwidespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we\\npresent BLOOM, a 176B-parameter open-access language model designed and built thanks\\nto a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of\\nsources in 46 natural and 13 programming languages (59 in total). We find that BLOOM\\nachieves competitive performance on a wide variety of benchmarks, with stronger results\\nafter undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI\\nLicense.1\\nKeywords: Language models, collaborative research\\n1. Introduction\\nPretrained language models have become a cornerstone of modern natural language processing (NLP) pipelines because they often produce better performance from smaller quantities of labeled data. The development of ELMo (Peters et al., 2018), ULMFiT (Howard', metadata={'chunk-id': '13', 'id': datetime.date(2211, 11, 22), 'source': 'http://arxiv.org/pdf/2211.05100', 'title': 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model'}),\n",
              " Document(page_content='exploit (Shoeybi et al., 2019; Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022;\\nRae et al., 2021; Wang et al., 2021; Zeng et al., 2021; Zhang et al., 2022) the benefits of scale.\\nA major factor in the success of this approach is the way that task-specific examples are\\nformatted when fed into the model. Brown et al. (2020) popularized the idea of designing\\n\\x10prompts\\x11 that provide natural-language descriptions of the task and also allow inputting\\na few demonstrations of input-output behavior.\\nSocial Limitations of LLM Development While the continued increase in the size of\\nlarge language models has resulted in improvements across a wide range of tasks, it has also\\n5\\nBigScience Workshop\\nexacerbatedissueswiththeirdevelopmentanduse(Benderetal.,2021). Thecomputational\\nexpense of large models also prohibits the majority of the research community from participating in their development, evaluation and routine use. Moreover, the computational costs\\nhave also lead to concerns about the carbon footprint stemming from the training and use\\nof large language models (Strubell et al., 2019; Lacoste et al., 2019; Schwartz et al., 2020;\\nBannour et al., 2021), and existing carbon footprint studies have likely under-estimated', metadata={'chunk-id': '20', 'id': datetime.date(2211, 11, 22), 'source': 'http://arxiv.org/pdf/2211.05100', 'title': 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model'}),\n",
              " Document(page_content='language model with performance comparable to recently developed systems, but also to\\ndocument the coordinated process that went into its development (Section 2.2). The purpose of this paper is to provide a high-level overview of these design steps while referencing\\nthe individual reports we produced over the course of developing BLOOM.\\n2. Background\\nBefore describing the BLOOM model itself, in this section we provide necessary background\\non LLMs as well as an organizational overview of the BigScience effort.\\n2.1 Language Modeling\\nLanguage modeling refers to the task of modeling the probability of a sequence of tokens in a\\ntext (Shannon, 1948), where a token is a unit of text (e.g. word, subword, character or byte,\\netc., as discussed by Mielke et al., 2021). In this work (and in most current applications of\\nlanguage modeling) we model the joint probability of tokens in a text as:\\np(x) =p(x1;:::;x T) =TY\\nt=1p(xtjx<t) (1)\\nwherexis a sequence of tokens, xtis thetthtoken, and x<tis the sequence of tokens\\nprecedingxt. This approach is referred to as autoregressive language modeling and can be\\nseen as iteratively predicting the probability of the next token.', metadata={'chunk-id': '16', 'id': datetime.date(2211, 11, 22), 'source': 'http://arxiv.org/pdf/2211.05100', 'title': 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model'}),\n",
              " Document(page_content='Pythia : A Suite for Analyzing Large Language Models\\nAcross Training and Scaling\\nStella Biderman* 1 2Hailey Schoelkopf* 1 3Quentin Anthony1Herbie Bradley1 4Kyle O’Brien1\\nEric Hallahan1Mohammad Aflah Khan5Shivanshu Purohit6 1USVSN Sai Prashanth1Edward Raff2\\nAviya Skowron1Lintang Sutawika1 7Oskar van der Wal8\\nAbstract\\nHow do large language models (LLMs) develop\\nand evolve over the course of training? How do\\nthese patterns change as models scale? To answer these questions, we introduce Pythia , a suite\\nof 16 LLMs all trained on public data seen in\\nthe exact same order and ranging in size from\\n70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16\\nmodels, alongside tools to download and reconstruct their exact training dataloaders for further\\nstudy. We intend Pythia to facilitate research in\\nmany areas, and we present several case studies including novel results in memorization, term\\nfrequency effects on few-shot performance, and\\nreducing gender bias. We demonstrate that this\\nhighly controlled setup can be used to yield novel\\ninsights toward LLMs and their training dynamics. Trained models, analysis code, training', metadata={'chunk-id': '0', 'id': datetime.date(2304, 11, 22), 'source': 'http://arxiv.org/pdf/2304.01373', 'title': 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling'}),\n",
              " Document(page_content='features represented at least in 5languages. The probing performance of both LLMs is\\nsimilar despite the difference in size. We find that the LLMs infer Mood and Person well\\nwith no regard for language. Number, NumType (numeral type), and Voice are moderately\\ninferred in most languages. The models generally show worse qualities in the other categories, indicating that they do not encode such morphological information. The possible\\nexplanation of such difference in performance may be the diversity of possible values of\\nthese categories. For example, Mood and Person share similar values across the presented\\nlanguages, while the set of Case values is highly dependent on the language.\\nCorrelation The correlation analysis results support conclusions on the probing performance and reveals contributing factors (see Table 13). Both models show similar results on\\nthe languages with different language scripts. Results of BLOOM-1B7 are highly correlated\\nwith language family, probing dataset size, and pretraining dataset size. According to the\\nresults of Mann-Whithey U test, BLOOM-1B7 shows significantly better results ( p<0:01)\\nthan BLOOM. However, BLOOM shows more stable performance on different languages in\\nspite of the amount of data it has seen during pretraining. This might indicate the better\\ngeneralization abilities of the model with more parameters.\\nDiscussion It should be noted that the following questions remain for further research:', metadata={'chunk-id': '129', 'id': datetime.date(2211, 11, 22), 'source': 'http://arxiv.org/pdf/2211.05100', 'title': 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model'}),\n",
              " Document(page_content='2\\nPythia: A Suite for Analyzing Large Language Models\\nsurprise, we find that despite making choices we expect to\\nhurt performance at smaller scales, we find that our models\\nperform the same as equi-parameter OPT models across all\\nscales. We discuss areas where our results contradict widely\\naccepted maxims for training LLMs in Section 2.6.\\n2.1. Requirements for a Scientific Suite of LLMs\\nPythia is envisioned as a suite for enabling and empowering\\nscientific research on the capacities and limitations of large\\nlanguage models. After surveying the existing literature, we\\nfound no existing suite of models which satisfied all of the\\nfollowing conditions:\\nPublic Access Models are publicly released and are\\ntrained on publicly available data.\\nTraining Provenance Intermediate checkpoints are available for analysis, all models are trained with the same data\\nordering, and intermediate checkpoints can be linked with\\nthe exact data seen up to that checkpoint. Training procedure as well as model and training hyperparameters are\\nwell-documented.\\nConsistency Across Scale Model scaling sequences\\nshould have self-consistent design decisions that reasonably adhere to common practice for training state-of-the-art\\nlarge models. Model sizes should cover a variety of scales\\nacross multiple orders of magnitude.\\nTable 2 provides our assessment of a number of popular\\nlanguage model suites along these criteria. We note that', metadata={'chunk-id': '9', 'id': datetime.date(2304, 11, 22), 'source': 'http://arxiv.org/pdf/2304.01373', 'title': 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling'}),\n",
              " Document(page_content='across multiple orders of magnitude.\\nTable 2 provides our assessment of a number of popular\\nlanguage model suites along these criteria. We note that\\nfor “number of checkpoints” we go with the number of\\ncheckpoints by the model in the model suite with the fewest\\ncheckpoints . While some model suites (e.g., GPT-Neo, OPT,\\nBLOOM) have a subset that have more available, for most\\nresearch purposes this is insufficient. This is exacerbated by\\nthe fact that typically smaller models are the ones with more\\ncheckpoints; the only model suite from the above list whose\\nlargest model has more checkpoints than smaller ones is\\nGPT-Neo.\\n2.2. Training Data\\nWe train our models on the Pile (Gao et al., 2020; Biderman et al., 2022), a curated collection of English language datasets for training large language models that\\nis popular for training large autoregressive transformers.\\nThis dataset has three major benefits over its competitors:\\nfirst, it is freely and publicly available; second, it reports\\na higher downstream performance (Le Scao et al., 2022)\\nthan popular crawl-based datasets C4 (Raffel et al., 2020;\\nDodge et al., 2021) and OSCAR (Su ´arez et al., 2019);', metadata={'chunk-id': '10', 'id': datetime.date(2304, 11, 22), 'source': 'http://arxiv.org/pdf/2304.01373', 'title': 'Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling'}),\n",
              " Document(page_content='and an average of 2.7 nDCG on ten lowresource languages Mr.TyDi. Subsequently,\\nwe delve into the potential for distilling the\\nranking capabilities of ChatGPT into a specialized model. Our small specialized model that\\ntrained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated\\nMS MARCO data on BEIR. The code to reproduce our results is available at www.github.\\ncom/sunnweiwei/RankGPT\\n1 Introduction\\nLarge Language Models (LLMs), such as ChatGPT and GPT-4 (OpenAI, 2022, 2023), are revolutionizing natural language processing with strong\\nzero-shot and few-shot generalization. By pretraining on large-scale text corpora and alignment\\nﬁne-tuning to follow human instructions, LLMs\\nhave demonstrated their superior capability in language understanding, generation, interaction, and\\nreasoning (Ouyang et al., 2022).\\nThe success of LLMs has also attracted broad\\nattention in the Information Retrieval (IR) community. These studies mainly focus on exploiting the\\ngeneration ability of LLMs, rather than exploring\\nthe LLMs’ ability for searching. For example, New\\nBing utilizes GPT-4 to generate responses based on\\nthe retrieved documents (Microsoft, 2023). As a', metadata={'chunk-id': '1', 'id': datetime.date(2304, 11, 22), 'source': 'http://arxiv.org/pdf/2304.09542', 'title': 'Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent'}),\n",
              " Document(page_content='that uses the LLM to read natural language problems and generate programs as the intermediate\\nreasoning steps, but ofﬂoads the solution step to a\\nruntime such as a Python interpreter. With PAL,\\ndecomposing the natural language problem into\\nrunnable steps remains the only learning task for\\nthe LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a\\nneural LLM and a symbolic interpreter across 13\\nmathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning\\ntasks, generating code using an LLM and reasoning using a Python interpreter leads to more\\naccurate results than much larger models. For example, PALusing CODEX achieves state-of-theart few-shot accuracy on the GSM 8Kbenchmark\\nof math word problems, surpassing PaLM-540 B\\nwhich uses chain-of-thought by absolute 15% top1. Our code and data are publicly available at\\nhttp://reasonwithpal.com .\\n*The ﬁrst three authors contributed equally.1Language Technologies Institute, Carnegie Mellon University, USA2Inspired\\nCognition, USA.1. Introduction\\nUntil as recently as two years ago, reasoning was considered', metadata={'chunk-id': '1', 'id': datetime.date(2211, 11, 22), 'source': 'http://arxiv.org/pdf/2211.10435', 'title': 'PAL: Program-aided Language Models'}),\n",
              " Document(page_content='ELMo (Peters et al., 2018) and BERT (Devlin et al.,\\n2019) have emerged as universal tools that capture\\na diverse range of linguistic and factual knowledge.\\nRecently, Petroni et al. (2019) introduced LAMA\\n(LAnguage Model Analysis) to investigate whether\\nPLMs can recall factual knowledge that is part of\\ntheir training corpus. Since the PLM training objective is to predict masked tokens, question answering (QA) tasks can be reformulated as cloze\\nquestions. For example, “Who wrote ‘Dubliners’?”\\nis reformulated as “[MASK] wrote ‘Dubliners’.” In\\nthis setup, Petroni et al. (2019) show that PLMs outperform automatically extracted knowledge bases\\non QA. In this paper, we investigate this capability\\nof PLMs in the context of (1) negation and what\\nwe call (2) mispriming .\\n(1) Negation. To study the effect of negation\\non PLMs, we introduce the negated LAMA dataset .\\nWe insert negation elements (e.g., “not”) in LAMA\\ncloze questions (e.g., “The theory of relativity was', metadata={'chunk-id': '1', 'id': datetime.date(1911, 11, 22), 'source': 'http://arxiv.org/pdf/1911.03343', 'title': 'Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly'}),\n",
              " Document(page_content='healthcare (Patel and Lam, 2023), law (ChatGPT\\nand Perlman, 2022), science (Stokel-Walker, 2023),\\n*Equal contribution\\n1While the majority of these models are trained on English,\\nrecent studies have also obtained similar advancements in\\nother languages (Lin et al., 2021; Shliazhko et al., 2022).\\nFigure 1: Overview of Intervention Strategies. A typical ML/NLP model development process involves data\\ncollection/curation, model training and design, inference, and ﬁnally application deployment. For each\\nphase of this development cycle, different techniques\\ncan be adopted to mitigate harms. Our survey presents\\na taxonomy of intervention strategies organized around\\nthe different phases where they can be applied.\\nand more. Since language is inherently a tool of\\npower—the primary means by which people and societies perpetuate stereotypes and manipulate opinions (Bar-Tal et al., 2013; Chong and Druckman,\\n2007, inter alia )—LMs that are deployed to millions of users also hold similar power, but our understanding of their risks/harms has lagged behind\\n(Bender et al., 2021).\\nIndeed, LMs have been shown to introduce vulnerabilities and threats, both inadvertent and malicious, to individual users, social groups, and content integrity. Without social context and content\\ncontrol, deployed language generators have quickly', metadata={'chunk-id': '2', 'id': datetime.date(2210, 11, 22), 'source': 'http://arxiv.org/pdf/2210.07700', 'title': 'Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey'}),\n",
              " Document(page_content='2022) generates pseudo-queries using GPT-3, and Promptagator (Dai et al., 2023) proposes a few-shot\\ndense retrieval to leverage a few demonstrations from the target domain for pseudo queries generation.\\nFurthermore, LLMs have been used for content generation (Yu et al., 2023) and web browsing (Nakano\\net al., 2021; Izacard et al., 2022b; Microsoft, 2023).In this paper, we explore using ChatGPT and GPT-4\\nin passage re-ranking tasks, propose an instructional permutation generation method, and conduct a\\ncomprehensive evaluation of benchmarks from various domains, tasks, and languages.\\nB.2 LLMs Distillation\\nDespite their impressive capabilities, LLMs such as GPT-4 often come with high costs and lack opensource availability. As a result, considerable research has explored ways to distill the capabilities of\\nLLMs into specialized, custom models. For instance, Fu et al. (2023) and Magister et al. (2022) have\\nsuccessfully distilled the reasoning ability of LLMs into smaller models. Self-instruct (Wang et al.,\\n2022) and Alpaca (Taori et al., 2023) propose iterative approaches to distill GPT-3 using their outputs.', metadata={'chunk-id': '51', 'id': datetime.date(2304, 11, 22), 'source': 'http://arxiv.org/pdf/2304.09542', 'title': 'Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent'})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F4q65OEiizU2"
      },
      "source": [
        "Putting this together in another `SequentialChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LTRjTIKzi2-g"
      },
      "outputs": [],
      "source": [
        "retrieval_chain = TransformChain(\n",
        "    input_variables=[\"question\"],\n",
        "    output_variables=[\"query\", \"contexts\"],\n",
        "    transform=retrieval_transform\n",
        ")\n",
        "\n",
        "rag_chain = SequentialChain(\n",
        "    chains=[retrieval_chain, qa_chain],\n",
        "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
        "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rda74xhpjE6A"
      },
      "source": [
        "And asking again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9UcBY71cjGgX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and capabilities of Large Language Model Llama 2?', '2. How does Llama 2 compare to other Large Language Models in terms of performance and efficiency?', '3. What are the applications and use cases of Llama 2 in the field of Machine Learning and Natural Language Processing?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 56 ms, sys: 840 µs, total: 56.9 ms\n",
            "Wall time: 6min 23s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I don\\'t have information about \"llama 2\" in the provided contexts.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "out = rag_chain({\"question\": question})\n",
        "out[\"text\"]\n",
        "\n",
        "# CPU times: user 56 ms, sys: 840 µs, total: 56.9 ms\n",
        "# Wall time: 6min 23s"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8jULksgk7gLA"
      },
      "source": [
        "After finishing, delete your Pinecone index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
