{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monday, November 27, 2023\n",
    "\n",
    "[LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101](https://www.youtube.com/watch?v=eqOfr4AGLk8&list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F&index=4)\n",
    "\n",
    "This notebook does not use OpenAI.\n",
    "\n",
    "Hmm [tiktoken](https://github.com/openai/tiktoken) is an open source BPE tokeniser for use with OpenAI's models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/xx-langchain-chunking.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/xx-langchain-chunking.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# Preparing Text Data for use with Retrieval-Augmented LLMs\n",
    "\n",
    "In this walkthrough we'll take a look at an example and some of the considerations when we need to prepare text data for retrieval augmented question-answering using **L**arge **L**anguage **M**odels (LLMs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few Python libraries we must `pip install` for this notebook to run, those are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain tiktoken matplotlib seaborn tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will download the LangChain docs from [langchain.readthedocs.io/](https://langchain.readthedocs.io/latest/en/). We get all `.html` files located on the site like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: wget: command not found\n",
      "CPU times: user 531 µs, sys: 3.92 ms, total: 4.46 ms\n",
      "Wall time: 484 ms\n"
     ]
    }
   ],
   "source": [
    "# To get this to run, I had to run this ...\n",
    "#  wget -r -A.html -P rtdocs https://api.python.langchain.com/en/latest/\n",
    "# ... in an outside terminal window then manually move it into the desired folder of this repo ...\n",
    "# And this downloads 158.4mb of data!\n",
    "\n",
    "# This does not work ... cuz that url is now dead ...\n",
    "# !wget -r -A.html -P rtdocs https://langchain.readthedocs.io/en/latest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads all HTML into the `rtdocs` directory. Now we can use LangChain itself to process these docs. We do this using the `ReadTheDocsLoader` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "\n",
    "loader = ReadTheDocsLoader('rtdocs')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with `3086` processed doc pages. Let's take a look at the format each one contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='langchain_core API Reference¶\\nlangchain_core.agents¶\\nClasses¶\\nagents.AgentAction\\nA full description of an action for an ActionAgent to execute.\\nagents.AgentActionMessageLog\\nOverride init to support instantiation by position for backward compat.\\nagents.AgentFinish\\nThe final return value of an ActionAgent.\\nlangchain_core.caches¶\\nClasses¶\\ncaches.BaseCache()\\nBase interface for cache.\\nlangchain_core.callbacks¶\\nClasses¶\\ncallbacks.base.AsyncCallbackHandler()\\nAsync callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackHandler()\\nBase callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackManager(handlers)\\nBase callback manager that handles callbacks from LangChain.\\ncallbacks.base.CallbackManagerMixin()\\nMixin for callback manager.\\ncallbacks.base.ChainManagerMixin()\\nMixin for chain callbacks.\\ncallbacks.base.LLMManagerMixin()\\nMixin for LLM callbacks.\\ncallbacks.base.RetrieverManagerMixin()\\nMixin for Retriever callbacks.\\ncallbacks.base.RunManagerMixin()\\nMixin for run manager.\\ncallbacks.base.ToolManagerMixin()\\nMixin for tool callbacks.\\ncallbacks.manager.AsyncCallbackManager(handlers)\\nAsync callback manager that handles callbacks from LangChain.\\ncallbacks.manager.AsyncCallbackManagerForChainGroup(...)\\nAsync callback manager for the chain group.\\ncallbacks.manager.AsyncCallbackManagerForChainRun(*,\\xa0...)\\nAsync callback manager for chain run.\\ncallbacks.manager.AsyncCallbackManagerForLLMRun(*,\\xa0...)\\nAsync callback manager for LLM run.\\ncallbacks.manager.AsyncCallbackManagerForRetrieverRun(*,\\xa0...)\\nAsync callback manager for retriever run.\\ncallbacks.manager.AsyncCallbackManagerForToolRun(*,\\xa0...)\\nAsync callback manager for tool run.\\ncallbacks.manager.AsyncParentRunManager(*,\\xa0...)\\nAsync Parent Run Manager.\\ncallbacks.manager.AsyncRunManager(*,\\xa0run_id,\\xa0...)\\nAsync Run Manager.\\ncallbacks.manager.BaseRunManager(*,\\xa0run_id,\\xa0...)\\nBase class for run manager (a bound callback manager).\\ncallbacks.manager.CallbackManager(handlers)\\nCallback manager that handles callbacks from LangChain.\\ncallbacks.manager.CallbackManagerForChainGroup(...)\\nCallback manager for the chain group.\\ncallbacks.manager.CallbackManagerForChainRun(*,\\xa0...)\\nCallback manager for chain run.\\ncallbacks.manager.CallbackManagerForLLMRun(*,\\xa0...)\\nCallback manager for LLM run.\\ncallbacks.manager.CallbackManagerForRetrieverRun(*,\\xa0...)\\nCallback manager for retriever run.\\ncallbacks.manager.CallbackManagerForToolRun(*,\\xa0...)\\nCallback manager for tool run.\\ncallbacks.manager.ParentRunManager(*,\\xa0...[,\\xa0...])\\nSync Parent Run Manager.\\ncallbacks.manager.RunManager(*,\\xa0run_id,\\xa0...)\\nSync Run Manager.\\ncallbacks.stdout.StdOutCallbackHandler([color])\\nCallback Handler that prints to std out.\\ncallbacks.streaming_stdout.StreamingStdOutCallbackHandler()\\nCallback handler for streaming.\\nFunctions¶\\ncallbacks.manager.ahandle_event(handlers,\\xa0...)\\nGeneric event handler for AsyncCallbackManager.\\ncallbacks.manager.atrace_as_chain_group(...)\\nGet an async callback manager for a chain group in a context manager.\\ncallbacks.manager.handle_event(handlers,\\xa0...)\\nGeneric event handler for CallbackManager.\\ncallbacks.manager.trace_as_chain_group(...)\\nGet a callback manager for a chain group in a context manager.\\nlangchain_core.chat_history¶\\nClasses¶\\nchat_history.BaseChatMessageHistory()\\nAbstract base class for storing chat message history.\\nlangchain_core.chat_sessions¶\\nClasses¶\\nchat_sessions.ChatSession\\nChat Session represents a single conversation, channel, or other group of messages.\\nlangchain_core.documents¶\\nClasses¶\\ndocuments.base.Document\\nClass for storing a piece of text and associated metadata.\\ndocuments.transformers.BaseDocumentTransformer()\\nAbstract base class for document transformation systems.\\nlangchain_core.embeddings¶\\nClasses¶\\nembeddings.Embeddings()\\nInterface for embedding models.\\nlangchain_core.example_selectors¶\\nLogic for selecting examples to include in prompts.\\nClasses¶\\nexample_selectors.base.BaseExampleSelector()\\nInterface for selecting examples to include in prompts.\\nexample_selectors.length_based.LengthBasedExampleSelector\\nSelect examples based on length.\\nexample_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector\\nExampleSelector that selects examples based on Max Marginal Relevance.\\nexample_selectors.semantic_similarity.SemanticSimilarityExampleSelector\\nExample selector that selects examples based on SemanticSimilarity.\\nFunctions¶\\nexample_selectors.semantic_similarity.sorted_values(values)\\nReturn a list of values in dict sorted by key.\\nlangchain_core.exceptions¶\\nClasses¶\\nexceptions.LangChainException\\nGeneral LangChain exception.\\nexceptions.OutputParserException(error[,\\xa0...])\\nException that output parsers should raise to signify a parsing error.\\nexceptions.TracerException\\nBase class for exceptions in tracers module.\\nlangchain_core.language_models¶\\nClasses¶\\nlanguage_models.base.BaseLanguageModel\\nAbstract base class for interfacing with language models.\\nlanguage_models.chat_models.BaseChatModel\\nBase class for Chat models.\\nlanguage_models.chat_models.SimpleChatModel\\nSimple Chat Model.\\nlanguage_models.llms.BaseLLM\\nBase LLM abstract interface.\\nlanguage_models.llms.LLM\\nBase LLM abstract class.\\nFunctions¶\\nlanguage_models.llms.create_base_retry_decorator(...)\\nCreate a retry decorator for a given LLM and provided list of error types.\\nlanguage_models.llms.get_prompts(params,\\xa0prompts)\\nGet prompts that are already cached.\\nlanguage_models.llms.update_cache(...)\\nUpdate the cache and get the LLM output.\\nlangchain_core.load¶\\nSerialization and deserialization.\\nClasses¶\\nload.load.Reviver([secrets_map,\\xa0...])\\nReviver for JSON objects.\\nload.serializable.BaseSerialized\\nBase class for serialized objects.\\nload.serializable.Serializable\\nSerializable base class.\\nload.serializable.SerializedConstructor\\nSerialized constructor.\\nload.serializable.SerializedNotImplemented\\nSerialized not implemented.\\nload.serializable.SerializedSecret\\nSerialized secret.\\nFunctions¶\\nload.dump.default(obj)\\nReturn a default value for a Serializable object or a SerializedNotImplemented object.\\nload.dump.dumpd(obj)\\nReturn a json dict representation of an object.\\nload.dump.dumps(obj,\\xa0*[,\\xa0pretty])\\nReturn a json string representation of an object.\\nload.load.load(obj,\\xa0*[,\\xa0secrets_map,\\xa0...])\\nRevive a LangChain class from a JSON object.\\nload.load.loads(text,\\xa0*[,\\xa0secrets_map,\\xa0...])\\nRevive a LangChain class from a JSON string.\\nload.serializable.to_json_not_implemented(obj)\\nSerialize a \"not implemented\" object.\\nload.serializable.try_neq_default(value,\\xa0...)\\nlangchain_core.memory¶\\nClasses¶\\nmemory.BaseMemory\\nAbstract base class for memory in Chains.\\nlangchain_core.messages¶\\nClasses¶\\nmessages.ai.AIMessage\\nA Message from an AI.\\nmessages.ai.AIMessageChunk\\nA Message chunk from an AI.\\nmessages.base.BaseMessage\\nThe base abstract Message class.\\nmessages.base.BaseMessageChunk\\nA Message chunk, which can be concatenated with other Message chunks.\\nmessages.chat.ChatMessage\\nA Message that can be assigned an arbitrary speaker (i.e.\\nmessages.chat.ChatMessageChunk\\nA Chat Message chunk.\\nmessages.function.FunctionMessage\\nA Message for passing the result of executing a function back to a model.\\nmessages.function.FunctionMessageChunk\\nA Function Message chunk.\\nmessages.human.HumanMessage\\nA Message from a human.\\nmessages.human.HumanMessageChunk\\nA Human Message chunk.\\nmessages.system.SystemMessage\\nA Message for priming AI behavior, usually passed in as the first of a sequence of input messages.\\nmessages.system.SystemMessageChunk\\nA System Message chunk.\\nmessages.tool.ToolMessage\\nA Message for passing the result of executing a tool back to a model.\\nmessages.tool.ToolMessageChunk\\nA Tool Message chunk.\\nFunctions¶\\nmessages.base.merge_content(first_content,\\xa0...)\\nmessages.base.message_to_dict(message)\\nmessages.base.messages_to_dict(messages)\\nConvert a sequence of Messages to a list of dictionaries.\\nlangchain_core.output_parsers¶\\nClasses¶\\noutput_parsers.base.BaseGenerationOutputParser\\nBase class to parse the output of an LLM call.\\noutput_parsers.base.BaseLLMOutputParser()\\nAbstract base class for parsing the outputs of a model.\\noutput_parsers.base.BaseOutputParser\\nBase class to parse the output of an LLM call.\\noutput_parsers.list.CommaSeparatedListOutputParser\\nParse the output of an LLM call to a comma-separated list.\\noutput_parsers.list.ListOutputParser\\nParse the output of an LLM call to a list.\\noutput_parsers.list.MarkdownListOutputParser\\nParse a markdown list.\\noutput_parsers.list.NumberedListOutputParser\\nParse a numbered list.\\noutput_parsers.string.StrOutputParser\\nOutputParser that parses LLMResult into the top likely string.\\noutput_parsers.transform.BaseCumulativeTransformOutputParser\\nBase class for an output parser that can handle streaming input.\\noutput_parsers.transform.BaseTransformOutputParser\\nBase class for an output parser that can handle streaming input.\\nlangchain_core.outputs¶\\nClasses¶\\noutputs.chat_generation.ChatGeneration\\nA single chat generation output.\\noutputs.chat_generation.ChatGenerationChunk\\nA ChatGeneration chunk, which can be concatenated with other\\noutputs.chat_result.ChatResult\\nClass that contains all results for a single chat model call.\\noutputs.generation.Generation\\nA single text generation output.\\noutputs.generation.GenerationChunk\\nA Generation chunk, which can be concatenated with other Generation chunks.\\noutputs.llm_result.LLMResult\\nClass that contains all results for a batched LLM call.\\noutputs.run_info.RunInfo\\nClass that contains metadata for a single execution of a Chain or model.\\nlangchain_core.prompt_values¶\\nClasses¶\\nprompt_values.ChatPromptValue\\nChat prompt value.\\nprompt_values.ChatPromptValueConcrete\\nChat prompt value which explicitly lists out the message types it accepts.\\nprompt_values.PromptValue\\nBase abstract class for inputs to any language model.\\nprompt_values.StringPromptValue\\nString prompt value.\\nlangchain_core.prompts¶\\nPrompt is the input to the model.\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\nand working with prompts easy.\\nClass hierarchy:\\nBasePromptTemplate --> PipelinePromptTemplate\\n                       StringPromptTemplate --> PromptTemplate\\n                                                FewShotPromptTemplate\\n                                                FewShotPromptWithTemplates\\n                       BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\\nBaseMessagePromptTemplate --> MessagesPlaceholder\\n                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                  HumanMessagePromptTemplate\\n                                                                  AIMessagePromptTemplate\\n                                                                  SystemMessagePromptTemplate\\nClasses¶\\nprompts.base.BasePromptTemplate\\nBase class for all prompt templates, returning a prompt.\\nprompts.chat.AIMessagePromptTemplate\\nAI message prompt template.\\nprompts.chat.BaseChatPromptTemplate\\nBase class for chat prompt templates.\\nprompts.chat.BaseMessagePromptTemplate\\nBase class for message prompt templates.\\nprompts.chat.BaseStringMessagePromptTemplate\\nBase class for message prompt templates that use a string prompt template.\\nprompts.chat.ChatMessagePromptTemplate\\nChat message prompt template.\\nprompts.chat.ChatPromptTemplate\\nA prompt template for chat models.\\nprompts.chat.HumanMessagePromptTemplate\\nHuman message prompt template.\\nprompts.chat.MessagesPlaceholder\\nPrompt template that assumes variable is already list of messages.\\nprompts.chat.SystemMessagePromptTemplate\\nSystem message prompt template.\\nprompts.few_shot.FewShotChatMessagePromptTemplate\\nChat prompt template that supports few-shot examples.\\nprompts.few_shot.FewShotPromptTemplate\\nPrompt template that contains few shot examples.\\nprompts.few_shot_with_templates.FewShotPromptWithTemplates\\nPrompt template that contains few shot examples.\\nprompts.pipeline.PipelinePromptTemplate\\nA prompt template for composing multiple prompt templates together.\\nprompts.prompt.PromptTemplate\\nA prompt template for a language model.\\nprompts.string.StringPromptTemplate\\nString prompt that exposes the format method, returning a prompt.\\nFunctions¶\\nprompts.base.format_document(doc,\\xa0prompt)\\nFormat a document into a string based on a prompt template.\\nprompts.loading.load_prompt(path)\\nUnified method for loading a prompt from LangChainHub or local fs.\\nprompts.loading.load_prompt_from_config(config)\\nLoad prompt from Config Dict.\\nprompts.string.check_valid_template(...)\\nCheck that template string is valid.\\nprompts.string.get_template_variables(...)\\nGet the variables from the template.\\nprompts.string.jinja2_formatter(template,\\xa0...)\\nFormat a template using jinja2.\\nprompts.string.validate_jinja2(template,\\xa0...)\\nValidate that the input variables are valid for the template.\\nlangchain_core.retrievers¶\\nClasses¶\\nretrievers.BaseRetriever\\nAbstract base class for a Document retrieval system.\\nlangchain_core.runnables¶\\nLangChain Runnable and the LangChain Expression Language (LCEL).\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\nSupport for async allows servers hosting LCEL based programs to scale better\\nfor higher concurrent loads.\\nStreaming of intermediate outputs as they’re being generated allows for\\ncreating more responsive UX.\\nThis module contains schema and implementation of LangChain Runnables primitives.\\nClasses¶\\nrunnables.base.Runnable()\\nA unit of work that can be invoked, batched, streamed, transformed and composed.\\nrunnables.base.RunnableBinding\\nA runnable that delegates calls to another runnable with a set of kwargs.\\nrunnables.base.RunnableBindingBase\\nA runnable that delegates calls to another runnable with a set of kwargs.\\nrunnables.base.RunnableEach\\nA runnable that delegates calls to another runnable with each element of the input sequence.\\nrunnables.base.RunnableEachBase\\nA runnable that delegates calls to another runnable with each element of the input sequence.\\nrunnables.base.RunnableGenerator(transform)\\nA runnable that runs a generator function.\\nrunnables.base.RunnableLambda(func[,\\xa0afunc])\\nRunnableLambda converts a python callable into a Runnable.\\nrunnables.base.RunnableMap\\nalias of RunnableParallel\\nrunnables.base.RunnableParallel\\nA runnable that runs a mapping of runnables in parallel, and returns a mapping of their outputs.\\nrunnables.base.RunnableSequence\\nA sequence of runnables, where the output of each is the input of the next.\\nrunnables.base.RunnableSerializable\\nA Runnable that can be serialized to JSON.\\nrunnables.branch.RunnableBranch\\nA Runnable that selects which branch to run based on a condition.\\nrunnables.config.EmptyDict\\nEmpty dict type.\\nrunnables.config.RunnableConfig\\nConfiguration for a Runnable.\\nrunnables.configurable.DynamicRunnable\\nA Serializable Runnable that can be dynamically configured.\\nrunnables.configurable.RunnableConfigurableAlternatives\\nA Runnable that can be dynamically configured.\\nrunnables.configurable.RunnableConfigurableFields\\nA Runnable that can be dynamically configured.\\nrunnables.configurable.StrEnum(value[,\\xa0...])\\nA string enum.\\nrunnables.fallbacks.RunnableWithFallbacks\\nA Runnable that can fallback to other Runnables if it fails.\\nrunnables.history.RunnableWithMessageHistory\\nA runnable that manages chat message history for another runnable.\\nrunnables.passthrough.RunnableAssign\\nA runnable that assigns key-value pairs to Dict[str, Any] inputs.\\nrunnables.passthrough.RunnablePassthrough\\nA runnable to passthrough inputs unchanged or with additional keys.\\nrunnables.retry.RunnableRetry\\nRetry a Runnable if it fails.\\nrunnables.router.RouterInput\\nA Router input.\\nrunnables.router.RouterRunnable\\nA runnable that routes to a set of runnables based on Input[\\'key\\'].\\nrunnables.utils.AddableDict\\nDictionary that can be added to another dictionary.\\nrunnables.utils.ConfigurableField(id[,\\xa0...])\\nA field that can be configured by the user.\\nrunnables.utils.ConfigurableFieldMultiOption(id,\\xa0...)\\nA field that can be configured by the user with multiple default values.\\nrunnables.utils.ConfigurableFieldSingleOption(id,\\xa0...)\\nA field that can be configured by the user with a default value.\\nrunnables.utils.ConfigurableFieldSpec(id,\\xa0...)\\nA field that can be configured by the user.\\nrunnables.utils.GetLambdaSource()\\nGet the source code of a lambda function.\\nrunnables.utils.IsFunctionArgDict()\\nCheck if the first argument of a function is a dict.\\nrunnables.utils.IsLocalDict(name,\\xa0keys)\\nCheck if a name is a local dict.\\nrunnables.utils.SupportsAdd(*args,\\xa0**kwargs)\\nProtocol for objects that support addition.\\nFunctions¶\\nrunnables.base.coerce_to_runnable(thing)\\nCoerce a runnable-like object into a Runnable.\\nrunnables.config.acall_func_with_variable_args(...)\\nCall function that may optionally accept a run_manager and/or config.\\nrunnables.config.call_func_with_variable_args(...)\\nCall function that may optionally accept a run_manager and/or config.\\nrunnables.config.ensure_config([config])\\nEnsure that a config is a dict with all keys present.\\nrunnables.config.get_async_callback_manager_for_config(config)\\nGet an async callback manager for a config.\\nrunnables.config.get_callback_manager_for_config(config)\\nGet a callback manager for a config.\\nrunnables.config.get_config_list(config,\\xa0length)\\nGet a list of configs from a single config or a list of configs.\\nrunnables.config.get_executor_for_config(config)\\nGet an executor for a config.\\nrunnables.config.merge_configs(*configs)\\nMerge multiple configs into one.\\nrunnables.config.patch_config(config,\\xa0*[,\\xa0...])\\nPatch a config with new values.\\nrunnables.configurable.make_options_spec(...)\\nMake a ConfigurableFieldSpec for a ConfigurableFieldSingleOption or ConfigurableFieldMultiOption.\\nrunnables.passthrough.aidentity(x)\\nAn async identity function\\nrunnables.passthrough.identity(x)\\nAn identity function\\nrunnables.utils.aadd(addables)\\nAsynchronously add a sequence of addable objects together.\\nrunnables.utils.accepts_config(callable)\\nCheck if a callable accepts a config argument.\\nrunnables.utils.accepts_run_manager(callable)\\nCheck if a callable accepts a run_manager argument.\\nrunnables.utils.add(addables)\\nAdd a sequence of addable objects together.\\nrunnables.utils.gated_coro(semaphore,\\xa0coro)\\nRun a coroutine with a semaphore.\\nrunnables.utils.gather_with_concurrency(n,\\xa0...)\\nGather coroutines with a limit on the number of concurrent coroutines.\\nrunnables.utils.get_function_first_arg_dict_keys(func)\\nGet the keys of the first argument of a function if it is a dict.\\nrunnables.utils.get_lambda_source(func)\\nGet the source code of a lambda function.\\nrunnables.utils.get_unique_config_specs(specs)\\nGet the unique config specs from a sequence of config specs.\\nrunnables.utils.indent_lines_after_first(...)\\nIndent all lines of text after the first line.\\nlangchain_core.stores¶\\nClasses¶\\nstores.BaseStore()\\nAbstract interface for a key-value store.\\nlangchain_core.tools¶\\nBase implementation for tools or skills.\\nClasses¶\\ntools.BaseTool\\nInterface LangChain tools must implement.\\ntools.SchemaAnnotationError\\nRaised when \\'args_schema\\' is missing or has an incorrect type annotation.\\ntools.StructuredTool\\nTool that can operate on any number of inputs.\\ntools.Tool\\nTool that takes in function or coroutine directly.\\ntools.ToolException\\nAn optional exception that tool throws when execution error occurs.\\nFunctions¶\\ntools.create_schema_from_function(...)\\nCreate a pydantic schema from a function\\'s signature.\\ntools.tool(*args[,\\xa0return_direct,\\xa0...])\\nMake tools out of functions, can be used with or without arguments.\\nlangchain_core.tracers¶\\nClasses¶\\ntracers.base.BaseTracer(**kwargs)\\nBase interface for tracers.\\ntracers.evaluation.EvaluatorCallbackHandler(...)\\nA tracer that runs a run evaluator whenever a run is persisted.\\ntracers.langchain.LangChainTracer([...])\\nAn implementation of the SharedTracer that POSTS to the langchain endpoint.\\ntracers.langchain_v1.LangChainTracerV1(**kwargs)\\nAn implementation of the SharedTracer that POSTS to the langchain endpoint.\\ntracers.log_stream.LogEntry\\nA single entry in the run log.\\ntracers.log_stream.LogStreamCallbackHandler(*)\\nA tracer that streams run logs to a stream.\\ntracers.log_stream.RunLog(*ops,\\xa0state)\\nA run log.\\ntracers.log_stream.RunLogPatch(*ops)\\nA patch to the run log.\\ntracers.log_stream.RunState\\nState of the run.\\ntracers.root_listeners.RootListenersTracer(*,\\xa0...)\\ntracers.run_collector.RunCollectorCallbackHandler([...])\\nA tracer that collects all nested runs in a list.\\ntracers.schemas.BaseRun\\nBase class for Run.\\ntracers.schemas.ChainRun\\nClass for ChainRun.\\ntracers.schemas.LLMRun\\nClass for LLMRun.\\ntracers.schemas.Run\\nRun schema for the V2 API in the Tracer.\\ntracers.schemas.ToolRun\\nClass for ToolRun.\\ntracers.schemas.TracerSession\\nTracerSessionV1 schema for the V2 API.\\ntracers.schemas.TracerSessionBase\\nBase class for TracerSession.\\ntracers.schemas.TracerSessionV1\\nTracerSessionV1 schema.\\ntracers.schemas.TracerSessionV1Base\\nBase class for TracerSessionV1.\\ntracers.schemas.TracerSessionV1Create\\nCreate class for TracerSessionV1.\\ntracers.stdout.ConsoleCallbackHandler(**kwargs)\\nTracer that prints to the console.\\ntracers.stdout.FunctionCallbackHandler(...)\\nTracer that calls a function with a single str parameter.\\nFunctions¶\\ntracers.context.collect_runs()\\nCollect all run traces in context.\\ntracers.context.register_configure_hook(...)\\ntracers.context.tracing_enabled([session_name])\\nGet the Deprecated LangChainTracer in a context manager.\\ntracers.context.tracing_v2_enabled([...])\\nInstruct LangChain to log all runs in context to LangSmith.\\ntracers.evaluation.wait_for_all_evaluators()\\nWait for all tracers to finish.\\ntracers.langchain.get_client()\\nGet the client.\\ntracers.langchain.log_error_once(method,\\xa0...)\\nLog an error once.\\ntracers.langchain.wait_for_all_tracers()\\nWait for all tracers to finish.\\ntracers.langchain_v1.get_headers()\\nGet the headers for the LangChain API.\\ntracers.schemas.RunTypeEnum()\\nRunTypeEnum.\\ntracers.stdout.elapsed(run)\\nGet the elapsed time of a run.\\ntracers.stdout.try_json_stringify(obj,\\xa0fallback)\\nTry to stringify an object to JSON.\\nlangchain_core.utils¶\\nUtility functions for LangChain.\\nThese functions do not depend on any other LangChain module.\\nClasses¶\\nutils.aiter.NoLock()\\nDummy lock that provides the proper interface but no protection\\nutils.aiter.Tee(iterable[,\\xa0n,\\xa0lock])\\nCreate n separate asynchronous iterators over iterable\\nutils.aiter.atee\\nalias of Tee\\nutils.formatting.StrictFormatter()\\nA subclass of formatter that checks for extra keys.\\nutils.iter.NoLock()\\nDummy lock that provides the proper interface but no protection\\nutils.iter.Tee(iterable[,\\xa0n,\\xa0lock])\\nCreate n separate asynchronous iterators over iterable\\nutils.iter.safetee\\nalias of Tee\\nFunctions¶\\nutils.aiter.py_anext(iterator[,\\xa0default])\\nPure-Python implementation of anext() for testing purposes.\\nutils.aiter.tee_peer(iterator,\\xa0buffer,\\xa0...)\\nAn individual iterator of a tee()\\nutils.env.env_var_is_set(env_var)\\nCheck if an environment variable is set.\\nutils.input.get_bolded_text(text)\\nGet bolded text.\\nutils.input.get_color_mapping(items[,\\xa0...])\\nGet mapping for items to a support color.\\nutils.input.get_colored_text(text,\\xa0color)\\nGet colored text.\\nutils.input.print_text(text[,\\xa0color,\\xa0end,\\xa0file])\\nPrint text with highlighting and no end characters.\\nutils.iter.batch_iterate(size,\\xa0iterable)\\nUtility batching function.\\nutils.iter.tee_peer(iterator,\\xa0buffer,\\xa0peers,\\xa0...)\\nAn individual iterator of a tee()\\nutils.loading.try_load_from_hub(path,\\xa0...)\\nLoad configuration from hub.\\nutils.pydantic.get_pydantic_major_version()\\nGet the major version of Pydantic.\\nutils.utils.build_extra_kwargs(extra_kwargs,\\xa0...)\\nBuild extra kwargs from values and extra_kwargs.\\nutils.utils.check_package_version(package[,\\xa0...])\\nCheck the version of a package.\\nutils.utils.convert_to_secret_str(value)\\nConvert a string to a SecretStr if needed.\\nutils.utils.get_pydantic_field_names(...)\\nGet field names, including aliases, for a pydantic class.\\nutils.utils.guard_import(module_name,\\xa0*[,\\xa0...])\\nDynamically imports a module and raises a helpful exception if the module is not installed.\\nutils.utils.mock_now(dt_value)\\nContext manager for mocking out datetime.now() in unit tests.\\nutils.utils.raise_for_status_with_text(response)\\nRaise an error with the response text.\\nutils.utils.xor_args(*arg_groups)\\nValidate specified keyword args are mutually exclusive.\\nlangchain_core.vectorstores¶\\nClasses¶\\nvectorstores.VectorStore()\\nInterface for vector store.\\nvectorstores.VectorStoreRetriever\\nBase Retriever class for VectorStore.', metadata={'source': 'rtdocs/api.python.langchain.com/en/latest/core_api_reference.html'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the plaintext page content like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain_core API Reference¶\n",
      "langchain_core.agents¶\n",
      "Classes¶\n",
      "agents.AgentAction\n",
      "A full description of an action for an ActionAgent to execute.\n",
      "agents.AgentActionMessageLog\n",
      "Override init to support instantiation by position for backward compat.\n",
      "agents.AgentFinish\n",
      "The final return value of an ActionAgent.\n",
      "langchain_core.caches¶\n",
      "Classes¶\n",
      "caches.BaseCache()\n",
      "Base interface for cache.\n",
      "langchain_core.callbacks¶\n",
      "Classes¶\n",
      "callbacks.base.AsyncCallbackHandler()\n",
      "Async callback handler that handles callbacks from LangChain.\n",
      "callbacks.base.BaseCallbackHandler()\n",
      "Base callback handler that handles callbacks from LangChain.\n",
      "callbacks.base.BaseCallbackManager(handlers)\n",
      "Base callback manager that handles callbacks from LangChain.\n",
      "callbacks.base.CallbackManagerMixin()\n",
      "Mixin for callback manager.\n",
      "callbacks.base.ChainManagerMixin()\n",
      "Mixin for chain callbacks.\n",
      "callbacks.base.LLMManagerMixin()\n",
      "Mixin for LLM callbacks.\n",
      "callbacks.base.RetrieverManagerMixin()\n",
      "Mixin for Retriever callbacks.\n",
      "callbacks.base.RunManagerMixin()\n",
      "Mixin for run manager.\n",
      "callbacks.base.ToolManagerMixin()\n",
      "Mixin for tool callbacks.\n",
      "callbacks.manager.AsyncCallbackManager(handlers)\n",
      "Async callback manager that handles callbacks from LangChain.\n",
      "callbacks.manager.AsyncCallbackManagerForChainGroup(...)\n",
      "Async callback manager for the chain group.\n",
      "callbacks.manager.AsyncCallbackManagerForChainRun(*, ...)\n",
      "Async callback manager for chain run.\n",
      "callbacks.manager.AsyncCallbackManagerForLLMRun(*, ...)\n",
      "Async callback manager for LLM run.\n",
      "callbacks.manager.AsyncCallbackManagerForRetrieverRun(*, ...)\n",
      "Async callback manager for retriever run.\n",
      "callbacks.manager.AsyncCallbackManagerForToolRun(*, ...)\n",
      "Async callback manager for tool run.\n",
      "callbacks.manager.AsyncParentRunManager(*, ...)\n",
      "Async Parent Run Manager.\n",
      "callbacks.manager.AsyncRunManager(*, run_id, ...)\n",
      "Async Run Manager.\n",
      "callbacks.manager.BaseRunManager(*, run_id, ...)\n",
      "Base class for run manager (a bound callback manager).\n",
      "callbacks.manager.CallbackManager(handlers)\n",
      "Callback manager that handles callbacks from LangChain.\n",
      "callbacks.manager.CallbackManagerForChainGroup(...)\n",
      "Callback manager for the chain group.\n",
      "callbacks.manager.CallbackManagerForChainRun(*, ...)\n",
      "Callback manager for chain run.\n",
      "callbacks.manager.CallbackManagerForLLMRun(*, ...)\n",
      "Callback manager for LLM run.\n",
      "callbacks.manager.CallbackManagerForRetrieverRun(*, ...)\n",
      "Callback manager for retriever run.\n",
      "callbacks.manager.CallbackManagerForToolRun(*, ...)\n",
      "Callback manager for tool run.\n",
      "callbacks.manager.ParentRunManager(*, ...[, ...])\n",
      "Sync Parent Run Manager.\n",
      "callbacks.manager.RunManager(*, run_id, ...)\n",
      "Sync Run Manager.\n",
      "callbacks.stdout.StdOutCallbackHandler([color])\n",
      "Callback Handler that prints to std out.\n",
      "callbacks.streaming_stdout.StreamingStdOutCallbackHandler()\n",
      "Callback handler for streaming.\n",
      "Functions¶\n",
      "callbacks.manager.ahandle_event(handlers, ...)\n",
      "Generic event handler for AsyncCallbackManager.\n",
      "callbacks.manager.atrace_as_chain_group(...)\n",
      "Get an async callback manager for a chain group in a context manager.\n",
      "callbacks.manager.handle_event(handlers, ...)\n",
      "Generic event handler for CallbackManager.\n",
      "callbacks.manager.trace_as_chain_group(...)\n",
      "Get a callback manager for a chain group in a context manager.\n",
      "langchain_core.chat_history¶\n",
      "Classes¶\n",
      "chat_history.BaseChatMessageHistory()\n",
      "Abstract base class for storing chat message history.\n",
      "langchain_core.chat_sessions¶\n",
      "Classes¶\n",
      "chat_sessions.ChatSession\n",
      "Chat Session represents a single conversation, channel, or other group of messages.\n",
      "langchain_core.documents¶\n",
      "Classes¶\n",
      "documents.base.Document\n",
      "Class for storing a piece of text and associated metadata.\n",
      "documents.transformers.BaseDocumentTransformer()\n",
      "Abstract base class for document transformation systems.\n",
      "langchain_core.embeddings¶\n",
      "Classes¶\n",
      "embeddings.Embeddings()\n",
      "Interface for embedding models.\n",
      "langchain_core.example_selectors¶\n",
      "Logic for selecting examples to include in prompts.\n",
      "Classes¶\n",
      "example_selectors.base.BaseExampleSelector()\n",
      "Interface for selecting examples to include in prompts.\n",
      "example_selectors.length_based.LengthBasedExampleSelector\n",
      "Select examples based on length.\n",
      "example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector\n",
      "ExampleSelector that selects examples based on Max Marginal Relevance.\n",
      "example_selectors.semantic_similarity.SemanticSimilarityExampleSelector\n",
      "Example selector that selects examples based on SemanticSimilarity.\n",
      "Functions¶\n",
      "example_selectors.semantic_similarity.sorted_values(values)\n",
      "Return a list of values in dict sorted by key.\n",
      "langchain_core.exceptions¶\n",
      "Classes¶\n",
      "exceptions.LangChainException\n",
      "General LangChain exception.\n",
      "exceptions.OutputParserException(error[, ...])\n",
      "Exception that output parsers should raise to signify a parsing error.\n",
      "exceptions.TracerException\n",
      "Base class for exceptions in tracers module.\n",
      "langchain_core.language_models¶\n",
      "Classes¶\n",
      "language_models.base.BaseLanguageModel\n",
      "Abstract base class for interfacing with language models.\n",
      "language_models.chat_models.BaseChatModel\n",
      "Base class for Chat models.\n",
      "language_models.chat_models.SimpleChatModel\n",
      "Simple Chat Model.\n",
      "language_models.llms.BaseLLM\n",
      "Base LLM abstract interface.\n",
      "language_models.llms.LLM\n",
      "Base LLM abstract class.\n",
      "Functions¶\n",
      "language_models.llms.create_base_retry_decorator(...)\n",
      "Create a retry decorator for a given LLM and provided list of error types.\n",
      "language_models.llms.get_prompts(params, prompts)\n",
      "Get prompts that are already cached.\n",
      "language_models.llms.update_cache(...)\n",
      "Update the cache and get the LLM output.\n",
      "langchain_core.load¶\n",
      "Serialization and deserialization.\n",
      "Classes¶\n",
      "load.load.Reviver([secrets_map, ...])\n",
      "Reviver for JSON objects.\n",
      "load.serializable.BaseSerialized\n",
      "Base class for serialized objects.\n",
      "load.serializable.Serializable\n",
      "Serializable base class.\n",
      "load.serializable.SerializedConstructor\n",
      "Serialized constructor.\n",
      "load.serializable.SerializedNotImplemented\n",
      "Serialized not implemented.\n",
      "load.serializable.SerializedSecret\n",
      "Serialized secret.\n",
      "Functions¶\n",
      "load.dump.default(obj)\n",
      "Return a default value for a Serializable object or a SerializedNotImplemented object.\n",
      "load.dump.dumpd(obj)\n",
      "Return a json dict representation of an object.\n",
      "load.dump.dumps(obj, *[, pretty])\n",
      "Return a json string representation of an object.\n",
      "load.load.load(obj, *[, secrets_map, ...])\n",
      "Revive a LangChain class from a JSON object.\n",
      "load.load.loads(text, *[, secrets_map, ...])\n",
      "Revive a LangChain class from a JSON string.\n",
      "load.serializable.to_json_not_implemented(obj)\n",
      "Serialize a \"not implemented\" object.\n",
      "load.serializable.try_neq_default(value, ...)\n",
      "langchain_core.memory¶\n",
      "Classes¶\n",
      "memory.BaseMemory\n",
      "Abstract base class for memory in Chains.\n",
      "langchain_core.messages¶\n",
      "Classes¶\n",
      "messages.ai.AIMessage\n",
      "A Message from an AI.\n",
      "messages.ai.AIMessageChunk\n",
      "A Message chunk from an AI.\n",
      "messages.base.BaseMessage\n",
      "The base abstract Message class.\n",
      "messages.base.BaseMessageChunk\n",
      "A Message chunk, which can be concatenated with other Message chunks.\n",
      "messages.chat.ChatMessage\n",
      "A Message that can be assigned an arbitrary speaker (i.e.\n",
      "messages.chat.ChatMessageChunk\n",
      "A Chat Message chunk.\n",
      "messages.function.FunctionMessage\n",
      "A Message for passing the result of executing a function back to a model.\n",
      "messages.function.FunctionMessageChunk\n",
      "A Function Message chunk.\n",
      "messages.human.HumanMessage\n",
      "A Message from a human.\n",
      "messages.human.HumanMessageChunk\n",
      "A Human Message chunk.\n",
      "messages.system.SystemMessage\n",
      "A Message for priming AI behavior, usually passed in as the first of a sequence of input messages.\n",
      "messages.system.SystemMessageChunk\n",
      "A System Message chunk.\n",
      "messages.tool.ToolMessage\n",
      "A Message for passing the result of executing a tool back to a model.\n",
      "messages.tool.ToolMessageChunk\n",
      "A Tool Message chunk.\n",
      "Functions¶\n",
      "messages.base.merge_content(first_content, ...)\n",
      "messages.base.message_to_dict(message)\n",
      "messages.base.messages_to_dict(messages)\n",
      "Convert a sequence of Messages to a list of dictionaries.\n",
      "langchain_core.output_parsers¶\n",
      "Classes¶\n",
      "output_parsers.base.BaseGenerationOutputParser\n",
      "Base class to parse the output of an LLM call.\n",
      "output_parsers.base.BaseLLMOutputParser()\n",
      "Abstract base class for parsing the outputs of a model.\n",
      "output_parsers.base.BaseOutputParser\n",
      "Base class to parse the output of an LLM call.\n",
      "output_parsers.list.CommaSeparatedListOutputParser\n",
      "Parse the output of an LLM call to a comma-separated list.\n",
      "output_parsers.list.ListOutputParser\n",
      "Parse the output of an LLM call to a list.\n",
      "output_parsers.list.MarkdownListOutputParser\n",
      "Parse a markdown list.\n",
      "output_parsers.list.NumberedListOutputParser\n",
      "Parse a numbered list.\n",
      "output_parsers.string.StrOutputParser\n",
      "OutputParser that parses LLMResult into the top likely string.\n",
      "output_parsers.transform.BaseCumulativeTransformOutputParser\n",
      "Base class for an output parser that can handle streaming input.\n",
      "output_parsers.transform.BaseTransformOutputParser\n",
      "Base class for an output parser that can handle streaming input.\n",
      "langchain_core.outputs¶\n",
      "Classes¶\n",
      "outputs.chat_generation.ChatGeneration\n",
      "A single chat generation output.\n",
      "outputs.chat_generation.ChatGenerationChunk\n",
      "A ChatGeneration chunk, which can be concatenated with other\n",
      "outputs.chat_result.ChatResult\n",
      "Class that contains all results for a single chat model call.\n",
      "outputs.generation.Generation\n",
      "A single text generation output.\n",
      "outputs.generation.GenerationChunk\n",
      "A Generation chunk, which can be concatenated with other Generation chunks.\n",
      "outputs.llm_result.LLMResult\n",
      "Class that contains all results for a batched LLM call.\n",
      "outputs.run_info.RunInfo\n",
      "Class that contains metadata for a single execution of a Chain or model.\n",
      "langchain_core.prompt_values¶\n",
      "Classes¶\n",
      "prompt_values.ChatPromptValue\n",
      "Chat prompt value.\n",
      "prompt_values.ChatPromptValueConcrete\n",
      "Chat prompt value which explicitly lists out the message types it accepts.\n",
      "prompt_values.PromptValue\n",
      "Base abstract class for inputs to any language model.\n",
      "prompt_values.StringPromptValue\n",
      "String prompt value.\n",
      "langchain_core.prompts¶\n",
      "Prompt is the input to the model.\n",
      "Prompt is often constructed\n",
      "from multiple components. Prompt classes and functions make constructing\n",
      "and working with prompts easy.\n",
      "Class hierarchy:\n",
      "BasePromptTemplate --> PipelinePromptTemplate\n",
      "                       StringPromptTemplate --> PromptTemplate\n",
      "                                                FewShotPromptTemplate\n",
      "                                                FewShotPromptWithTemplates\n",
      "                       BaseChatPromptTemplate --> AutoGPTPrompt\n",
      "                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\n",
      "BaseMessagePromptTemplate --> MessagesPlaceholder\n",
      "                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\n",
      "                                                                  HumanMessagePromptTemplate\n",
      "                                                                  AIMessagePromptTemplate\n",
      "                                                                  SystemMessagePromptTemplate\n",
      "Classes¶\n",
      "prompts.base.BasePromptTemplate\n",
      "Base class for all prompt templates, returning a prompt.\n",
      "prompts.chat.AIMessagePromptTemplate\n",
      "AI message prompt template.\n",
      "prompts.chat.BaseChatPromptTemplate\n",
      "Base class for chat prompt templates.\n",
      "prompts.chat.BaseMessagePromptTemplate\n",
      "Base class for message prompt templates.\n",
      "prompts.chat.BaseStringMessagePromptTemplate\n",
      "Base class for message prompt templates that use a string prompt template.\n",
      "prompts.chat.ChatMessagePromptTemplate\n",
      "Chat message prompt template.\n",
      "prompts.chat.ChatPromptTemplate\n",
      "A prompt template for chat models.\n",
      "prompts.chat.HumanMessagePromptTemplate\n",
      "Human message prompt template.\n",
      "prompts.chat.MessagesPlaceholder\n",
      "Prompt template that assumes variable is already list of messages.\n",
      "prompts.chat.SystemMessagePromptTemplate\n",
      "System message prompt template.\n",
      "prompts.few_shot.FewShotChatMessagePromptTemplate\n",
      "Chat prompt template that supports few-shot examples.\n",
      "prompts.few_shot.FewShotPromptTemplate\n",
      "Prompt template that contains few shot examples.\n",
      "prompts.few_shot_with_templates.FewShotPromptWithTemplates\n",
      "Prompt template that contains few shot examples.\n",
      "prompts.pipeline.PipelinePromptTemplate\n",
      "A prompt template for composing multiple prompt templates together.\n",
      "prompts.prompt.PromptTemplate\n",
      "A prompt template for a language model.\n",
      "prompts.string.StringPromptTemplate\n",
      "String prompt that exposes the format method, returning a prompt.\n",
      "Functions¶\n",
      "prompts.base.format_document(doc, prompt)\n",
      "Format a document into a string based on a prompt template.\n",
      "prompts.loading.load_prompt(path)\n",
      "Unified method for loading a prompt from LangChainHub or local fs.\n",
      "prompts.loading.load_prompt_from_config(config)\n",
      "Load prompt from Config Dict.\n",
      "prompts.string.check_valid_template(...)\n",
      "Check that template string is valid.\n",
      "prompts.string.get_template_variables(...)\n",
      "Get the variables from the template.\n",
      "prompts.string.jinja2_formatter(template, ...)\n",
      "Format a template using jinja2.\n",
      "prompts.string.validate_jinja2(template, ...)\n",
      "Validate that the input variables are valid for the template.\n",
      "langchain_core.retrievers¶\n",
      "Classes¶\n",
      "retrievers.BaseRetriever\n",
      "Abstract base class for a Document retrieval system.\n",
      "langchain_core.runnables¶\n",
      "LangChain Runnable and the LangChain Expression Language (LCEL).\n",
      "The LangChain Expression Language (LCEL) offers a declarative method to build\n",
      "production-grade programs that harness the power of LLMs.\n",
      "Programs created using LCEL and LangChain Runnables inherently support\n",
      "synchronous, asynchronous, batch, and streaming operations.\n",
      "Support for async allows servers hosting LCEL based programs to scale better\n",
      "for higher concurrent loads.\n",
      "Streaming of intermediate outputs as they’re being generated allows for\n",
      "creating more responsive UX.\n",
      "This module contains schema and implementation of LangChain Runnables primitives.\n",
      "Classes¶\n",
      "runnables.base.Runnable()\n",
      "A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
      "runnables.base.RunnableBinding\n",
      "A runnable that delegates calls to another runnable with a set of kwargs.\n",
      "runnables.base.RunnableBindingBase\n",
      "A runnable that delegates calls to another runnable with a set of kwargs.\n",
      "runnables.base.RunnableEach\n",
      "A runnable that delegates calls to another runnable with each element of the input sequence.\n",
      "runnables.base.RunnableEachBase\n",
      "A runnable that delegates calls to another runnable with each element of the input sequence.\n",
      "runnables.base.RunnableGenerator(transform)\n",
      "A runnable that runs a generator function.\n",
      "runnables.base.RunnableLambda(func[, afunc])\n",
      "RunnableLambda converts a python callable into a Runnable.\n",
      "runnables.base.RunnableMap\n",
      "alias of RunnableParallel\n",
      "runnables.base.RunnableParallel\n",
      "A runnable that runs a mapping of runnables in parallel, and returns a mapping of their outputs.\n",
      "runnables.base.RunnableSequence\n",
      "A sequence of runnables, where the output of each is the input of the next.\n",
      "runnables.base.RunnableSerializable\n",
      "A Runnable that can be serialized to JSON.\n",
      "runnables.branch.RunnableBranch\n",
      "A Runnable that selects which branch to run based on a condition.\n",
      "runnables.config.EmptyDict\n",
      "Empty dict type.\n",
      "runnables.config.RunnableConfig\n",
      "Configuration for a Runnable.\n",
      "runnables.configurable.DynamicRunnable\n",
      "A Serializable Runnable that can be dynamically configured.\n",
      "runnables.configurable.RunnableConfigurableAlternatives\n",
      "A Runnable that can be dynamically configured.\n",
      "runnables.configurable.RunnableConfigurableFields\n",
      "A Runnable that can be dynamically configured.\n",
      "runnables.configurable.StrEnum(value[, ...])\n",
      "A string enum.\n",
      "runnables.fallbacks.RunnableWithFallbacks\n",
      "A Runnable that can fallback to other Runnables if it fails.\n",
      "runnables.history.RunnableWithMessageHistory\n",
      "A runnable that manages chat message history for another runnable.\n",
      "runnables.passthrough.RunnableAssign\n",
      "A runnable that assigns key-value pairs to Dict[str, Any] inputs.\n",
      "runnables.passthrough.RunnablePassthrough\n",
      "A runnable to passthrough inputs unchanged or with additional keys.\n",
      "runnables.retry.RunnableRetry\n",
      "Retry a Runnable if it fails.\n",
      "runnables.router.RouterInput\n",
      "A Router input.\n",
      "runnables.router.RouterRunnable\n",
      "A runnable that routes to a set of runnables based on Input['key'].\n",
      "runnables.utils.AddableDict\n",
      "Dictionary that can be added to another dictionary.\n",
      "runnables.utils.ConfigurableField(id[, ...])\n",
      "A field that can be configured by the user.\n",
      "runnables.utils.ConfigurableFieldMultiOption(id, ...)\n",
      "A field that can be configured by the user with multiple default values.\n",
      "runnables.utils.ConfigurableFieldSingleOption(id, ...)\n",
      "A field that can be configured by the user with a default value.\n",
      "runnables.utils.ConfigurableFieldSpec(id, ...)\n",
      "A field that can be configured by the user.\n",
      "runnables.utils.GetLambdaSource()\n",
      "Get the source code of a lambda function.\n",
      "runnables.utils.IsFunctionArgDict()\n",
      "Check if the first argument of a function is a dict.\n",
      "runnables.utils.IsLocalDict(name, keys)\n",
      "Check if a name is a local dict.\n",
      "runnables.utils.SupportsAdd(*args, **kwargs)\n",
      "Protocol for objects that support addition.\n",
      "Functions¶\n",
      "runnables.base.coerce_to_runnable(thing)\n",
      "Coerce a runnable-like object into a Runnable.\n",
      "runnables.config.acall_func_with_variable_args(...)\n",
      "Call function that may optionally accept a run_manager and/or config.\n",
      "runnables.config.call_func_with_variable_args(...)\n",
      "Call function that may optionally accept a run_manager and/or config.\n",
      "runnables.config.ensure_config([config])\n",
      "Ensure that a config is a dict with all keys present.\n",
      "runnables.config.get_async_callback_manager_for_config(config)\n",
      "Get an async callback manager for a config.\n",
      "runnables.config.get_callback_manager_for_config(config)\n",
      "Get a callback manager for a config.\n",
      "runnables.config.get_config_list(config, length)\n",
      "Get a list of configs from a single config or a list of configs.\n",
      "runnables.config.get_executor_for_config(config)\n",
      "Get an executor for a config.\n",
      "runnables.config.merge_configs(*configs)\n",
      "Merge multiple configs into one.\n",
      "runnables.config.patch_config(config, *[, ...])\n",
      "Patch a config with new values.\n",
      "runnables.configurable.make_options_spec(...)\n",
      "Make a ConfigurableFieldSpec for a ConfigurableFieldSingleOption or ConfigurableFieldMultiOption.\n",
      "runnables.passthrough.aidentity(x)\n",
      "An async identity function\n",
      "runnables.passthrough.identity(x)\n",
      "An identity function\n",
      "runnables.utils.aadd(addables)\n",
      "Asynchronously add a sequence of addable objects together.\n",
      "runnables.utils.accepts_config(callable)\n",
      "Check if a callable accepts a config argument.\n",
      "runnables.utils.accepts_run_manager(callable)\n",
      "Check if a callable accepts a run_manager argument.\n",
      "runnables.utils.add(addables)\n",
      "Add a sequence of addable objects together.\n",
      "runnables.utils.gated_coro(semaphore, coro)\n",
      "Run a coroutine with a semaphore.\n",
      "runnables.utils.gather_with_concurrency(n, ...)\n",
      "Gather coroutines with a limit on the number of concurrent coroutines.\n",
      "runnables.utils.get_function_first_arg_dict_keys(func)\n",
      "Get the keys of the first argument of a function if it is a dict.\n",
      "runnables.utils.get_lambda_source(func)\n",
      "Get the source code of a lambda function.\n",
      "runnables.utils.get_unique_config_specs(specs)\n",
      "Get the unique config specs from a sequence of config specs.\n",
      "runnables.utils.indent_lines_after_first(...)\n",
      "Indent all lines of text after the first line.\n",
      "langchain_core.stores¶\n",
      "Classes¶\n",
      "stores.BaseStore()\n",
      "Abstract interface for a key-value store.\n",
      "langchain_core.tools¶\n",
      "Base implementation for tools or skills.\n",
      "Classes¶\n",
      "tools.BaseTool\n",
      "Interface LangChain tools must implement.\n",
      "tools.SchemaAnnotationError\n",
      "Raised when 'args_schema' is missing or has an incorrect type annotation.\n",
      "tools.StructuredTool\n",
      "Tool that can operate on any number of inputs.\n",
      "tools.Tool\n",
      "Tool that takes in function or coroutine directly.\n",
      "tools.ToolException\n",
      "An optional exception that tool throws when execution error occurs.\n",
      "Functions¶\n",
      "tools.create_schema_from_function(...)\n",
      "Create a pydantic schema from a function's signature.\n",
      "tools.tool(*args[, return_direct, ...])\n",
      "Make tools out of functions, can be used with or without arguments.\n",
      "langchain_core.tracers¶\n",
      "Classes¶\n",
      "tracers.base.BaseTracer(**kwargs)\n",
      "Base interface for tracers.\n",
      "tracers.evaluation.EvaluatorCallbackHandler(...)\n",
      "A tracer that runs a run evaluator whenever a run is persisted.\n",
      "tracers.langchain.LangChainTracer([...])\n",
      "An implementation of the SharedTracer that POSTS to the langchain endpoint.\n",
      "tracers.langchain_v1.LangChainTracerV1(**kwargs)\n",
      "An implementation of the SharedTracer that POSTS to the langchain endpoint.\n",
      "tracers.log_stream.LogEntry\n",
      "A single entry in the run log.\n",
      "tracers.log_stream.LogStreamCallbackHandler(*)\n",
      "A tracer that streams run logs to a stream.\n",
      "tracers.log_stream.RunLog(*ops, state)\n",
      "A run log.\n",
      "tracers.log_stream.RunLogPatch(*ops)\n",
      "A patch to the run log.\n",
      "tracers.log_stream.RunState\n",
      "State of the run.\n",
      "tracers.root_listeners.RootListenersTracer(*, ...)\n",
      "tracers.run_collector.RunCollectorCallbackHandler([...])\n",
      "A tracer that collects all nested runs in a list.\n",
      "tracers.schemas.BaseRun\n",
      "Base class for Run.\n",
      "tracers.schemas.ChainRun\n",
      "Class for ChainRun.\n",
      "tracers.schemas.LLMRun\n",
      "Class for LLMRun.\n",
      "tracers.schemas.Run\n",
      "Run schema for the V2 API in the Tracer.\n",
      "tracers.schemas.ToolRun\n",
      "Class for ToolRun.\n",
      "tracers.schemas.TracerSession\n",
      "TracerSessionV1 schema for the V2 API.\n",
      "tracers.schemas.TracerSessionBase\n",
      "Base class for TracerSession.\n",
      "tracers.schemas.TracerSessionV1\n",
      "TracerSessionV1 schema.\n",
      "tracers.schemas.TracerSessionV1Base\n",
      "Base class for TracerSessionV1.\n",
      "tracers.schemas.TracerSessionV1Create\n",
      "Create class for TracerSessionV1.\n",
      "tracers.stdout.ConsoleCallbackHandler(**kwargs)\n",
      "Tracer that prints to the console.\n",
      "tracers.stdout.FunctionCallbackHandler(...)\n",
      "Tracer that calls a function with a single str parameter.\n",
      "Functions¶\n",
      "tracers.context.collect_runs()\n",
      "Collect all run traces in context.\n",
      "tracers.context.register_configure_hook(...)\n",
      "tracers.context.tracing_enabled([session_name])\n",
      "Get the Deprecated LangChainTracer in a context manager.\n",
      "tracers.context.tracing_v2_enabled([...])\n",
      "Instruct LangChain to log all runs in context to LangSmith.\n",
      "tracers.evaluation.wait_for_all_evaluators()\n",
      "Wait for all tracers to finish.\n",
      "tracers.langchain.get_client()\n",
      "Get the client.\n",
      "tracers.langchain.log_error_once(method, ...)\n",
      "Log an error once.\n",
      "tracers.langchain.wait_for_all_tracers()\n",
      "Wait for all tracers to finish.\n",
      "tracers.langchain_v1.get_headers()\n",
      "Get the headers for the LangChain API.\n",
      "tracers.schemas.RunTypeEnum()\n",
      "RunTypeEnum.\n",
      "tracers.stdout.elapsed(run)\n",
      "Get the elapsed time of a run.\n",
      "tracers.stdout.try_json_stringify(obj, fallback)\n",
      "Try to stringify an object to JSON.\n",
      "langchain_core.utils¶\n",
      "Utility functions for LangChain.\n",
      "These functions do not depend on any other LangChain module.\n",
      "Classes¶\n",
      "utils.aiter.NoLock()\n",
      "Dummy lock that provides the proper interface but no protection\n",
      "utils.aiter.Tee(iterable[, n, lock])\n",
      "Create n separate asynchronous iterators over iterable\n",
      "utils.aiter.atee\n",
      "alias of Tee\n",
      "utils.formatting.StrictFormatter()\n",
      "A subclass of formatter that checks for extra keys.\n",
      "utils.iter.NoLock()\n",
      "Dummy lock that provides the proper interface but no protection\n",
      "utils.iter.Tee(iterable[, n, lock])\n",
      "Create n separate asynchronous iterators over iterable\n",
      "utils.iter.safetee\n",
      "alias of Tee\n",
      "Functions¶\n",
      "utils.aiter.py_anext(iterator[, default])\n",
      "Pure-Python implementation of anext() for testing purposes.\n",
      "utils.aiter.tee_peer(iterator, buffer, ...)\n",
      "An individual iterator of a tee()\n",
      "utils.env.env_var_is_set(env_var)\n",
      "Check if an environment variable is set.\n",
      "utils.input.get_bolded_text(text)\n",
      "Get bolded text.\n",
      "utils.input.get_color_mapping(items[, ...])\n",
      "Get mapping for items to a support color.\n",
      "utils.input.get_colored_text(text, color)\n",
      "Get colored text.\n",
      "utils.input.print_text(text[, color, end, file])\n",
      "Print text with highlighting and no end characters.\n",
      "utils.iter.batch_iterate(size, iterable)\n",
      "Utility batching function.\n",
      "utils.iter.tee_peer(iterator, buffer, peers, ...)\n",
      "An individual iterator of a tee()\n",
      "utils.loading.try_load_from_hub(path, ...)\n",
      "Load configuration from hub.\n",
      "utils.pydantic.get_pydantic_major_version()\n",
      "Get the major version of Pydantic.\n",
      "utils.utils.build_extra_kwargs(extra_kwargs, ...)\n",
      "Build extra kwargs from values and extra_kwargs.\n",
      "utils.utils.check_package_version(package[, ...])\n",
      "Check the version of a package.\n",
      "utils.utils.convert_to_secret_str(value)\n",
      "Convert a string to a SecretStr if needed.\n",
      "utils.utils.get_pydantic_field_names(...)\n",
      "Get field names, including aliases, for a pydantic class.\n",
      "utils.utils.guard_import(module_name, *[, ...])\n",
      "Dynamically imports a module and raises a helpful exception if the module is not installed.\n",
      "utils.utils.mock_now(dt_value)\n",
      "Context manager for mocking out datetime.now() in unit tests.\n",
      "utils.utils.raise_for_status_with_text(response)\n",
      "Raise an error with the response text.\n",
      "utils.utils.xor_args(*arg_groups)\n",
      "Validate specified keyword args are mutually exclusive.\n",
      "langchain_core.vectorstores¶\n",
      "Classes¶\n",
      "vectorstores.VectorStore()\n",
      "Interface for vector store.\n",
      "vectorstores.VectorStoreRetriever\n",
      "Base Retriever class for VectorStore.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain_core.language_models.llms.update_cache¶\n",
      "langchain_core.language_models.llms.update_cache(existing_prompts: Dict[int, List], llm_string: str, missing_prompt_idxs: List[int], new_results: LLMResult, prompts: List[str]) → Optional[dict][source]¶\n",
      "Update the cache and get the LLM output.\n"
     ]
    }
   ],
   "source": [
    "print(docs[5].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the source of each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.llms.update_cache.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5].metadata['source'].replace('rtdocs/', 'https://')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, we need to also consider the length of each page with respect to the number of tokens that will reasonably fit within the window of the latest LLMs. We will use `gpt-3.5-turbo` as an example.\n",
    "\n",
    "To count the number of tokens that `gpt-3.5-turbo` will use for some text we need to initialize the `tiktoken` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the tokenizer we defined the encoder as `\"cl100k_base\"`. This is a specific tiktoken encoder which is used by `gpt-3.5-turbo`. Other encoders exist. At the time of writing the OpenAI specific tokenizers (using `tiktoken`) are summarized as:\n",
    "\n",
    "| Encoder | Models |\n",
    "| --- | --- |\n",
    "| `cl100k_base` | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002` |\n",
    "| `p50k_base` | `text-davinci-003`, `code-davinci-002`, `code-cushman-002` |\n",
    "| `r50k_base` | `text-davinci-001`, `davinci`, `text-similarity-davinci-001` |\n",
    "| `gpt2` | `gpt2` |\n",
    "\n",
    "You can find these details in the [Tiktoken `model.py` script](https://github.com/openai/tiktoken/blob/main/tiktoken/model.py), or using `tiktoken.encoding_for_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `tiktoken_len` function, let's count and visualize the number of tokens across our webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = [tiktoken_len(doc.page_content) for doc in docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see `min`, average, and `max` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0\n",
      "Avg: 1552\n",
      "Max: 35097\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Min: {min(token_counts)}\n",
    "Avg: {int(sum(token_counts) / len(token_counts))}\n",
    "Max: {max(token_counts)}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/YAAAIjCAYAAACpnIB8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWDklEQVR4nO3deViU9f7/8RcMoCCKCi4/t3IJXNilQxJGWdpCVtriSXNJ09JS+x4Nl9Rcck075rGySEnNJY/Zqp2O1rEsNbWjgYXknspJAVNiUWDm/v3hYU6TG4wjcI/Px3V56Xzuz/2Z9z3vGfXF3HOPh2EYhgAAAAAAgCl5VnYBAAAAAADAeQR7AAAAAABMjGAPAAAAAICJEewBAAAAADAxgj0AAAAAACZGsAcAAAAAwMQI9gAAAAAAmBjBHgAAAAAAEyPYAwAAAABgYgR7AAAuIiQkRJMnT67sMlBOR48eVUhIiNasWVPZpQAAUCG8KrsAAABcKSQkpEzzlixZotjY2KtcjeutX79e7777rtLS0pSfn6/atWurffv2+vOf/6wOHTpUdnk6fvy4Vq1apTvuuENt2rRxyZqjR4/WZ599pp07d15we0hIiHr16qUJEyZc0f0sW7ZMvr6+6t69+xWtAwBARSPYAwDcyqxZsxxuf/jhh/rmm2/OG2/ZsmVFlnXFDMPQ2LFjtWbNGrVt21aPP/64goKClJWVpfXr16tfv35asWKFoqOjK7XOEydOaP78+WrcuLHLgn15NW7cWKmpqfLyKt9/c1asWKE6deoQ7AEApkOwBwC4lfvvv9/h9vfff69vvvnmvHGzWbRokdasWaO+fftqzJgx8vDwsG8bPHiwPvjgg3IHWXfl4eGhatWqVXYZ5VZYWChfX9/KLgMAYEJ8xh4AcM0pKCjQjBkzlJCQoNDQUN15551auHChDMO47L6vvfaaWrduraVLl9rHvvzyS/Xs2VORkZGKiorSoEGDtHfvXof9Ro8eraioKB0/flxDhgxRVFSUbrrpJs2cOVNWq/WS93nmzBm9+eabatGihUaNGuUQ6ks98MADCg8Pt98+cuSIhg0bpj/96U+KiIjQI488oo0bNzrss2bNGoWEhOjo0aMO499++61CQkL07bff2sd69+6te++9V/v27VPv3r0VERGhjh07Kjk52WG/hx56SJI0ZswYhYSEOHzW/dChQxo6dKhuvvlmhYWF6ZZbbtH//d//6bfffrvk8ZfXhT5jn5WVpTFjxuiWW25RaGio4uPjNXjwYPuxd+rUSXv37tW2bdvsdffu3du+f1keT0k6duyYnnrqKUVGRqpDhw6aNm2aNm3adNHHc/fu3erVq5ciIiL08ssvS5I2bNigQYMGKT4+XqGhobrjjjv06quvnvc8KV1jz549euyxxxQREaHOnTvrH//4hyRp27ZtevjhhxUeHq4777xTmzdvdtljDACoWvjRPgDgmmIYhgYPHmwPoW3atNGmTZs0a9YsHT9+XGPHjr3ovn/961/1xhtvaPLkyXrkkUckSR988IFGjx6t+Ph4jRw5UoWFhVqxYoV69uyp999/X02aNLHvb7VaNWDAAIWHhyspKUlbtmzRokWL1LRpU/Xs2fOi9/vdd9/p1KlT6tOnjywWy2WPMTs7W3/+859VWFio3r17q06dOnr//fc1ePBgzZs3T507dy7HI/Y/p0+f1hNPPKHOnTvr7rvv1meffabZs2crODhYCQkJatmypYYNG6Z58+apR48eat++vSQpOjpaRUVFGjBggIqKivTYY48pKChIx48f18aNG5Wbm6uaNWte9v5PnjzpVN2SNHToUO3bt0+PPfaYGjdurJMnT+qbb77Rf/7zHzVp0kRjx47VlClT5Ofnp6eeekqSFBQUJKnsj2dBQYH69u2rrKws9enTR0FBQfrkk08cAv3vnTp1SgMHDlRiYqLuu+8+BQYGSpLef/99+fn56fHHH5efn5+2bt2qefPmKS8vT6NGjXJY4/Tp03rqqad0zz336K677tKKFSv0l7/8RTabTdOmTdOf//xn3XvvvVq4cKGGDRumjRs3yt/f3+nHEQBQRRkAALixSZMmGcHBwfbb69evN4KDg43XXnvNYd7QoUONkJAQ4/Dhw/ax4OBgY9KkSYZhGMaMGTOM1q1bG2vWrLFvz8vLM2JiYoxx48Y5rJWVlWW0b9/eYXzUqFFGcHCwMX/+fIe5DzzwgNGtW7dLHsPixYuN4OBgY/369WU65qlTpxrBwcHG9u3bHWrt1KmTcdtttxlWq9UwDMN47733jODgYOPIkSMO+2/dutUIDg42tm7dah977LHHjODgYOP999+3j509e9a4+eabjaFDh9rHUlNTjeDgYOO9995zWPPHH380goODjU8//bRMx/B7pY/dpX6V9skwDOPIkSMONZw+fdoIDg423nrrrUveT2JiovHYY4+dN17Wx3PRokXn9enMmTPGXXfdddHHc8WKFefdX2Fh4Xlj48ePNyIiIoyzZ8+et8bHH39sH9u/f78RHBxstG7d2ti1a5d9fNOmTRfsCwDAPXAqPgDgmvLVV1/JYrE4nGYtSf3795dhGPrqq68cxg3D0OTJk7VkyRK99NJL6tatm33b5s2blZubq8TERJ08edL+y9PTUxERERd8p/bRRx91uN2+ffvzToX/o7y8PElSjRo1ynSMX375pcLDwxUTE2Mfq1Gjhnr06KFjx45p3759ZVrnj/z8/ByuVeDj46OwsDAdOXLksvuWvkv89ddfq7CwsNz3Xa1aNaWkpFzw1+VUr15d3t7e2rZtm06fPl3u+y7r47lp0yY1aNBAt99+u0PdpWd3/JGPj88FL9RXvXp1+5/z8vJ08uRJxcTEqLCwUAcOHHCY6+fnp8TERPvtFi1aqFatWmrZsqUiIiLs46V/LkuvAADmw6n4AIBryrFjx1S/fv3zTkcuvUr+sWPHHMY/+OADFRQUaOLEibr33nsdth06dEiS1Ldv3wve1x/vo1q1aqpbt67DWEBAwGXDZuk6+fn5l5xXKjMz0yHUlWrRooV9e3BwcJnW+r2GDRue9/n+gIAAZWRkXHbfpk2b6vHHH1dKSoo+/vhjxcTEqFOnTrrvvvvKdBq+xWJRXFxcuWuWzgXokSNHaubMmbr55psVERGhW2+9VQ888IDq1at32f3L+ngeO3ZMzZo1O+8xatas2QXXbdCggXx8fM4b37t3r+bOnautW7faf6hT6o/XI7hQT2rWrKmGDRueNyZJubm5F6wFAGBuBHsAAC4hOjpae/bs0bJly3T33Xerdu3a9m3Gfy+2N2vWrAsGxD9+Hr4sn4+/kNIAmZGRoTvuuMOpNS7kQhfhkySbzXbBcWfrLzV69Gh169ZNn3/+ub755hu9+OKLeuONN7Rq1arzgqir9evXT506ddKGDRv09ddf65VXXtGbb76pxYsXq23btlf1vi/m9+/Ml8rNzdVjjz0mf39/DRs2TM2aNVO1atX0ww8/aPbs2ef15mI9udi4UYYLRAIAzIdT8QEA15TGjRvrxIkT570TWnqKc+PGjR3Gr7vuOi1cuFAnTpzQE0884bBf06ZNJUmBgYGKi4s771dsbKxLam7fvr0CAgK0du3ay15BX5IaNWqkgwcPnjdeeoyNGjWSJNWqVUvS+e8C//GshfK42A8LSoWEhGjIkCFatmyZli1bpuPHj2vFihVO3195NGvWTP3799eiRYv0ySefqLi4WIsWLbJvv1jtZX08GzdurJ9//vm88Pzzzz+XucZt27bp1KlTmjFjhvr27avbbrtNcXFxCggIKPMaAIBrD8EeAHBNueWWW2S1WrVs2TKH8bffflseHh665ZZbztundevWevPNN7V//34NHjxYZ86ckSR17NhR/v7+euONN1RcXHzefldyFfff8/X11RNPPKH9+/dr9uzZF3zX9cMPP1RqaqokKSEhQampqdq5c6d9e0FBgVatWqXGjRurVatWkv53ivj27dvt86xWq1atWnVFtUrnn/Kdl5enkpISh7Hg4GB5enqqqKjI6fsri8LCQp09e9ZhrFmzZqpRo4bDffv6+l7wVPWyPp7x8fE6fvy4Pv/8c/u8s2fPluvx9PQ891+z3/e4qKhIy5cvL/MaAIBrD6fiAwCuKZ06dVJsbKz++te/6tixYwoJCdE333yjzz//XH379r3o56EjIyP12muvadCgQRo2bJheffVV+fv7a+LEiUpKSlL37t11zz33qG7dusrMzNSXX36p6OhoTZgwwSV1P/HEE9q3b58WLVqkb7/9VnfeeaeCgoKUnZ2tDRs2KDU1VStXrpQkDRo0SGvXrtXAgQPVu3dvBQQE6IMPPtDRo0f1t7/9zR4eb7jhBkVGRurll1/W6dOnFRAQoHXr1p0XwMujWbNmqlWrllauXKkaNWrIz89P4eHhysjI0OTJk3XXXXfp+uuvl9Vq1YcffiiLxaI777zTJY/RxRw6dEj9+vXTXXfdpVatWslisWjDhg3Kzs52uPBcu3bttGLFCr322mu67rrrVLduXXXo0KHMj2ePHj30zjvvaMSIEerTp4/q1aunjz/+WNWqVZN0+bMZJCkqKkoBAQEaPXq0evfuLQ8PD3344YecQg8AuCSCPQDgmuLp6anXX39d8+bN07p167RmzRo1btxYSUlJ6t+//yX37dChg+bOnathw4YpKSlJc+bMUdeuXVW/fn29+eabWrhwoYqKitSgQQPFxMRc8IrnV1L3rFmzdPvtt2vVqlVatGiR8vLyVKdOHd1444167rnnFBUVJenc96+vXLlSL730kt555x2dPXtWISEhWrBggW699VaHdWfPnq0JEybozTffVK1atfTQQw8pNjZWjz/+uFN1ent7a8aMGXr55Zc1ceJElZSUaPr06brxxhsVHx+vf/3rXzp+/Lh8fX0VEhKi5ORkRUZGXuGjc2kNGzZUYmKitmzZoo8++kgWi0UtWrTQ3LlzHX6o8PTTTyszM1NvvfWW8vPz9ac//UkdOnQo8+NZo0YNLV68WC+++KKWLFkiPz8/PfDAA4qKitLQoUPtAf9S6tSpowULFmjmzJmaO3euatWqpfvuu08dOnTQgAEDrsbDAwBwAx4GPwIGAAC4at5++21Nnz5dX331lRo0aFDZ5QAA3BCfsQcAAHCR0usvlDp79qzeffddXX/99YR6AMBVw6n4AAAALvLMM8+oUaNGat26tfLy8vTRRx/pwIEDmj17dmWXBgBwY5yKDwAA4CJvv/22Vq9erWPHjslqtapVq1Z64okndM8991R2aQAAN0awBwAAAADAxPiMPQAAAAAAJkawBwAAAADAxLh4XhnYbDaVlJTI09NTHh4elV0OAAAAAMDNGYYhm80mLy8veXpe+j15gn0ZlJSUKC0trbLLAAAAAABcY8LCwuTj43PJOQT7Mij96UhYWJgsFkslV3NpVqtVaWlppqgV5Ud/3R89dm/0173RX/dHj90b/XV/Zutxab2Xe7deItiXSenp9xaLxRRPAMlctaL86K/7o8fujf66N/rr/uixe6O/7s9sPS7Lx8G5eB4AAAAAACZGsAcAAAAAwMQI9gAAAAAAmBjBHgAAAAAAEyPYAwAAAABgYgR7AAAAAABMjGAPAAAAAICJEewBAAAAADAxgj0AAAAAACZGsAcAAAAAwMQqNdhv375dTz31lOLj4xUSEqINGzZcdO6ECRMUEhKit99+22H81KlTGjFihKKjoxUTE6OxY8cqPz/fYc6ePXvUs2dPhYWFKSEhQcnJyVfjcAAAAAAAqHCVGuwLCgoUEhKiF1544ZLz1q9fr++//17169c/b9vIkSO1b98+paSkaMGCBdqxY4cmTJhg356Xl6cBAwaoUaNGWrNmjZKSkjR//ny9++67Lj8eAAAAAAAqmldl3nlCQoISEhIuOef48eOaMmWKFi5cqCeffNJh2/79+7Vp0yatXr1aYWFhkqRx48Zp0KBBSkpKUoMGDfTRRx+puLhY06ZNk4+Pj2644Qalp6crJSVFPXr0uGrHBgAAAABARajUYH85NptNzz33nAYMGKAbbrjhvO07d+5UrVq17KFekuLi4uTp6anU1FR17txZu3btUkxMjHx8fOxz4uPjlZycrNOnTysgIKDM9Vit1is7oApQWqMZakX50V/3R4/dG/11b/TX/dFj90Z/3Z/ZelyeOqt0sE9OTpaXl5f69Olzwe3Z2dmqW7euw5iXl5cCAgKUlZVln9OkSROHOUFBQfZt5Qn2aWlp5Sm/UpmpVpQf/XV/9Ni90V/3Rn/dHz12b/TX/bljj6tssN+9e7eWLFmiNWvWyMPDo7LLkSSFhYXJYrFUdhmXZLValZaWZopaUX701/3RY/dGf90b/XV/9Ni90V/3Z7Yel9ZbFlU22O/YsUM5OTm67bbb7GNWq1UzZ87UkiVL9MUXXygoKEgnT5502K+kpESnT59WvXr1JJ17dz47O9thTunt0nfuy8pisZjiCSCZq1aUH/11f/TYvdFf90Z/3R89dm/01/25Y4+rbLC///77FRcX5zA2YMAA3X///erevbskKSoqSrm5udq9e7dCQ0MlSVu3bpXNZlN4eLgkKTIyUnPnzlVxcbG8vb0lSZs3b1bz5s3LdRo+AAAAAABVUaUG+/z8fP3888/220ePHlV6eroCAgLUqFEj1alTx2G+t7e3goKC1KJFC0lSy5Yt1bFjR40fP16TJk1ScXGxpkyZosTERDVo0ECS1LVrV7366qt6/vnnNXDgQO3du1dLlizRmDFjKu5AK1jpDzAAAAAAAO6vUoP97t27HS6MN336dElSt27dNGPGjDKtMXv2bE2ZMkV9+/aVp6enunTponHjxtm316xZUwsXLtTkyZPVvXt31alTR0OGDHHrr7pr267dFZ9aYrMZ8vSsGtc2AAAAAABcXKUG+9jYWGVkZJR5/hdffHHeWO3atTVnzpxL7te6dWstX7683PWZlZfFohVf/KITp4qd2r9+bW892qmhi6sCAAAAAFwNVfYz9rgyJ34tVubJs5VdBgAAAADgKvOs7AIAAAAAAIDzCPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABOr1GC/fft2PfXUU4qPj1dISIg2bNhg31ZcXKyXXnpJXbt2VWRkpOLj45WUlKTjx487rHHq1CmNGDFC0dHRiomJ0dixY5Wfn+8wZ8+ePerZs6fCwsKUkJCg5OTkCjk+AAAAAACutkoN9gUFBQoJCdELL7xw3rYzZ87oxx9/1ODBg7VmzRrNnz9fBw8e1ODBgx3mjRw5Uvv27VNKSooWLFigHTt2aMKECfbteXl5GjBggBo1aqQ1a9YoKSlJ8+fP17vvvnvVjw8AAAAAgKvNqzLvPCEhQQkJCRfcVrNmTaWkpDiMjR8/Xg8//LAyMzPVqFEj7d+/X5s2bdLq1asVFhYmSRo3bpwGDRqkpKQkNWjQQB999JGKi4s1bdo0+fj46IYbblB6erpSUlLUo0ePq36MAAAAAABcTZUa7MsrLy9PHh4eqlWrliRp586dqlWrlj3US1JcXJw8PT2Vmpqqzp07a9euXYqJiZGPj499Tnx8vJKTk3X69GkFBASU+f6tVqvrDuYqsdlsslgsMmTIMJxc5L/7meF4rzWlPaE37oseuzf6697or/ujx+6N/ro/s/W4PHWaJtifPXtWs2fPVmJiovz9/SVJ2dnZqlu3rsM8Ly8vBQQEKCsryz6nSZMmDnOCgoLs28oT7NPS0q7kECqEr6+v2rZtq8LCAuXlFTq1RoFfiSQpIyNDhYXOrYGrywzPRVwZeuze6K97o7/ujx67N/rr/tyxx6YI9sXFxRo+fLgMw9CkSZMqrY6wsDBZLJZKu/+ysNlskiRfXz/5+zvXXj/fapKkkJAQl9UF17BarUpLSzPFcxHOocfujf66N/rr/uixe6O/7s9sPS6ttyyqfLAvLi7Ws88+q8zMTC1evNj+br107p33kydPOswvKSnR6dOnVa9ePfuc7Oxshzmlt0vfuS8ri8ViiieAJHnIQx4eTu8sSaY51muRmZ6LcA49dm/0173RX/dHj90b/XV/7tjjKv099qWh/vDhw3r77bdVp04dh+1RUVHKzc3V7t277WNbt26VzWZTeHi4JCkyMlI7duxQcXGxfc7mzZvVvHnzcp2GDwAAAABAVVSpwT4/P1/p6elKT0+XJB09elTp6enKzMxUcXGxhg0bpt27d2v27NmyWq3KyspSVlaWioqKJEktW7ZUx44dNX78eKWmpuq7777TlClTlJiYqAYNGkiSunbtKm9vbz3//PPau3ev1q1bpyVLlujxxx+vtOMGAAAAAMBVKvVU/N27d6tPnz7229OnT5ckdevWTc8884y++OILSdL999/vsN+SJUsUGxsrSZo9e7amTJmivn37ytPTU126dNG4cePsc2vWrKmFCxdq8uTJ6t69u+rUqaMhQ4bwVXcAAAAAALdQqcE+NjZWGRkZF91+qW2lateurTlz5lxyTuvWrbV8+fJy1wcAAAAAQFVXpT9jDwAAAAAALo1gDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDEKjXYb9++XU899ZTi4+MVEhKiDRs2OGw3DEOvvPKK4uPjFR4ern79+unQoUMOc06dOqURI0YoOjpaMTExGjt2rPLz8x3m7NmzRz179lRYWJgSEhKUnJx8tQ8NAAAAAIAKUanBvqCgQCEhIXrhhRcuuD05OVlLly7VxIkTtWrVKvn6+mrAgAE6e/asfc7IkSO1b98+paSkaMGCBdqxY4cmTJhg356Xl6cBAwaoUaNGWrNmjZKSkjR//ny9++67V/34AAAAAAC42rwq884TEhKUkJBwwW2GYWjJkiUaPHiw7rjjDknSrFmzFBcXpw0bNigxMVH79+/Xpk2btHr1aoWFhUmSxo0bp0GDBikpKUkNGjTQRx99pOLiYk2bNk0+Pj664YYblJ6erpSUFPXo0aPCjhUAAAAAgKuhUoP9pRw9elRZWVmKi4uzj9WsWVMRERHauXOnEhMTtXPnTtWqVcse6iUpLi5Onp6eSk1NVefOnbVr1y7FxMTIx8fHPic+Pl7Jyck6ffq0AgICylyT1Wp1zcFdRTabTRaLRYYMGYaTi/x3PzMc77WmtCf0xn3RY/dGf90b/XV/9Ni90V/3Z7Yel6fOKhvss7KyJEmBgYEO44GBgcrOzpYkZWdnq27dug7bvby8FBAQYN8/OztbTZo0cZgTFBRk31aeYJ+Wlla+g6gEvr6+atu2rQoLC5SXV+jUGgV+JZKkjIwMFRY6twauLjM8F3Fl6LF7o7/ujf66P3rs3uiv+3PHHlfZYF8VhYWFyWKxVHYZl2Sz2SRJvr5+8vd3rr1+vtUkSSEhIS6rC65htVqVlpZmiucinEOP3Rv9dW/01/3RY/dGf92f2XpcWm9ZVNlgX69ePUlSTk6O6tevbx/PyclR69atJZ175/3kyZMO+5WUlOj06dP2/YOCguzv8JcqvV36zn1ZWSwWUzwBJMlDHvLwcHpnSTLNsV6LzPRchHPosXujv+6N/ro/euze6K/7c8ceV9nvsW/SpInq1aunLVu22Mfy8vL0/fffKyoqSpIUFRWl3Nxc7d692z5n69atstlsCg8PlyRFRkZqx44dKi4uts/ZvHmzmjdvXq7T8AEAAAAAqIoqNdjn5+crPT1d6enpks5dMC89PV2ZmZny8PBQnz599Prrr+vzzz9XRkaGkpKSVL9+fftV8lu2bKmOHTtq/PjxSk1N1XfffacpU6YoMTFRDRo0kCR17dpV3t7eev7557V3716tW7dOS5Ys0eOPP15pxw0AAAAAgKtU6qn4u3fvVp8+fey3p0+fLknq1q2bZsyYoYEDB6qwsFATJkxQbm6u2rdvr7feekvVqlWz7zN79mxNmTJFffv2laenp7p06aJx48bZt9esWVMLFy7U5MmT1b17d9WpU0dDhgzhq+4AAAAAAG6hUoN9bGysMjIyLrrdw8NDw4cP1/Dhwy86p3bt2pozZ84l76d169Zavny503UCAAAAAFBVVdnP2AMAAAAAgMsj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDEnAr2R44ccXUdAAAAAADACU4F+86dO6t379768MMPdfbsWVfXBAAAAAAAysipYP/+++8rJCREM2bM0M0336wJEyYoNTXV1bUBAAAAAIDLcCrYt2nTRuPGjdOmTZs0bdo0nThxQj179tS9996rlJQUnTx50tV1AgAAAACAC7iii+d5eXmpS5cumjdvnkaOHKnDhw9r5syZSkhIUFJSkk6cOOGqOgEAAAAAwAVcUbBPS0vTxIkTFR8fr5SUFPXv31/r169XSkqKTpw4oSFDhlxRcVarVXPnzlWnTp0UHh6uO+64Q6+++qoMw7DPMQxDr7zyiuLj4xUeHq5+/frp0KFDDuucOnVKI0aMUHR0tGJiYjR27Fjl5+dfUW0AAAAAAFQFXs7slJKSojVr1ujgwYO65ZZb7O/Se3qe+zlB06ZNNWPGDHXq1OmKiktOTtaKFSs0c+ZMtWrVSrt379aYMWNUs2ZN9enTxz5n6dKlmjFjhpo0aaJXXnlFAwYM0Lp161StWjVJ0siRI5WVlaWUlBQVFxdr7NixmjBhgubMmXNF9QEAAAAAUNmcCvYrVqzQgw8+qG7duql+/foXnFO3bl1NnTr1iorbuXOnbr/9dt16662SpCZNmmjt2rX2C/UZhqElS5Zo8ODBuuOOOyRJs2bNUlxcnDZs2KDExETt379fmzZt0urVqxUWFiZJGjdunAYNGqSkpCQ1aNDgimoEAAAAAKAyORXs//nPf152jo+Pj7p16+bM8nZRUVFatWqVDh48qObNm2vPnj367rvvNHr0aEnS0aNHlZWVpbi4OPs+NWvWVEREhHbu3KnExETt3LlTtWrVsod6SYqLi5Onp6dSU1PVuXPnMtdjtVqv6Hgqgs1mk8VikSFDv/vEQvn8dz8zHO+1prQn9MZ90WP3Rn/dG/11f/TYvdFf92e2HpenTqeC/XvvvSc/Pz/dfffdDuOffvqpzpw5c8WBvtSgQYOUl5enu+++WxaLRVarVf/3f/+n++67T5KUlZUlSQoMDHTYLzAwUNnZ2ZKk7Oxs1a1b12G7l5eXAgIC7PuXVVpamrOHUmF8fX3Vtm1bFRYWKC+v0Kk1CvxKJEkZGRkqLHRuDVxdZngu4srQY/dGf90b/XV/9Ni90V/35449dirYv/nmm5o0adJ544GBgRo/frzLgv2nn36qjz/+WHPmzFGrVq2Unp6u6dOnq379+i67j/IICwuTxWKp8PstD5vNJkny9fWTv79T7ZWf77lrE4SEhLisLriG1WpVWlqaKZ6LcA49dm/0173RX/dHj90b/XV/Zutxab1l4VTyy8zMVJMmTc4bb9Sokf7zn/84s+QFzZo1S4MGDVJiYqKkc0EzMzNTb7zxhrp166Z69epJknJychw+65+Tk6PWrVtLkoKCgnTy5EmHdUtKSnT69Gn7/mVlsVhM8QSQJA95yMPD6Z0lyTTHei0y03MRzqHH7o3+ujf66/7osXujv+7PHXvs1NfdBQYGKiMj47zxPXv2qHbt2ldak92ZM2fk8Yd0arFY7F9316RJE9WrV09btmyxb8/Ly9P333+vqKgoSec+p5+bm6vdu3fb52zdulU2m03h4eEuqxUAAAAAgMrg1Dv2iYmJmjp1qmrUqKEbb7xRkrRt2zZNmzbN/u66K9x2221asGCBGjVqZD8VPyUlRQ8++KAkycPDQ3369NHrr7+u6667zv51d/Xr17dfJb9ly5bq2LGjxo8fr0mTJqm4uFhTpkxRYmIiV8QHAAAAAJieU8F++PDhOnbsmPr16ycvr3NL2Gw23X///fq///s/lxU3btw4vfLKK5o0aZL9dPsePXro6aefts8ZOHCgCgsLNWHCBOXm5qp9+/Z666237N9hL0mzZ8/WlClT1LdvX3l6eqpLly4aN26cy+oEAAAAAKCyOBXsfXx8NHfuXB08eFB79uxR9erVFRwcrMaNG7u0OH9/fz3//PN6/vnnLzrHw8NDw4cP1/Dhwy86p3bt2pozZ45LawMAAAAAoCpw7rLp/9W8eXM1b97cVbUAAAAAAIBycirYW61WrVmzRlu3blVOTo79K9ZKLVmyxCXFAQAAAACAS3Mq2E+dOlXvv/++EhISdMMNN5x35XoAAAAAAFAxnAr2a9eu1dy5c5WQkODqegAAAAAAQDk49T323t7eatasmatrAQAAAAAA5eRUsO/fv7+WLFkiwzBcXQ8AAAAAACgHp07F/+677/Ttt9/qq6++0g033GD/LvtS8+fPd0lxAAAAAADg0pwK9rVq1VLnzp1dXQsAAAAAACgnp4L99OnTXV0HAAAAAABwglOfsZekkpISbd68WStXrlReXp4k6fjx48rPz3dZcQAAAAAA4NKcesf+2LFjeuKJJ/Sf//xHRUVFuvnmm+Xv76/k5GQVFRVp8uTJrq4TAAAAAABcgFPv2E+dOlWhoaHatm2bqlWrZh/v3Lmztm7d6rLiAAAAAADApTl9VfwVK1bIx8fHYbxx48Y6fvy4SwoDAAAAAACX59Q79jabTTab7bzxX375RTVq1LjiogAAAAAAQNk4FexvvvlmLV682GEsPz9ff/vb35SQkOCSwgAAAAAAwOU5FexHjx6tf//737rnnntUVFSkkSNHqlOnTjp+/LhGjhzp6hoBAAAAAMBFOPUZ+4YNG+rDDz/U2rVrlZGRoYKCAj300EPq2rWrqlev7uoaAQAAAADARTgV7CXJy8tL999/vytrAQAAAAAA5eRUsP/ggw8uuf2BBx5wZlkAAAAAAFBOTgX7qVOnOtwuKSlRYWGhvL295evrS7AHAAAAAKCCOBXst2/fft7YoUOHNHHiRA0YMOCKiwIAAAAAAGXj1FXxL+T666/XiBEjzns3HwAAAAAAXD0uC/bSuQvqnThxwpVLAgAAAACAS3DqVPzPP//c4bZhGMrKytKyZcsUHR3tksIAAAAAAMDlORXsn376aYfbHh4eqlu3rm666SaNGjXKJYUBAAAAAIDLcyrY79mzx9V1oAqp6WuRzWbI09PD6TWudH8AAAAAQNk4Fezh3qpX85Snp4dWfPGLTpwqLvf+9Wt769FODa9CZQAAAACAP3Iq2E+fPr3Mc8eMGePMXaAKOHGqWJk5Zyu7DAAAAADAJTgV7H/88Uelp6erpKREzZs3l3Tue+w9PT3Vtm1b+zwPD07FBgAAAADganIq2Hfq1Ek1atTQzJkzFRAQIEk6ffq0xowZo5iYGPXv39+lRQIAAAAAgAtz6nvsFy1apBEjRthDvSQFBATo2Wef1aJFi1xWHAAAAAAAuDSngn1eXp5Onjx53vjJkyeVn59/xUUBAAAAAICycSrYd+7cWWPGjNE///lP/fLLL/rll1/02Wef6fnnn1eXLl1cXSMAAAAAALgIpz5jP2nSJM2cOVMjRoxQSUmJJMliseihhx5SUlKSSwsEAAAAAAAX51Sw9/X11cSJE5WUlKSff/5ZktSsWTP5+fm5tDgAAAAAAHBpTp2KXyorK0tZWVm6/vrr5efnJ8MwXFUXAAAAAAAoA6fesf/111/17LPP6ttvv5WHh4f++c9/qmnTpho7dqwCAgI0evRoV9cJAAAAAAAuwKl37KdPny4vLy9t3LhR1atXt4/fc8892rRpk8uKAwAAAAAAl+bUO/bffPONFi5cqIYNGzqMX3/99crMzHRJYQAAAAAA4PKcese+oKDA4Z36UqdOnZKPj88VFwUAAAAAAMrGqWAfExOjDz74wGHMZrPprbfeUmxsrCvqAgAAAAAAZeDUqfjPPfec+vXrp927d6u4uFgvvfSS9u3bp9OnT2vFihWurhEAAAAAAFyEU8E+ODhYn332md555x3VqFFDBQUF6ty5s3r16qX69eu7ukYAAAAAAHAR5Q72xcXFeuKJJzRp0iQNHjz4atQEAAAAAADKqNyfsff29lZGRsbVqAUAAAAAAJSTUxfPu++++7R69WpX1wIAAAAAAMrJqc/YW61WrVixQps3b1ZoaKh8fX0dto8ZM8YlxQEAAAAAgEsrV7A/cuSIGjdurJ9++klt27aVJB08eNBhjoeHh+uqAwAAAAAAl1SuYN+lSxd9/fXXWrp0qSTp2Wef1bhx4xQUFHRVigMAAAAAAJdWrs/YG4bhcPurr75SYWGhSwsCAAAAAABl59TF80r9MegDAAAAAICKVa5g7+HhwWfoAQAAAACoQsr1GXvDMDR69Gj5+PhIkoqKijRx4sTzroo/f/5811UIAAAAAAAuqlzv2Hfr1k2BgYGqWbOmatasqfvuu0/169e33y795UrHjx/XyJEjFRsbq/DwcHXt2lVpaWn27YZh6JVXXlF8fLzCw8PVr18/HTp0yGGNU6dOacSIEYqOjlZMTIzGjh2r/Px8l9YJAAAAAEBlKNc79tOnT79adVzQ6dOn9eijjyo2NlbJycmqU6eODh8+rICAAPuc5ORkLV26VDNmzFCTJk30yiuvaMCAAVq3bp2qVasmSRo5cqSysrKUkpKi4uJijR07VhMmTNCcOXMq9HgAAAAAAHC1cgX7ipacnKyGDRs6/EChadOm9j8bhqElS5Zo8ODBuuOOOyRJs2bNUlxcnDZs2KDExETt379fmzZt0urVqxUWFiZJGjdunAYNGqSkpCQ1aNCgYg8KAAAAAAAXqtLB/osvvlB8fLyGDRum7du3q0GDBurZs6ceeeQRSdLRo0eVlZWluLg4+z41a9ZURESEdu7cqcTERO3cuVO1atWyh3pJiouLk6enp1JTU9W5c+cy12O1Wl13cFeJzWaTxWKRIUNOf2mB8b/fnVrjv/uY4fEym9LHlMfWfdFj90Z/3Rv9dX/02L3RX/dnth6Xp84qHeyPHDmiFStW6PHHH9dTTz2ltLQ0vfjii/L29la3bt2UlZUlSQoMDHTYLzAwUNnZ2ZKk7Oxs1a1b12G7l5eXAgIC7PuX1e8/219V+fr6qm3btiosLFBeXqFTa5w5a5EkFZ45o7y8vHLvX+BXIknKyMhQYaFzNeDSzPBcxJWhx+6N/ro3+uv+6LF7o7/uzx17XKWDvWEYCg0N1V/+8hdJUtu2bbV3716tXLlS3bp1q/B6wsLCZLFYKvx+y8Nms0mSfH395O/vXHurVzv3LQe+1avL37/8X2/o53vu2gYhISFO3T8uzmq1Ki0tzRTPRTiHHrs3+uve6K/7o8fujf66P7P1uLTesqjSwb5evXpq2bKlw1iLFi302Wef2bdLUk5OjurXr2+fk5OTo9atW0uSgoKCdPLkSYc1SkpKdPr0afv+ZWWxWEzxBJAkD3nIo/yZvHRn++9OrfHffczyWJmRmZ6LcA49dm/0173RX/dHj90b/XV/7tjjcn3dXUWLjo7WwYMHHcYOHTqkxo0bS5KaNGmievXqacuWLfbteXl5+v777xUVFSVJioqKUm5urnbv3m2fs3XrVtlsNoWHh1fAUQAAAAAAcPVU6WDft29fff/991qwYIEOHz6sjz/+WKtWrVLPnj0lSR4eHurTp49ef/11ff7558rIyFBSUpLq169vv0p+y5Yt1bFjR40fP16pqan67rvvNGXKFCUmJnJFfAAAAACA6VXpU/HDw8M1f/58vfzyy3r11VfVpEkTjR07Vvfdd599zsCBA1VYWKgJEyYoNzdX7du311tvvWX/DntJmj17tqZMmaK+ffvK09NTXbp00bhx4yrjkAAAAAAAcKkqHewl6bbbbtNtt9120e0eHh4aPny4hg8fftE5tWvX1pw5c65GeQAAAAAAVKoqfSo+AAAAAAC4NII9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMzVbB/8803FRISoqlTp9rHzp49q0mTJik2NlZRUVEaOnSosrOzHfbLzMzUoEGDFBERoQ4dOmjmzJkqKSmp6PIBAAAAAHA50wT71NRUrVy5UiEhIQ7j06ZN07/+9S/NnTtXS5cu1YkTJ/TMM8/Yt1utVj355JMqLi7WypUrNWPGDL3//vuaN29eRR8CAAAAAAAuZ4pgn5+fr+eee04vvviiAgIC7OO//fab3nvvPY0ePVodOnRQaGiopk2bpp07d2rXrl2SpK+//lr79u3TSy+9pDZt2ighIUHDhw/XsmXLVFRUVElHBAAAAACAa3hVdgFlMXnyZCUkJCguLk6vv/66fXz37t0qLi5WXFycfaxly5Zq1KiRdu3apcjISO3atUvBwcEKCgqyz4mPj9fEiRO1b98+tW3btsx1WK1W1xzQVWSz2WSxWGTIkGE4uYjxv9+dWuO/+5jh8TKb0seUx9Z90WP3Rn/dG/11f/TYvdFf92e2Hpenziof7NeuXasff/xRq1evPm9bdna2vL29VatWLYfxwMBAZWVl2ef8PtRLst8unVNWaWlp5ZpfGXx9fdW2bVsVFhYoL6/QqTXOnLVIkgrPnFFeXl659y/wO3f9goyMDBUWOlcDLs0Mz0VcGXrs3uive6O/7o8euzf66/7cscdVOtj/5z//0dSpU7Vo0SJVq1atsstRWFiYLBZLZZdxSTabTZLk6+snf3/n2lu9mu+5NapXl7+/R7n39/M916s/Xg8BV85qtSotLc0Uz0U4hx67N/rr3uiv+6PH7o3+uj+z9bi03rKo0sH+hx9+UE5Ojrp3724fs1qt2r59u5YtW6aFCxequLhYubm5Du/a5+TkqF69epLOvTufmprqsG7pVfNL55SVxWIxxRNAkjzkIY/yZ/LSne2/O7XGf/cxy2NlRmZ6LsI59Ni90V/3Rn/dHz12b/TX/bljj6t0sL/pppv08ccfO4yNGTNGLVq00MCBA/X//t//k7e3t7Zs2aI777xTknTgwAFlZmYqMjJSkhQZGakFCxYoJydHgYGBkqTNmzfL399frVq1qtDjAQAAAADA1ap0sPf391dwcLDDmJ+fn2rXrm0ff/DBBzVjxgwFBATI399fL774oqKiouzBPj4+Xq1atVJSUpKee+45ZWVlae7cuerVq5d8fHwq+pAAAAAAAHCpKh3sy2Ls2LHy9PTUsGHDVFRUpPj4eL3wwgv27RaLRQsWLNDEiRPVo0cP+fr6qlu3bho2bFglVg0AAAAAgGuYLtgvXbrU4Xa1atX0wgsvOIT5P2rcuLGSk5OvdmkAAAAAAFQ4z8ouAAAAAAAAOI9gDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrCHW7PZjCqxBgAAAABcLV6VXQBwNXl6emjFF7/oxKlip/avX9tbj3Zq6OKqAAAAAMB1CPZweydOFSsz52xllwEAAAAAVwWn4gMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmVqWD/RtvvKEHH3xQUVFR6tChg4YMGaIDBw44zDl79qwmTZqk2NhYRUVFaejQocrOznaYk5mZqUGDBikiIkIdOnTQzJkzVVJSUpGHAgAAAADAVVGlg/22bdvUq1cvrVq1SikpKSopKdGAAQNUUFBgnzNt2jT961//0ty5c7V06VKdOHFCzzzzjH271WrVk08+qeLiYq1cuVIzZszQ+++/r3nz5lXGIQEAAAAA4FJVOtgvXLhQ3bt31w033KDWrVtrxowZyszM1A8//CBJ+u233/Tee+9p9OjR6tChg0JDQzVt2jTt3LlTu3btkiR9/fXX2rdvn1566SW1adNGCQkJGj58uJYtW6aioqJKPDoAAAAAAK6cV2UXUB6//fabJCkgIECStHv3bhUXFysuLs4+p2XLlmrUqJF27dqlyMhI7dq1S8HBwQoKCrLPiY+P18SJE7Vv3z61bdu2zPdvtVpddCRXj81mk8VikSFDhuHkIsb/fndqjf/uUxUeL4vF4vxxSFXqWKT/1VFV6oHr0WP3Rn/dG/11f/TYvdFf92e2HpenTtMEe5vNpmnTpik6OlrBwcGSpOzsbHl7e6tWrVoOcwMDA5WVlWWf8/tQL8l+u3ROWaWlpTlbfoXx9fVV27ZtVVhYoLy8QqfWOHPWIkkqPHNGeXl55d6/wO/c9QsyMjJUWOhcDa5Q+lgUFOY7/VhUlWP5IzM8F3Fl6LF7o7/ujf66P3rs3uiv+3PHHpsm2E+aNEl79+7V8uXLK62GsLCwc+8AV2E2m02S5OvrJ39/59pbvZrvuTWqV5e/v0e59/fzrSZJCgkJcer+Xc3Pt4bTj0VVOxar1aq0tDRTPBfhHHrs3uive6O/7o8euzf66/7M1uPSesvCFMF+8uTJ2rhxo9555x01bNjQPh4UFKTi4mLl5uY6vGufk5OjevXq2eekpqY6rFd61fzSOWVlsVhM8QSQJA95yKP8mbx0Z/vvTq3x332qzGPl7HH8d1+pCh3Lf5npuQjn0GP3Rn/dG/11f/TYvdFf9+eOPa7SF88zDEOTJ0/W+vXrtXjxYjVt2tRhe2hoqLy9vbVlyxb72IEDB5SZmanIyEhJUmRkpH766Sfl5OTY52zevFn+/v5q1apVhRzHtaamr0U2m7Mfav8fV6wBAAAAAO6uSr9jP2nSJH3yySd67bXXVKNGDftn4mvWrKnq1aurZs2aevDBBzVjxgwFBATI399fL774oqKiouzBPj4+Xq1atVJSUpKee+45ZWVlae7cuerVq5d8fHwq8ejcV/VqnvL09NCKL37RiVPFTq1Rv7a3Hu3U8PITAQAAAOAaV6WD/YoVKyRJvXv3dhifPn26unfvLkkaO3asPD09NWzYMBUVFSk+Pl4vvPCCfa7FYtGCBQs0ceJE9ejRQ76+vurWrZuGDRtWcQdyjTpxqliZOWcruwwAAAAAcGtVOthnZGRcdk61atX0wgsvOIT5P2rcuLGSk5NdWRoAAAAAAFVClf6MPQAAAAAAuDSCPQAAAAAAJkawBwAAAADAxAj2AAAAAACYGMEeAAAAAAATI9gDAAAAAGBiBHsAAAAAAEyMYA8AAAAAgIkR7AEAAAAAMDGCPQAAAAAAJkawBwAAAADAxAj2AAAAAACYGMEeAAAAAAATI9gDAAAAAGBiBHsAAAAAAEyMYA8AAAAAgIkR7AEAAAAAMDGCPXAJNX0tstmMK17HFWuU8vX1ddlaAAAAAMzPq7ILAKqy6tU85enpoRVf/KITp4qdWqN+bW892qnhFddisxmyWCxq27btFa3h6elxxbUAAAAAqDoI9kAZnDhVrMycs5VaQ+kPGA5lnpKfbw2pnPncVT9gAAAAAFC1EOwBEznxa7GOnCiUv7+XPHjjHQAAAID4jD0AAAAAAKZGsAcAAAAAwMQI9sBV5qor6wMAAADAhfAZe+Aqc8WV9UOa+OquPwW5uDIAAAAA7oBgD1SQK7myfr3a3i6uBgAAAIC74FR8AAAAAABMjGAPAAAAAICJEewBAAAAADAxgj0AAAAAACZGsEeVxFfEAQAAAEDZcFV8VEl8RRwAAAAAlA3BHlUaXxEHAAAAAJfGqfgAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPbANaKmr0U2m3HF67hiDQAAAACu41XZBQCoGNWrecrT00MrvvhFJ04VO7VG/dreerRTQxdXBgAAAOBKEOyBa8yJU8XKzDlb2WUAAAAAcBFOxQcAAAAAwMQI9gAAAAAAmBjBHgAAAAAAEyPYAwAAAABgYgR7AAAAAABMjGAPAAAAAICJEewBAAAAADAxgj0AAAAAACZGsAdQZjV9LbLZjCtexxVrAAAAADjHq7ILqEjLli3TwoULlZWVpdatW2v8+PEKDw+v7LIA06hezVOenh5a8cUvOnGq2Kk16tf21qOdGrq4MgAAAODadc0E+3Xr1mn69OmaNGmSIiIitHjxYg0YMED/+Mc/FBgYWNnlAaZy4lSxMnPOVnYZAAAAAHQNnYqfkpKiRx55RA8++KBatWqlSZMmqXr16nrvvfcquzQAJnW1Ppbg6+t7xesCAADgfO76/6xr4h37oqIi/fDDD3ryySftY56enoqLi9POnTsvu79hGPZ1LBbLVavTFWw2mySpQW2LLB7eTq0R6O8pq9WqhrUtsqj8a1zp/qxx6TXOnq0u3+pe8pBHpdbh7BrNGniruLhEnp7lq/+PbDajSqyx4btfdTqvxKl969fx0U1taskwHGsICQmRJFmt1jKt46rHwjBsV7SGh4enW9VxNY7FZrOpevXqKi4uLlN/q8JjWhVqMIvy9rey0dvyM1uPnXWtPjeulf66gyt5jpb+P6u4uKTKP0dLn4elefRSPIyyzDK548eP65ZbbtHKlSsVFRVlH581a5a2b9+uv//975fcv6ioSGlpaVe7TAAAAAAAHISFhcnHx+eSc66Jd+yvlJeXl8LCwuTp6SkPjyv76SUAAAAAAJdjGIZsNpu8vC4f26+JYF+nTh1ZLBbl5OQ4jOfk5CgoKOiy+3t6el72JyQAAAAAAFSGa+LieT4+PmrXrp22bNliH7PZbNqyZYvDqfkAAAAAAJjNNfGOvSQ9/vjjGjVqlEJDQxUeHq7FixersLBQ3bt3r+zSAAAAAABw2jUT7O+55x6dPHlS8+bNU1ZWltq0aaO33nqrTKfiAwAAAABQVV0TV8UHAAAAAMBdXROfsQcAAAAAwF0R7AEAAAAAMDGCPQAAAAAAJkawBwAAAADAxAj2bmTZsmXq1KmTwsLC9PDDDys1NbWyS8IF/O1vf1NISIjDr7vuusu+/ezZs5o0aZJiY2MVFRWloUOHKjs722GNzMxMDRo0SBEREerQoYNmzpypkpIShznffvutunXrptDQUHXu3Flr1qypkOO71mzfvl1PPfWU4uPjFRISog0bNjhsNwxDr7zyiuLj4xUeHq5+/frp0KFDDnNOnTqlESNGKDo6WjExMRo7dqzy8/Md5uzZs0c9e/ZUWFiYEhISlJycfF4tn376qe666y6FhYWpa9eu+vLLL11+vNeiy/V49OjR572mBwwY4DCHHlddb7zxhh588EFFRUWpQ4cOGjJkiA4cOOAwpyL/XubfctcqS3979+593mt4woQJDnPob9W1fPlyde3aVdHR0YqOjlaPHj0c/m7k9Wtul+svr9/fMeAW1q5da7Rr185YvXq1sXfvXmPcuHFGTEyMkZ2dXdml4Q/mzZtnJCYmGidOnLD/ysnJsW+fMGGCkZCQYGzevNlIS0szHnnkEaNHjx727SUlJca9995r9OvXz/jxxx+NjRs3GrGxscacOXPsc37++WcjIiLCmD59urFv3z5j6dKlRps2bYyvvvqqQo/1WrBx40bj5ZdfNv75z38awcHBxvr16x22v/HGG0b79u2N9evXG+np6cZTTz1ldOrUyThz5ox9zoABA4z77rvP2LVrl7F9+3ajc+fOxl/+8hf79t9++82Ii4szRowYYfz000/GJ598YoSHhxsrV660z/nuu++MNm3aGMnJyca+ffuMv/71r0a7du2MjIyMq/8guLnL9XjUqFHGgAEDHF7Tp06dcphDj6uu/v37G++9957x008/Genp6cbAgQONW2+91cjPz7fPqai/l/m33PXK0t/HHnvMGDdunMNr+LfffrNvp79V2+eff25s3LjROHjwoHHgwAHj5ZdfNtq1a2f89NNPhmHw+jW7y/WX1+//EOzdxEMPPWRMmjTJfttqtRrx8fHGG2+8UYlV4ULmzZtn3HfffRfclpuba7Rr18749NNP7WP79u0zgoODjZ07dxqGcS5ktG7d2sjKyrLPWb58uREdHW2cPXvWMAzDmDVrlpGYmOiw9rPPPmv079/fxUeD3/tj6LPZbMbNN99svPXWW/ax3NxcIzQ01Pjkk08Mw/hff1NTU+1zvvzySyMkJMT45ZdfDMMwjGXLlhk33nijvb+GYRgvvfSSceedd9pvDx8+3Bg0aJBDPQ8//LAxfvx41x7kNe5iwX7w4MEX3Ycem0tOTo4RHBxsbNu2zTCMiv17mX/Lr74/9tcwzgWDF1988aL70F/zufHGG41Vq1bx+nVTpf01DF6/v8ep+G6gqKhIP/zwg+Li4uxjnp6eiouL086dOyuxMlzM4cOHFR8fr9tvv10jRoxQZmamJGn37t0qLi526GXLli3VqFEj7dq1S5K0a9cuBQcHKygoyD4nPj5eeXl52rdvn31Ohw4dHO4zPj7evgYqxtGjR5WVleXQz5o1ayoiIsL+2ty5c6dq1aqlsLAw+5y4uDh5enraT/HatWuXYmJi5OPjY58THx+vgwcP6vTp0/Y59LzybNu2TR06dNCdd96pF154Qb/++qt9Gz02l99++02SFBAQIKni/l7m3/KK8cf+lvr4448VGxure++9V3PmzFFhYaF9G/01D6vVqrVr16qgoEBRUVG8ft3MH/tbitfvOV6VXQCu3K+//iqr1arAwECH8cDAwPM+R4bKFx4erunTp6t58+bKysrSq6++ql69eunjjz9Wdna2vL29VatWLYd9AgMDlZWVJUnKzs52+MtJkv325ebk5eXpzJkzql69+tU6PPxOaT8u9Nos/Xxfdna26tat67Ddy8tLAQEBDv1s0qSJw5zS/mZnZysgIOCCPf/9/eDq6dixozp37qwmTZroyJEjevnllzVw4EC9++67slgs9NhEbDabpk2bpujoaAUHB0tShf29fPr0af4tv8ou1F9Juvfee9WoUSPVr19fGRkZmj17tg4ePKj58+dLor9mkJGRoT//+c86e/as/Pz89Oqrr6pVq1ZKT0/n9esGLtZfidfv7xHsgQqWkJBg/3Pr1q0VERGh2267TZ9++imBGzChxMRE+59LL9xzxx132N/Fh3lMmjRJe/fu1fLlyyu7FFwFF+tvjx497H8OCQlRvXr11K9fP/38889q1qxZRZcJJzRv3lwffPCBfvvtN3322WcaNWqU3nnnncouCy5ysf62atWK1+/vcCq+G6hTp44sFotycnIcxnNycs776ROqnlq1aun666/Xzz//rKCgIBUXFys3N9dhTk5OjurVqyfp3E8Q//gOXenty83x9/fnhwcVqLQfl3ptBgUF6eTJkw7bS0pKdPr06TL1/Pfr/HEOfwdUjqZNm6pOnTo6fPiwJHpsFpMnT9bGjRu1ePFiNWzY0D5eUX8v82/51XWx/l5IRESEJDm8hulv1ebj46PrrrtOoaGhGjFihFq3bq0lS5bw+nUTF+vvhVzLr1+CvRvw8fFRu3bttGXLFvuYzWbTli1bHD5/gqopPz9fR44cUb169RQaGipvb2+HXh44cECZmZmKjIyUJEVGRuqnn35y+Mtl8+bN8vf3t5+WFBkZqa1btzrcz+bNm+1roGI0adJE9erVc+hnXl6evv/+e/trMyoqSrm5udq9e7d9ztatW2Wz2RQeHi7pXD937Nih4uJi+5zNmzerefPm9s+J0vOq45dfftGpU6fs/2Ggx1WbYRiaPHmy1q9fr8WLF6tp06YO2yvq72X+Lb86LtffC0lPT5f0v//001/zsdlsKioq4vXrpkr7eyHX9Ou3sq/eB9dYu3atERoaaqxZs8bYt2+fMX78eCMmJsbhCpCoGmbMmGF8++23xpEjR4zvvvvO6NevnxEbG2v/yrsJEyYYt956q7FlyxYjLS3N6NGjxwW/lqV///5Genq68dVXXxk33XTTBb+2Y+bMmca+ffuMd955h6+7u0ry8vKMH3/80fjxxx+N4OBgIyUlxfjxxx+NY8eOGYZx7uvuYmJijA0bNhh79uwxBg8efMGvu3vggQeM77//3tixY4fRpUsXh69Cy83NNeLi4oznnnvO+Omnn4y1a9caERER530VWtu2bY2FCxca+/btM+bNm8dXobnIpXqcl5dnzJgxw9i5c6dx5MgRY/PmzUa3bt2MLl26OFzhnh5XXS+88ILRvn1749tvv3X4uqTCwkL7nIr6e5l/y13vcv09fPiwMX/+fCMtLc04cuSIsWHDBuP22283evXqZV+D/lZts2fPNrZt22YcOXLE2LNnjzF79mwjJCTE+Prrrw3D4PVrdpfqL69fRwR7N7J06VLj1ltvNdq1a2c89NBDxq5duyq7JFzAs88+a9x8881Gu3btjI4dOxrPPvuscfjwYfv2M2fOGBMnTjRuvPFGIyIiwnj66aeNEydOOKxx9OhR44knnjDCw8ON2NhYY8aMGUZxcbHDnK1btxr333+/0a5dO+P222833nvvvQo5vmvN1q1bjeDg4PN+jRo1yjCMc195N3fuXCMuLs4IDQ01+vbtaxw4cMBhjV9//dX4y1/+YkRGRhrR0dHG6NGjjby8PIc56enpxqOPPmqEhoYaHTt2vODXq6xbt87o0qWL0a5dOyMxMdHYuHHj1Tvwa8ilelxYWGj079/fuOmmm4x27doZt912mzFu3Ljz/qGnx1XXhXobHBzs8HdmRf69zL/lrnW5/mZmZhq9evUy/vSnPxmhoaFG586djZkzZzp8D7Zh0N+qbMyYMcZtt91mtGvXzrjpppuMvn372kO9YfD6NbtL9ZfXryMPwzCMyj5rAAAAAAAAOIfP2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAcA06evSoQkJClJ6eXtmlAACAK+RV2QUAAADnhISEXHL7M888o6FDh1ZQNWV3+PBhLViwQN98841Onjyp+vXrKzIyUo8//rjCwsIqrI6jR4/q9ttv1wcffKA2bdpU2P0CAOBqBHsAAEzq66+/tv953bp1mjdvnv7xj3/Yx/z8/CqjrEtKS0tTv379dMMNN2jy5Mlq0aKF8vPz9fnnn2vmzJl65513KrtEAABMh1PxAQAwqXr16tl/1axZUx4eHvbbgYGBSklJ0S233KLQ0FDdf//9+uqrry66ltVq1ZgxY3TXXXcpMzNTkrRhwwZ169ZNYWFhuv322zV//nyVlJTY9wkJCdHf//53Pf3004qIiFCXLl30+eefX/Q+DMPQmDFjdN1112n58uW69dZb1axZM7Vp00bPPPOMXnvtNfvcjIwM9enTR+Hh4YqNjdX48eOVn59v3967d29NnTrVYf0hQ4Zo9OjR9tudOnXSggULNGbMGEVFRenWW2/Vu+++a99+++23S5IeeOABhYSEqHfv3pd7yAEAqJII9gAAuKElS5YoJSVFo0aN0kcffaT4+HgNGTJEhw4dOm9uUVGRhg8frj179mj58uVq1KiRduzYoVGjRqlPnz5at26dJk+erDVr1mjBggUO+86fP1933323PvroI91yyy0aOXKkTp06dcGa0tPTtXfvXvXv31+enuf/F6RWrVqSpIKCAg0YMEABAQFavXq15s6dq82bN2vKlCnlfhxSUlIUGhqqDz74QD179tTEiRN14MABSdLf//53SdLbb7+tr7/+Wn/729/KvT4AAFUBwR4AADe0cOFCDRw4UImJiWrRooWee+45tW7dWosXL3aYl5+fr0GDBunkyZNasmSJ6tatK+lcYB80aJC6deumpk2b6uabb9bw4cO1cuVKh/27deume++9V9ddd53+8pe/qKCgQKmpqResqfSHCi1atLhk7Z988omKioo0c+ZMBQcHq0OHDpowYYI+/PBDZWdnl+txuOWWW9SrVy9dd911GjhwoOrUqaNvv/1WkuzHWrt2bdWrV0+1a9cu19oAAFQVfMYeAAA3k5eXpxMnTig6OtphPDo6Wnv27HEYGzFihBo2bKjFixerevXq9vE9e/bo3//+t8M79FarVWfPnlVhYaF8fX0lOV7Az8/PT/7+/jp58uQV1b9//36FhIQ4XCMgOjpaNptNBw8eVFBQUJnX+n19Hh4eCgoKUk5OzhXVBwBAVUOwBwDgGpaQkKCPPvpIO3fuVIcOHezjBQUFGjp0qLp06XLePtWqVbP/2dvb22Gbh4eHbDbbBe/r+uuvlyQdOHBAbdu2vaK6PTw8ZBiGw9jvP/9fysvL8b86F9oPAACz41R8AADcjL+/v+rXr69///vfDuP//ve/1apVK4exRx99VCNGjNCQIUO0bds2+3jbtm118OBBXXfddef9utDn48uiTZs2atWqlRYtWnTB8J+bmytJatmypTIyMlRQUOBQu6enp5o3by7p3Gn0WVlZ9u1Wq1V79+4tVz2lP5SwWq3lPhYAAKoSgj0AAG5owIABSk5O1rp163TgwAHNnj1be/bsUZ8+fc6b27t3bw0fPlxPPvmkduzYIUl6+umn9eGHH2r+/Pnau3ev9u/fr7Vr1+qvf/2r0zV5eHho+vTpOnTokHr27Kkvv/xSR44c0Z49e/T6669ryJAhkqSuXbvKx8dHo0eP1k8//aStW7dqypQpuv/+++2n4d9000368ssvtXHjRu3fv18TJ060/2CgrAIDA1W9enVt2rRJ2dnZ+u2335w+NgAAKhOn4gMA4Ib69OmjvLw8zZgxQydPnlTLli312muv2U+H/6N+/frJMAwNGjRIb731ljp27KgFCxbo1VdfVXJysry8vNSiRQs9/PDDV1RXeHi43nvvPS1YsEDjxo3Tr7/+qvr16ysqKkpjx46VJPn6+mrhwoWaOnWqHnroIfn6+qpLly4OX2X34IMPas+ePRo1apQsFov69eun2NjYctXi5eWlcePG6dVXX9W8efMUExOjpUuXXtHxAQBQGTwMPmgGAAAAAIBpcSo+AAAAAAAmRrAHAAAAAMDECPYAAAAAAJgYwR4AAAAAABMj2AMAAAAAYGIEewAAAAAATIxgDwAAAACAiRHsAQAAAAAwMYI9AAAAAAAmRrAHAAAAAMDECPYAAAAAAJjY/wea4hgtgou1UQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# set style and color palette for the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "# create histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(token_counts, kde=False, bins=50)\n",
    "\n",
    "# customize the plot info\n",
    "plt.title(\"Token Counts Histogram\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of pages seem to contain a lower number of tokens. But our limits for the number of tokens to add to each chunk is actually smaller than some of the smaller pages. But, how do we decide what this number should be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the Text\n",
    "\n",
    "At the time of writing, `gpt-3.5-turbo` supports a context window of 4096 tokens — that means that input tokens + generated ( / completion) output tokens, cannot total more than 4096 without hitting an error.\n",
    "\n",
    "So we 100% need to keep below this. If we assume a very safe margin of ~2000 tokens for the input prompt into `gpt-3.5-turbo`, leaving ~2000 tokens for conversation history and completion.\n",
    "\n",
    "With this ~2000 token limit we may want to include *five* snippets of relevant information, meaning each snippet can be no more than **400** token long.\n",
    "\n",
    "To create these snippets we use the `RecursiveCharacterTextSplitter` from LangChain. To measure the length of snippets we also need a *length function*. This is a function that consumes text, counts the number of tokens within the text (after tokenization using the `gpt-3.5-turbo` tokenizer), and returns that number. We define it like so:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the length function defined we can initialize our `RecursiveCharacterTextSplitter` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
    "    length_function=tiktoken_len,\n",
    "    separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the text for a document like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunks = text_splitter.split_text(docs[5].page_content) ... page 5 has issue, so ...\n",
    "chunks = text_splitter.split_text(docs[0].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 363)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `docs[5]` we created `2` chunks of token length `346` and `247`.\n",
    "\n",
    "This is for a single document, we need to do this over all of our documents. While we iterate through the docs to create these chunks we will reformat them into a format that looks like:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"abc-0\",\n",
    "        \"text\": \"some important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"abc-1\",\n",
    "        \"text\": \"the next chunk of important document text\",\n",
    "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
    "    }\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"id\"` will be created based on the URL of the text + it's chunk number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.python.langchain.com/en/latest/core_api_reference.html\n",
      "894c068cfc85\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "m = hashlib.md5()  # this will convert URL into unique ID\n",
    "\n",
    "# url = docs[5].metadata['source'].replace('rtdocs/', 'https://')\n",
    "url = docs[0].metadata['source'].replace('rtdocs/', 'https://')\n",
    "print(url)\n",
    "\n",
    "# convert URL to unique ID\n",
    "m.update(url.encode('utf-8'))\n",
    "uid = m.hexdigest()[:12]\n",
    "print(uid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the `uid` alongside chunk number and actual `url` to create the format needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '894c068cfc85-0',\n",
       "  'text': 'langchain_core API Reference¶\\nlangchain_core.agents¶\\nClasses¶\\nagents.AgentAction\\nA full description of an action for an ActionAgent to execute.\\nagents.AgentActionMessageLog\\nOverride init to support instantiation by position for backward compat.\\nagents.AgentFinish\\nThe final return value of an ActionAgent.\\nlangchain_core.caches¶\\nClasses¶\\ncaches.BaseCache()\\nBase interface for cache.\\nlangchain_core.callbacks¶\\nClasses¶\\ncallbacks.base.AsyncCallbackHandler()\\nAsync callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackHandler()\\nBase callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackManager(handlers)\\nBase callback manager that handles callbacks from LangChain.\\ncallbacks.base.CallbackManagerMixin()\\nMixin for callback manager.\\ncallbacks.base.ChainManagerMixin()\\nMixin for chain callbacks.\\ncallbacks.base.LLMManagerMixin()\\nMixin for LLM callbacks.\\ncallbacks.base.RetrieverManagerMixin()\\nMixin for Retriever callbacks.\\ncallbacks.base.RunManagerMixin()\\nMixin for run manager.\\ncallbacks.base.ToolManagerMixin()\\nMixin for tool callbacks.\\ncallbacks.manager.AsyncCallbackManager(handlers)\\nAsync callback manager that handles callbacks from LangChain.\\ncallbacks.manager.AsyncCallbackManagerForChainGroup(...)\\nAsync callback manager for the chain group.\\ncallbacks.manager.AsyncCallbackManagerForChainRun(*,\\xa0...)\\nAsync callback manager for chain run.\\ncallbacks.manager.AsyncCallbackManagerForLLMRun(*,\\xa0...)\\nAsync callback manager for LLM run.\\ncallbacks.manager.AsyncCallbackManagerForRetrieverRun(*,\\xa0...)\\nAsync callback manager for retriever run.\\ncallbacks.manager.AsyncCallbackManagerForToolRun(*,\\xa0...)\\nAsync callback manager for tool run.\\ncallbacks.manager.AsyncParentRunManager(*,\\xa0...)\\nAsync Parent Run Manager.\\ncallbacks.manager.AsyncRunManager(*,\\xa0run_id,\\xa0...)',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-1',\n",
       "  'text': 'Async Parent Run Manager.\\ncallbacks.manager.AsyncRunManager(*,\\xa0run_id,\\xa0...)\\nAsync Run Manager.\\ncallbacks.manager.BaseRunManager(*,\\xa0run_id,\\xa0...)\\nBase class for run manager (a bound callback manager).\\ncallbacks.manager.CallbackManager(handlers)\\nCallback manager that handles callbacks from LangChain.\\ncallbacks.manager.CallbackManagerForChainGroup(...)\\nCallback manager for the chain group.\\ncallbacks.manager.CallbackManagerForChainRun(*,\\xa0...)\\nCallback manager for chain run.\\ncallbacks.manager.CallbackManagerForLLMRun(*,\\xa0...)\\nCallback manager for LLM run.\\ncallbacks.manager.CallbackManagerForRetrieverRun(*,\\xa0...)\\nCallback manager for retriever run.\\ncallbacks.manager.CallbackManagerForToolRun(*,\\xa0...)\\nCallback manager for tool run.\\ncallbacks.manager.ParentRunManager(*,\\xa0...[,\\xa0...])\\nSync Parent Run Manager.\\ncallbacks.manager.RunManager(*,\\xa0run_id,\\xa0...)\\nSync Run Manager.\\ncallbacks.stdout.StdOutCallbackHandler([color])\\nCallback Handler that prints to std out.\\ncallbacks.streaming_stdout.StreamingStdOutCallbackHandler()\\nCallback handler for streaming.\\nFunctions¶\\ncallbacks.manager.ahandle_event(handlers,\\xa0...)\\nGeneric event handler for AsyncCallbackManager.\\ncallbacks.manager.atrace_as_chain_group(...)\\nGet an async callback manager for a chain group in a context manager.\\ncallbacks.manager.handle_event(handlers,\\xa0...)\\nGeneric event handler for CallbackManager.\\ncallbacks.manager.trace_as_chain_group(...)\\nGet a callback manager for a chain group in a context manager.\\nlangchain_core.chat_history¶\\nClasses¶\\nchat_history.BaseChatMessageHistory()\\nAbstract base class for storing chat message history.\\nlangchain_core.chat_sessions¶\\nClasses¶\\nchat_sessions.ChatSession\\nChat Session represents a single conversation, channel, or other group of messages.\\nlangchain_core.documents¶\\nClasses¶\\ndocuments.base.Document',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-2',\n",
       "  'text': 'langchain_core.documents¶\\nClasses¶\\ndocuments.base.Document\\nClass for storing a piece of text and associated metadata.\\ndocuments.transformers.BaseDocumentTransformer()\\nAbstract base class for document transformation systems.\\nlangchain_core.embeddings¶\\nClasses¶\\nembeddings.Embeddings()\\nInterface for embedding models.\\nlangchain_core.example_selectors¶\\nLogic for selecting examples to include in prompts.\\nClasses¶\\nexample_selectors.base.BaseExampleSelector()\\nInterface for selecting examples to include in prompts.\\nexample_selectors.length_based.LengthBasedExampleSelector\\nSelect examples based on length.\\nexample_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector\\nExampleSelector that selects examples based on Max Marginal Relevance.\\nexample_selectors.semantic_similarity.SemanticSimilarityExampleSelector\\nExample selector that selects examples based on SemanticSimilarity.\\nFunctions¶\\nexample_selectors.semantic_similarity.sorted_values(values)\\nReturn a list of values in dict sorted by key.\\nlangchain_core.exceptions¶\\nClasses¶\\nexceptions.LangChainException\\nGeneral LangChain exception.\\nexceptions.OutputParserException(error[,\\xa0...])\\nException that output parsers should raise to signify a parsing error.\\nexceptions.TracerException\\nBase class for exceptions in tracers module.\\nlangchain_core.language_models¶\\nClasses¶\\nlanguage_models.base.BaseLanguageModel\\nAbstract base class for interfacing with language models.\\nlanguage_models.chat_models.BaseChatModel\\nBase class for Chat models.\\nlanguage_models.chat_models.SimpleChatModel\\nSimple Chat Model.\\nlanguage_models.llms.BaseLLM\\nBase LLM abstract interface.\\nlanguage_models.llms.LLM\\nBase LLM abstract class.\\nFunctions¶\\nlanguage_models.llms.create_base_retry_decorator(...)\\nCreate a retry decorator for a given LLM and provided list of error types.\\nlanguage_models.llms.get_prompts(params,\\xa0prompts)\\nGet prompts that are already cached.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-3',\n",
       "  'text': 'Get prompts that are already cached.\\nlanguage_models.llms.update_cache(...)\\nUpdate the cache and get the LLM output.\\nlangchain_core.load¶\\nSerialization and deserialization.\\nClasses¶\\nload.load.Reviver([secrets_map,\\xa0...])\\nReviver for JSON objects.\\nload.serializable.BaseSerialized\\nBase class for serialized objects.\\nload.serializable.Serializable\\nSerializable base class.\\nload.serializable.SerializedConstructor\\nSerialized constructor.\\nload.serializable.SerializedNotImplemented\\nSerialized not implemented.\\nload.serializable.SerializedSecret\\nSerialized secret.\\nFunctions¶\\nload.dump.default(obj)\\nReturn a default value for a Serializable object or a SerializedNotImplemented object.\\nload.dump.dumpd(obj)\\nReturn a json dict representation of an object.\\nload.dump.dumps(obj,\\xa0*[,\\xa0pretty])\\nReturn a json string representation of an object.\\nload.load.load(obj,\\xa0*[,\\xa0secrets_map,\\xa0...])\\nRevive a LangChain class from a JSON object.\\nload.load.loads(text,\\xa0*[,\\xa0secrets_map,\\xa0...])\\nRevive a LangChain class from a JSON string.\\nload.serializable.to_json_not_implemented(obj)\\nSerialize a \"not implemented\" object.\\nload.serializable.try_neq_default(value,\\xa0...)\\nlangchain_core.memory¶\\nClasses¶\\nmemory.BaseMemory\\nAbstract base class for memory in Chains.\\nlangchain_core.messages¶\\nClasses¶\\nmessages.ai.AIMessage\\nA Message from an AI.\\nmessages.ai.AIMessageChunk\\nA Message chunk from an AI.\\nmessages.base.BaseMessage\\nThe base abstract Message class.\\nmessages.base.BaseMessageChunk\\nA Message chunk, which can be concatenated with other Message chunks.\\nmessages.chat.ChatMessage\\nA Message that can be assigned an arbitrary speaker (i.e.\\nmessages.chat.ChatMessageChunk\\nA Chat Message chunk.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-4',\n",
       "  'text': 'messages.chat.ChatMessageChunk\\nA Chat Message chunk.\\nmessages.function.FunctionMessage\\nA Message for passing the result of executing a function back to a model.\\nmessages.function.FunctionMessageChunk\\nA Function Message chunk.\\nmessages.human.HumanMessage\\nA Message from a human.\\nmessages.human.HumanMessageChunk\\nA Human Message chunk.\\nmessages.system.SystemMessage\\nA Message for priming AI behavior, usually passed in as the first of a sequence of input messages.\\nmessages.system.SystemMessageChunk\\nA System Message chunk.\\nmessages.tool.ToolMessage\\nA Message for passing the result of executing a tool back to a model.\\nmessages.tool.ToolMessageChunk\\nA Tool Message chunk.\\nFunctions¶\\nmessages.base.merge_content(first_content,\\xa0...)\\nmessages.base.message_to_dict(message)\\nmessages.base.messages_to_dict(messages)\\nConvert a sequence of Messages to a list of dictionaries.\\nlangchain_core.output_parsers¶\\nClasses¶\\noutput_parsers.base.BaseGenerationOutputParser\\nBase class to parse the output of an LLM call.\\noutput_parsers.base.BaseLLMOutputParser()\\nAbstract base class for parsing the outputs of a model.\\noutput_parsers.base.BaseOutputParser\\nBase class to parse the output of an LLM call.\\noutput_parsers.list.CommaSeparatedListOutputParser\\nParse the output of an LLM call to a comma-separated list.\\noutput_parsers.list.ListOutputParser\\nParse the output of an LLM call to a list.\\noutput_parsers.list.MarkdownListOutputParser\\nParse a markdown list.\\noutput_parsers.list.NumberedListOutputParser\\nParse a numbered list.\\noutput_parsers.string.StrOutputParser\\nOutputParser that parses LLMResult into the top likely string.\\noutput_parsers.transform.BaseCumulativeTransformOutputParser\\nBase class for an output parser that can handle streaming input.\\noutput_parsers.transform.BaseTransformOutputParser',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-5',\n",
       "  'text': 'output_parsers.transform.BaseTransformOutputParser\\nBase class for an output parser that can handle streaming input.\\nlangchain_core.outputs¶\\nClasses¶\\noutputs.chat_generation.ChatGeneration\\nA single chat generation output.\\noutputs.chat_generation.ChatGenerationChunk\\nA ChatGeneration chunk, which can be concatenated with other\\noutputs.chat_result.ChatResult\\nClass that contains all results for a single chat model call.\\noutputs.generation.Generation\\nA single text generation output.\\noutputs.generation.GenerationChunk\\nA Generation chunk, which can be concatenated with other Generation chunks.\\noutputs.llm_result.LLMResult\\nClass that contains all results for a batched LLM call.\\noutputs.run_info.RunInfo\\nClass that contains metadata for a single execution of a Chain or model.\\nlangchain_core.prompt_values¶\\nClasses¶\\nprompt_values.ChatPromptValue\\nChat prompt value.\\nprompt_values.ChatPromptValueConcrete\\nChat prompt value which explicitly lists out the message types it accepts.\\nprompt_values.PromptValue\\nBase abstract class for inputs to any language model.\\nprompt_values.StringPromptValue\\nString prompt value.\\nlangchain_core.prompts¶\\nPrompt is the input to the model.\\nPrompt is often constructed\\nfrom multiple components. Prompt classes and functions make constructing\\nand working with prompts easy.\\nClass hierarchy:\\nBasePromptTemplate --> PipelinePromptTemplate\\n                       StringPromptTemplate --> PromptTemplate\\n                                                FewShotPromptTemplate\\n                                                FewShotPromptWithTemplates\\n                       BaseChatPromptTemplate --> AutoGPTPrompt\\n                                                  ChatPromptTemplate --> AgentScratchPadChatPromptTemplate\\nBaseMessagePromptTemplate --> MessagesPlaceholder\\n                              BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate\\n                                                                  HumanMessagePromptTemplate\\n                                                                  AIMessagePromptTemplate\\n                                                                  SystemMessagePromptTemplate\\nClasses¶\\nprompts.base.BasePromptTemplate\\nBase class for all prompt templates, returning a prompt.\\nprompts.chat.AIMessagePromptTemplate\\nAI message prompt template.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-6',\n",
       "  'text': 'prompts.chat.AIMessagePromptTemplate\\nAI message prompt template.\\nprompts.chat.BaseChatPromptTemplate\\nBase class for chat prompt templates.\\nprompts.chat.BaseMessagePromptTemplate\\nBase class for message prompt templates.\\nprompts.chat.BaseStringMessagePromptTemplate\\nBase class for message prompt templates that use a string prompt template.\\nprompts.chat.ChatMessagePromptTemplate\\nChat message prompt template.\\nprompts.chat.ChatPromptTemplate\\nA prompt template for chat models.\\nprompts.chat.HumanMessagePromptTemplate\\nHuman message prompt template.\\nprompts.chat.MessagesPlaceholder\\nPrompt template that assumes variable is already list of messages.\\nprompts.chat.SystemMessagePromptTemplate\\nSystem message prompt template.\\nprompts.few_shot.FewShotChatMessagePromptTemplate\\nChat prompt template that supports few-shot examples.\\nprompts.few_shot.FewShotPromptTemplate\\nPrompt template that contains few shot examples.\\nprompts.few_shot_with_templates.FewShotPromptWithTemplates\\nPrompt template that contains few shot examples.\\nprompts.pipeline.PipelinePromptTemplate\\nA prompt template for composing multiple prompt templates together.\\nprompts.prompt.PromptTemplate\\nA prompt template for a language model.\\nprompts.string.StringPromptTemplate\\nString prompt that exposes the format method, returning a prompt.\\nFunctions¶\\nprompts.base.format_document(doc,\\xa0prompt)\\nFormat a document into a string based on a prompt template.\\nprompts.loading.load_prompt(path)\\nUnified method for loading a prompt from LangChainHub or local fs.\\nprompts.loading.load_prompt_from_config(config)\\nLoad prompt from Config Dict.\\nprompts.string.check_valid_template(...)\\nCheck that template string is valid.\\nprompts.string.get_template_variables(...)\\nGet the variables from the template.\\nprompts.string.jinja2_formatter(template,\\xa0...)\\nFormat a template using jinja2.\\nprompts.string.validate_jinja2(template,\\xa0...)',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-7',\n",
       "  'text': 'prompts.string.validate_jinja2(template,\\xa0...)\\nValidate that the input variables are valid for the template.\\nlangchain_core.retrievers¶\\nClasses¶\\nretrievers.BaseRetriever\\nAbstract base class for a Document retrieval system.\\nlangchain_core.runnables¶\\nLangChain Runnable and the LangChain Expression Language (LCEL).\\nThe LangChain Expression Language (LCEL) offers a declarative method to build\\nproduction-grade programs that harness the power of LLMs.\\nPrograms created using LCEL and LangChain Runnables inherently support\\nsynchronous, asynchronous, batch, and streaming operations.\\nSupport for async allows servers hosting LCEL based programs to scale better\\nfor higher concurrent loads.\\nStreaming of intermediate outputs as they’re being generated allows for\\ncreating more responsive UX.\\nThis module contains schema and implementation of LangChain Runnables primitives.\\nClasses¶\\nrunnables.base.Runnable()\\nA unit of work that can be invoked, batched, streamed, transformed and composed.\\nrunnables.base.RunnableBinding\\nA runnable that delegates calls to another runnable with a set of kwargs.\\nrunnables.base.RunnableBindingBase\\nA runnable that delegates calls to another runnable with a set of kwargs.\\nrunnables.base.RunnableEach\\nA runnable that delegates calls to another runnable with each element of the input sequence.\\nrunnables.base.RunnableEachBase\\nA runnable that delegates calls to another runnable with each element of the input sequence.\\nrunnables.base.RunnableGenerator(transform)\\nA runnable that runs a generator function.\\nrunnables.base.RunnableLambda(func[,\\xa0afunc])\\nRunnableLambda converts a python callable into a Runnable.\\nrunnables.base.RunnableMap\\nalias of RunnableParallel\\nrunnables.base.RunnableParallel\\nA runnable that runs a mapping of runnables in parallel, and returns a mapping of their outputs.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-8',\n",
       "  'text': \"runnables.base.RunnableSequence\\nA sequence of runnables, where the output of each is the input of the next.\\nrunnables.base.RunnableSerializable\\nA Runnable that can be serialized to JSON.\\nrunnables.branch.RunnableBranch\\nA Runnable that selects which branch to run based on a condition.\\nrunnables.config.EmptyDict\\nEmpty dict type.\\nrunnables.config.RunnableConfig\\nConfiguration for a Runnable.\\nrunnables.configurable.DynamicRunnable\\nA Serializable Runnable that can be dynamically configured.\\nrunnables.configurable.RunnableConfigurableAlternatives\\nA Runnable that can be dynamically configured.\\nrunnables.configurable.RunnableConfigurableFields\\nA Runnable that can be dynamically configured.\\nrunnables.configurable.StrEnum(value[,\\xa0...])\\nA string enum.\\nrunnables.fallbacks.RunnableWithFallbacks\\nA Runnable that can fallback to other Runnables if it fails.\\nrunnables.history.RunnableWithMessageHistory\\nA runnable that manages chat message history for another runnable.\\nrunnables.passthrough.RunnableAssign\\nA runnable that assigns key-value pairs to Dict[str, Any] inputs.\\nrunnables.passthrough.RunnablePassthrough\\nA runnable to passthrough inputs unchanged or with additional keys.\\nrunnables.retry.RunnableRetry\\nRetry a Runnable if it fails.\\nrunnables.router.RouterInput\\nA Router input.\\nrunnables.router.RouterRunnable\\nA runnable that routes to a set of runnables based on Input['key'].\\nrunnables.utils.AddableDict\\nDictionary that can be added to another dictionary.\\nrunnables.utils.ConfigurableField(id[,\\xa0...])\\nA field that can be configured by the user.\\nrunnables.utils.ConfigurableFieldMultiOption(id,\\xa0...)\\nA field that can be configured by the user with multiple default values.\",\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-9',\n",
       "  'text': 'A field that can be configured by the user with multiple default values.\\nrunnables.utils.ConfigurableFieldSingleOption(id,\\xa0...)\\nA field that can be configured by the user with a default value.\\nrunnables.utils.ConfigurableFieldSpec(id,\\xa0...)\\nA field that can be configured by the user.\\nrunnables.utils.GetLambdaSource()\\nGet the source code of a lambda function.\\nrunnables.utils.IsFunctionArgDict()\\nCheck if the first argument of a function is a dict.\\nrunnables.utils.IsLocalDict(name,\\xa0keys)\\nCheck if a name is a local dict.\\nrunnables.utils.SupportsAdd(*args,\\xa0**kwargs)\\nProtocol for objects that support addition.\\nFunctions¶\\nrunnables.base.coerce_to_runnable(thing)\\nCoerce a runnable-like object into a Runnable.\\nrunnables.config.acall_func_with_variable_args(...)\\nCall function that may optionally accept a run_manager and/or config.\\nrunnables.config.call_func_with_variable_args(...)\\nCall function that may optionally accept a run_manager and/or config.\\nrunnables.config.ensure_config([config])\\nEnsure that a config is a dict with all keys present.\\nrunnables.config.get_async_callback_manager_for_config(config)\\nGet an async callback manager for a config.\\nrunnables.config.get_callback_manager_for_config(config)\\nGet a callback manager for a config.\\nrunnables.config.get_config_list(config,\\xa0length)\\nGet a list of configs from a single config or a list of configs.\\nrunnables.config.get_executor_for_config(config)\\nGet an executor for a config.\\nrunnables.config.merge_configs(*configs)\\nMerge multiple configs into one.\\nrunnables.config.patch_config(config,\\xa0*[,\\xa0...])\\nPatch a config with new values.\\nrunnables.configurable.make_options_spec(...)',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-10',\n",
       "  'text': \"Patch a config with new values.\\nrunnables.configurable.make_options_spec(...)\\nMake a ConfigurableFieldSpec for a ConfigurableFieldSingleOption or ConfigurableFieldMultiOption.\\nrunnables.passthrough.aidentity(x)\\nAn async identity function\\nrunnables.passthrough.identity(x)\\nAn identity function\\nrunnables.utils.aadd(addables)\\nAsynchronously add a sequence of addable objects together.\\nrunnables.utils.accepts_config(callable)\\nCheck if a callable accepts a config argument.\\nrunnables.utils.accepts_run_manager(callable)\\nCheck if a callable accepts a run_manager argument.\\nrunnables.utils.add(addables)\\nAdd a sequence of addable objects together.\\nrunnables.utils.gated_coro(semaphore,\\xa0coro)\\nRun a coroutine with a semaphore.\\nrunnables.utils.gather_with_concurrency(n,\\xa0...)\\nGather coroutines with a limit on the number of concurrent coroutines.\\nrunnables.utils.get_function_first_arg_dict_keys(func)\\nGet the keys of the first argument of a function if it is a dict.\\nrunnables.utils.get_lambda_source(func)\\nGet the source code of a lambda function.\\nrunnables.utils.get_unique_config_specs(specs)\\nGet the unique config specs from a sequence of config specs.\\nrunnables.utils.indent_lines_after_first(...)\\nIndent all lines of text after the first line.\\nlangchain_core.stores¶\\nClasses¶\\nstores.BaseStore()\\nAbstract interface for a key-value store.\\nlangchain_core.tools¶\\nBase implementation for tools or skills.\\nClasses¶\\ntools.BaseTool\\nInterface LangChain tools must implement.\\ntools.SchemaAnnotationError\\nRaised when 'args_schema' is missing or has an incorrect type annotation.\\ntools.StructuredTool\\nTool that can operate on any number of inputs.\\ntools.Tool\",\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-11',\n",
       "  'text': \"tools.StructuredTool\\nTool that can operate on any number of inputs.\\ntools.Tool\\nTool that takes in function or coroutine directly.\\ntools.ToolException\\nAn optional exception that tool throws when execution error occurs.\\nFunctions¶\\ntools.create_schema_from_function(...)\\nCreate a pydantic schema from a function's signature.\\ntools.tool(*args[,\\xa0return_direct,\\xa0...])\\nMake tools out of functions, can be used with or without arguments.\\nlangchain_core.tracers¶\\nClasses¶\\ntracers.base.BaseTracer(**kwargs)\\nBase interface for tracers.\\ntracers.evaluation.EvaluatorCallbackHandler(...)\\nA tracer that runs a run evaluator whenever a run is persisted.\\ntracers.langchain.LangChainTracer([...])\\nAn implementation of the SharedTracer that POSTS to the langchain endpoint.\\ntracers.langchain_v1.LangChainTracerV1(**kwargs)\\nAn implementation of the SharedTracer that POSTS to the langchain endpoint.\\ntracers.log_stream.LogEntry\\nA single entry in the run log.\\ntracers.log_stream.LogStreamCallbackHandler(*)\\nA tracer that streams run logs to a stream.\\ntracers.log_stream.RunLog(*ops,\\xa0state)\\nA run log.\\ntracers.log_stream.RunLogPatch(*ops)\\nA patch to the run log.\\ntracers.log_stream.RunState\\nState of the run.\\ntracers.root_listeners.RootListenersTracer(*,\\xa0...)\\ntracers.run_collector.RunCollectorCallbackHandler([...])\\nA tracer that collects all nested runs in a list.\\ntracers.schemas.BaseRun\\nBase class for Run.\\ntracers.schemas.ChainRun\\nClass for ChainRun.\\ntracers.schemas.LLMRun\\nClass for LLMRun.\\ntracers.schemas.Run\\nRun schema for the V2 API in the Tracer.\\ntracers.schemas.ToolRun\",\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-12',\n",
       "  'text': 'Run schema for the V2 API in the Tracer.\\ntracers.schemas.ToolRun\\nClass for ToolRun.\\ntracers.schemas.TracerSession\\nTracerSessionV1 schema for the V2 API.\\ntracers.schemas.TracerSessionBase\\nBase class for TracerSession.\\ntracers.schemas.TracerSessionV1\\nTracerSessionV1 schema.\\ntracers.schemas.TracerSessionV1Base\\nBase class for TracerSessionV1.\\ntracers.schemas.TracerSessionV1Create\\nCreate class for TracerSessionV1.\\ntracers.stdout.ConsoleCallbackHandler(**kwargs)\\nTracer that prints to the console.\\ntracers.stdout.FunctionCallbackHandler(...)\\nTracer that calls a function with a single str parameter.\\nFunctions¶\\ntracers.context.collect_runs()\\nCollect all run traces in context.\\ntracers.context.register_configure_hook(...)\\ntracers.context.tracing_enabled([session_name])\\nGet the Deprecated LangChainTracer in a context manager.\\ntracers.context.tracing_v2_enabled([...])\\nInstruct LangChain to log all runs in context to LangSmith.\\ntracers.evaluation.wait_for_all_evaluators()\\nWait for all tracers to finish.\\ntracers.langchain.get_client()\\nGet the client.\\ntracers.langchain.log_error_once(method,\\xa0...)\\nLog an error once.\\ntracers.langchain.wait_for_all_tracers()\\nWait for all tracers to finish.\\ntracers.langchain_v1.get_headers()\\nGet the headers for the LangChain API.\\ntracers.schemas.RunTypeEnum()\\nRunTypeEnum.\\ntracers.stdout.elapsed(run)\\nGet the elapsed time of a run.\\ntracers.stdout.try_json_stringify(obj,\\xa0fallback)\\nTry to stringify an object to JSON.\\nlangchain_core.utils¶\\nUtility functions for LangChain.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-13',\n",
       "  'text': 'langchain_core.utils¶\\nUtility functions for LangChain.\\nThese functions do not depend on any other LangChain module.\\nClasses¶\\nutils.aiter.NoLock()\\nDummy lock that provides the proper interface but no protection\\nutils.aiter.Tee(iterable[,\\xa0n,\\xa0lock])\\nCreate n separate asynchronous iterators over iterable\\nutils.aiter.atee\\nalias of Tee\\nutils.formatting.StrictFormatter()\\nA subclass of formatter that checks for extra keys.\\nutils.iter.NoLock()\\nDummy lock that provides the proper interface but no protection\\nutils.iter.Tee(iterable[,\\xa0n,\\xa0lock])\\nCreate n separate asynchronous iterators over iterable\\nutils.iter.safetee\\nalias of Tee\\nFunctions¶\\nutils.aiter.py_anext(iterator[,\\xa0default])\\nPure-Python implementation of anext() for testing purposes.\\nutils.aiter.tee_peer(iterator,\\xa0buffer,\\xa0...)\\nAn individual iterator of a tee()\\nutils.env.env_var_is_set(env_var)\\nCheck if an environment variable is set.\\nutils.input.get_bolded_text(text)\\nGet bolded text.\\nutils.input.get_color_mapping(items[,\\xa0...])\\nGet mapping for items to a support color.\\nutils.input.get_colored_text(text,\\xa0color)\\nGet colored text.\\nutils.input.print_text(text[,\\xa0color,\\xa0end,\\xa0file])\\nPrint text with highlighting and no end characters.\\nutils.iter.batch_iterate(size,\\xa0iterable)\\nUtility batching function.\\nutils.iter.tee_peer(iterator,\\xa0buffer,\\xa0peers,\\xa0...)\\nAn individual iterator of a tee()\\nutils.loading.try_load_from_hub(path,\\xa0...)\\nLoad configuration from hub.\\nutils.pydantic.get_pydantic_major_version()\\nGet the major version of Pydantic.\\nutils.utils.build_extra_kwargs(extra_kwargs,\\xa0...)',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'},\n",
       " {'id': '894c068cfc85-14',\n",
       "  'text': 'utils.utils.build_extra_kwargs(extra_kwargs,\\xa0...)\\nBuild extra kwargs from values and extra_kwargs.\\nutils.utils.check_package_version(package[,\\xa0...])\\nCheck the version of a package.\\nutils.utils.convert_to_secret_str(value)\\nConvert a string to a SecretStr if needed.\\nutils.utils.get_pydantic_field_names(...)\\nGet field names, including aliases, for a pydantic class.\\nutils.utils.guard_import(module_name,\\xa0*[,\\xa0...])\\nDynamically imports a module and raises a helpful exception if the module is not installed.\\nutils.utils.mock_now(dt_value)\\nContext manager for mocking out datetime.now() in unit tests.\\nutils.utils.raise_for_status_with_text(response)\\nRaise an error with the response text.\\nutils.utils.xor_args(*arg_groups)\\nValidate specified keyword args are mutually exclusive.\\nlangchain_core.vectorstores¶\\nClasses¶\\nvectorstores.VectorStore()\\nInterface for vector store.\\nvectorstores.VectorStoreRetriever\\nBase Retriever class for VectorStore.',\n",
       "  'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        'id': f'{uid}-{i}',\n",
    "        'text': chunk,\n",
    "        'source': url\n",
    "    } for i, chunk in enumerate(chunks)\n",
    "]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the same logic across our full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fccfd6df7d54c60aa5ae9f2d1e382bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "15019"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "documents = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    url = doc.metadata['source'].replace('rtdocs/', 'https://')\n",
    "    m.update(url.encode('utf-8'))\n",
    "    uid = m.hexdigest()[:12]\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            'id': f'{uid}-{i}',\n",
    "            'text': chunk,\n",
    "            'source': url\n",
    "        })\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now left with `15019` documents. We can save them to a JSON lines (`.jsonl`) file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train.jsonl', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data from file we'd write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15019"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        documents.append(json.loads(line))\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f160502deb25-0',\n",
       " 'text': 'langchain_core API Reference¶\\nlangchain_core.agents¶\\nClasses¶\\nagents.AgentAction\\nA full description of an action for an ActionAgent to execute.\\nagents.AgentActionMessageLog\\nOverride init to support instantiation by position for backward compat.\\nagents.AgentFinish\\nThe final return value of an ActionAgent.\\nlangchain_core.caches¶\\nClasses¶\\ncaches.BaseCache()\\nBase interface for cache.\\nlangchain_core.callbacks¶\\nClasses¶\\ncallbacks.base.AsyncCallbackHandler()\\nAsync callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackHandler()\\nBase callback handler that handles callbacks from LangChain.\\ncallbacks.base.BaseCallbackManager(handlers)\\nBase callback manager that handles callbacks from LangChain.\\ncallbacks.base.CallbackManagerMixin()\\nMixin for callback manager.\\ncallbacks.base.ChainManagerMixin()\\nMixin for chain callbacks.\\ncallbacks.base.LLMManagerMixin()\\nMixin for LLM callbacks.\\ncallbacks.base.RetrieverManagerMixin()\\nMixin for Retriever callbacks.\\ncallbacks.base.RunManagerMixin()\\nMixin for run manager.\\ncallbacks.base.ToolManagerMixin()\\nMixin for tool callbacks.\\ncallbacks.manager.AsyncCallbackManager(handlers)\\nAsync callback manager that handles callbacks from LangChain.\\ncallbacks.manager.AsyncCallbackManagerForChainGroup(...)\\nAsync callback manager for the chain group.\\ncallbacks.manager.AsyncCallbackManagerForChainRun(*,\\xa0...)\\nAsync callback manager for chain run.\\ncallbacks.manager.AsyncCallbackManagerForLLMRun(*,\\xa0...)\\nAsync callback manager for LLM run.\\ncallbacks.manager.AsyncCallbackManagerForRetrieverRun(*,\\xa0...)\\nAsync callback manager for retriever run.\\ncallbacks.manager.AsyncCallbackManagerForToolRun(*,\\xa0...)\\nAsync callback manager for tool run.\\ncallbacks.manager.AsyncParentRunManager(*,\\xa0...)\\nAsync Parent Run Manager.\\ncallbacks.manager.AsyncRunManager(*,\\xa0run_id,\\xa0...)',\n",
       " 'source': 'https://api.python.langchain.com/en/latest/core_api_reference.html'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Sharing the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created our dataset and you can go ahead and use it in any way you like. However, if you'd like to share the dataset, or store it somewhere that you can get easy access to later — we can use [Hugging Face Datasets Hub](https://huggingface.co/datasets).\n",
    "\n",
    "To begin we first need to create an account by clicking the **Sign Up** button at [huggingface.co](https://huggingface.co/). Once done we click our profile button in the same location > click **New Dataset** > give it a name like *\"langchain-docs\"* > set the dataset to **Public** or **Private** > click **Create dataset**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
