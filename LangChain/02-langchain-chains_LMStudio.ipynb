{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monday, January 29, 2024\n",
        "\n",
        "Trying this again ... but with the conda environment 'langchain2'\n",
        "\n",
        "1) mamba install conda-forge::numexpr ... NOPE! Tons of conflicts!\n",
        "1) pip install numexpr ... no warnings, it just downloads numexpr version 2.9.0 \n",
        "\n",
        "This all runs. \n",
        "\n",
        "### Friday, January 26, 2024\n",
        "\n",
        "Trying this notebook using the LMStudio server from the 'langchain' conda environment.\n",
        "\n",
        "Nice! This all runs in one pass!\n",
        "\n",
        "### Monday, November 27, 2023\n",
        "\n",
        "Re-running this notebook ...\n",
        "\n",
        "Start : $1.65\n",
        "\n",
        "### Monday, November 20, 2023\n",
        "\n",
        "[LLM Chains using GPT 3.5 and other LLMs â€” LangChain #3](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZR3iGJJtdDE",
        "outputId": "9ebb9f66-add2-4567-e37e-05323073e26b"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU langchain openai   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a4ba72d",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/02-langchain-chains.ipynb)\n",
        "\n",
        "#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n",
        "\n",
        "# Getting Started with Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain â€” a `PromptTemplate` â€” to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain â€” a LLM.\n",
        "\n",
        "We'll start by importing all the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import re\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "v86cmyppxdfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v86cmyppxdfc",
        "outputId": "a897108c-be81-49b8-8802-d71741243e43"
      },
      "outputs": [],
      "source": [
        "# Yeah, this should be done in any notebook that uses OpenAI\n",
        "# OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd0fa6ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "baaa74b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# llm = OpenAI(\n",
        "#     temperature=0, \n",
        "#     openai_api_key=OPENAI_API_KEY\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1dfd723a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rob/miniforge3/envs/langchain2/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"NULL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e50c62dd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# what is the default temperature?\n",
        "llm.temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3baa74a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set it to what is expected for this example.\n",
        "llm.temperature = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    \n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What are chains anyway?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
        "\n",
        "The official definition of chains is the following:\n",
        "\n",
        "\n",
        "> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n",
        "\n",
        "\n",
        "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to a text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4644b2f",
      "metadata": {
        "id": "c4644b2f"
      },
      "source": [
        "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
        "\n",
        "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
        "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d283b6",
      "metadata": {
        "id": "e4d283b6"
      },
      "source": [
        "Let's take a peek into what these chains have to offer!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831827b7",
      "metadata": {
        "id": "831827b7"
      },
      "source": [
        "### Utility Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c66e4b4",
      "metadata": {
        "id": "6c66e4b4"
      },
      "source": [
        "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HF3XCWD2sVi0",
      "metadata": {
        "id": "HF3XCWD2sVi0"
      },
      "source": [
        "#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc92fb04",
      "metadata": {},
      "source": [
        "#### numexpr side note\n",
        "\n",
        "(The first time I run this, I get an error about the missing library 'numexpr', which of course means the LangChain LLMMathChain utility chain uses this library. However, when I then ran 'mamba install conda-forge::numexpr, I got back a ton of library conflicts, and so nothing was installed. So maybe an earlier release will work?? Looking at the [git repo](https://github.com/pydata/numexpr) it looks like version 2.8.7 from Sept 26, 2023 is worth trying so I ran 'mamba install conda-forge::numexpr==2.8.7' but NOPE! still a ton of conflicts!... Hmm so is there a way I can find out which version of this library should work?? Wait ... do I need to use = instead of == ? ... hmm nope same conflicts. Fuck it ... gonna run 'pip install numexpr' )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da4a0fa4",
      "metadata": {},
      "source": [
        "NumExpr is a fast numerical expression evaluator for NumPy. With it, expressions that operate on arrays (like '3*a+4*b') are accelerated and use less memory than doing the same calculation in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57f4dcd",
      "metadata": {},
      "source": [
        "I grabbed the following numexpr code from their [github repo](https://github.com/pydata/numexpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "44788ea2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numexpr as ne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9b3ac015",
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.arange(1e6)\n",
        "b = np.arange(1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d3cafaf1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.00000e+00, 2.00000e+00, 3.00000e+00, ..., 9.99998e+05,\n",
              "       9.99999e+05, 1.00000e+06])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ne.evaluate(\"a + 1\")  # a simple expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "37c455dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False, False, False, ...,  True,  True,  True])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ne.evaluate('a*b-4.1*a > 2.5*b') # a more complex expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "51a8c210",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([        nan,  1.72284457,  1.79067101, ...,  1.09567006,\n",
              "        0.17523598, -0.09597844])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ne.evaluate(\"sin(a) + arcsinh(a/b)\")   # you can also use functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d308483d",
      "metadata": {},
      "outputs": [],
      "source": [
        "s = np.array([b'abba', b'abbb', b'abbcdef'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "22c9897e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ True, False, False])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ne.evaluate(\"b'abba' == s\")   # string arrays are supported too"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5f8932",
      "metadata": {},
      "source": [
        " Notice how we express the expression as a string ... according to the [numexpr docs](https://numexpr.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e2834ecd",
      "metadata": {},
      "outputs": [],
      "source": [
        "numericalExpression = \"13**0.3432\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "92d4216a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(2.41160046)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ne.evaluate(numericalExpression)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97fae5ff",
      "metadata": {},
      "source": [
        "So very interesting! The next cell works if we use 'mistral instruct v0.2', but fails with 'nexusraven v2', which is a bigger model. Bigger is not always better, right?!\n",
        "\n",
        "It also fails with 'phi2'\n",
        "\n",
        "And this is why I love LMStudio. I can switch out models on the fly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "b4161561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4161561",
        "outputId": "16486831-80a5-41df-906e-376575b7f514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rob/miniforge3/envs/langchain2/lib/python3.11/site-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m```text\n",
            "13**0.3432\n",
            "```\n",
            "...numexpr.evaluate(\"13**0.3432\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Spent a total of 70 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Answer: 2.4116004626599237'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
        "# here we express the expression in more natural language ...\n",
        "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198eebb2",
      "metadata": {
        "id": "198eebb2"
      },
      "source": [
        "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a7a0821a",
      "metadata": {
        "id": "a7a0821a"
      },
      "source": [
        "**Enter prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86c5798",
      "metadata": {
        "id": "c86c5798"
      },
      "source": [
        "The question we send as input to the chain is not the only input that the llm recieves ðŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "62778ef4",
      "metadata": {
        "id": "62778ef4",
        "outputId": "211670a8-db56-4f68-d873-97d279870890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(llm_math.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708031d8",
      "metadata": {
        "id": "708031d8"
      },
      "source": [
        "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ðŸ§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "66b92768",
      "metadata": {
        "id": "66b92768",
        "outputId": "6c9b7f59-529d-409e-8562-5a622f326473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 408 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nThe answer is approximately 2.0857.\\n\\nTo calculate this value, you can use the following formula:\\n\\n13^(1/3.432) = 2.085695...\\n\\nSo, 13 raised to the power of .3432 is approximately equal to 2.0857.'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we set the prompt to only have the question we ask\n",
        "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# we ask the llm for the answer with no context\n",
        "\n",
        "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d147e7bf",
      "metadata": {
        "id": "d147e7bf"
      },
      "source": [
        "Wrong answer! Herein lies the power of prompting and one of our most important insights so far: \n",
        "\n",
        "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd2a31f",
      "metadata": {
        "id": "1cd2a31f"
      },
      "source": [
        "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "3488c5b6",
      "metadata": {
        "id": "3488c5b6",
        "outputId": "8b32a998-8e11-48bf-f21c-f5d086186508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, str],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        _run_manager.on_text(inputs[self.input_key])\n",
            "        llm_output = self.llm_chain.predict(\n",
            "            question=inputs[self.input_key],\n",
            "            stop=[\"```output\"],\n",
            "            callbacks=_run_manager.get_child(),\n",
            "        )\n",
            "        return self._process_llm_result(llm_output, _run_manager)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The inspect module provides several useful functions to help get information about live objects \n",
        "# such as modules, classes, methods, functions, tracebacks, frame objects, and code objects. \n",
        "print(inspect.getsource(llm_math._call))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa6b6c2e",
      "metadata": {
        "id": "fa6b6c2e"
      },
      "source": [
        "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator (Read-Eval-Print Loop). We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "67f96bd3",
      "metadata": {
        "id": "67f96bd3"
      },
      "source": [
        "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b109619a",
      "metadata": {
        "id": "b109619a"
      },
      "source": [
        "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly ðŸ”¥\n",
        "\n",
        "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381e329c",
      "metadata": {
        "id": "381e329c"
      },
      "source": [
        "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f66a25a2",
      "metadata": {
        "id": "f66a25a2"
      },
      "source": [
        "### Generic chains"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "70b32a84",
      "metadata": {
        "id": "70b32a84"
      },
      "source": [
        "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4b8e2048",
      "metadata": {
        "id": "4b8e2048"
      },
      "source": [
        "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ðŸ˜‰"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e778d2",
      "metadata": {
        "id": "a6e778d2"
      },
      "source": [
        "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "c794e00a",
      "metadata": {
        "id": "c794e00a"
      },
      "outputs": [],
      "source": [
        "def transform_func(inputs: dict) -> dict:\n",
        "    text = inputs[\"text\"]\n",
        "    \n",
        "    # replace multiple new lines and multiple spaces with a single one\n",
        "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    return {\"output_text\": text}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc1ac6",
      "metadata": {
        "id": "42dc1ac6"
      },
      "source": [
        "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "286f7295",
      "metadata": {
        "id": "286f7295"
      },
      "outputs": [],
      "source": [
        "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "977bf11a",
      "metadata": {
        "id": "977bf11a",
        "outputId": "8d6eaa5e-b417-4c17-a345-7b7e32071430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A random text with some irregular spacing.\\n Another one here as well.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f84cd0",
      "metadata": {
        "id": "b3f84cd0"
      },
      "source": [
        "Great! Now things will get interesting.\n",
        "\n",
        "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b77042a",
      "metadata": {
        "id": "5b77042a"
      },
      "source": [
        "First we will build the prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "73719a5d",
      "metadata": {
        "id": "73719a5d"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Paraphrase this text:\n",
        "\n",
        "{output_text}\n",
        "\n",
        "In the style of a {style}.\n",
        "\n",
        "Paraphrase: \"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b2ec83",
      "metadata": {
        "id": "83b2ec83"
      },
      "source": [
        "And next, initialize our chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "48a067ab",
      "metadata": {
        "id": "48a067ab"
      },
      "outputs": [],
      "source": [
        "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2324005d",
      "metadata": {
        "id": "2324005d"
      },
      "source": [
        "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
        "\n",
        "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5da4925",
      "metadata": {
        "id": "c5da4925"
      },
      "source": [
        "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "06f51f17",
      "metadata": {
        "id": "06f51f17"
      },
      "outputs": [],
      "source": [
        "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0f51d8",
      "metadata": {
        "id": "7f0f51d8"
      },
      "source": [
        "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "a8032489",
      "metadata": {
        "id": "a8032489"
      },
      "outputs": [],
      "source": [
        "input_text = \"\"\"\n",
        "Chains allow us to combine multiple \n",
        "\n",
        "\n",
        "components together to create a single, coherent application. \n",
        "\n",
        "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
        "\n",
        "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
        "\n",
        "\n",
        "combining chains with other components.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f55d21",
      "metadata": {
        "id": "b2f55d21"
      },
      "source": [
        "We are all set. Time to get creative!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "d507aa5c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 607 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Yo, check it out, we got these chains, man,\\nLinkin' up components like a master plan,\\nTake user input, put it through a template,\\nPromptTemplate makin' it look so neat and simple,\\nThen pass that formatted response to an LLM,\\nLet the language learnin' magic begin, it's a sight to see,\\nBut we ain't stoppin' there, we buildin' up the complexity,\\nLinkin' chains together, makin' 'em all intertwine,\\nOr mix and match with other components,\\nCreatin' applications that'll make your heart sing.\""
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf935ef7",
      "metadata": {},
      "source": [
        "(I did not run the next few cells because I don't want to use OpenAI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b52e19",
      "metadata": {
        "id": "60b52e19"
      },
      "source": [
        "## A note on langchain-hub"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "02f649da",
      "metadata": {
        "id": "02f649da"
      },
      "source": [
        "`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "411500c2",
      "metadata": {
        "id": "411500c2"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import load_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "f25d8bea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This needs to run for the load_chain command in the next cell to work ...\n",
        "# actually I think it doesn't matter ...\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-asdf' # replace any existing key with this bogus key ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b375e5b7",
      "metadata": {
        "id": "b375e5b7"
      },
      "source": [
        "Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "fbe8748d",
      "metadata": {
        "id": "fbe8748d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rob/miniforge3/envs/langchain2/lib/python3.11/site-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "8ac93664",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: Dict[str, str],\n",
            "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
            "    ) -> Dict[str, str]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        _run_manager.on_text(inputs[self.input_key])\n",
            "        llm_output = self.llm_chain.predict(\n",
            "            question=inputs[self.input_key],\n",
            "            stop=[\"```output\"],\n",
            "            callbacks=_run_manager.get_child(),\n",
            "        )\n",
            "        return self._process_llm_result(llm_output, _run_manager)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's inspect this object ... it should be the same as above.\n",
        "print(inspect.getsource(llm_math_chain._call))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdce231c",
      "metadata": {},
      "source": [
        "Go here [LangChain Hub](https://smith.langchain.com/hub) for a list of all available chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcfe67c",
      "metadata": {
        "id": "ebcfe67c"
      },
      "source": [
        "What if we want to change some of the configuration parameters? We can simply override it after loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "d0d54233",
      "metadata": {
        "id": "d0d54233",
        "outputId": "92eba1cf-e47b-4df1-cc51-caee5a6a720b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math_chain.verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "074f8806",
      "metadata": {
        "id": "074f8806"
      },
      "outputs": [],
      "source": [
        "llm_math_chain.verbose = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "465a6cbf",
      "metadata": {
        "id": "465a6cbf",
        "outputId": "0207bf08-0db0-4d85-e3b9-ac7922f344c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math_chain.verbose"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2cc688ca",
      "metadata": {},
      "source": [
        "That's it for this example on chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
