{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuesday, November 14, 2023\n",
        "\n",
        "https://www.youtube.com/watch?v=wgYctKFnQ74\n",
        "\n",
        "So with a few minor tweaks I got this to work! Nice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi6wMNmzAaiw"
      },
      "source": [
        "Thanks to the starter code - https://huggingface.co/spaces/PyaeSoneK/chatchat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MbWNtdEKLqHg",
        "outputId": "d751013f-6f4a-44f5-f4e4-867bfd10a6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Nov 14 11:07:22 2023       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1050        On  | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   55C    P0              N/A /  70W |    505MiB /  2048MiB |     22%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:02:00.0 Off |                  Off |\n",
            "|  0%   44C    P8              20W / 450W |     14MiB / 24564MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwzLtkSXFvQX"
      },
      "source": [
        "Install the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYub11znAeQ8",
        "outputId": "68f722c6-9abe-4a62-e05f-d712c8cacb04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -q langchain transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XWMKWyOFwg6"
      },
      "source": [
        "Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wAcBfyNqAlVS"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import json\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3bcq1wiFx5b"
      },
      "source": [
        "Download the Model - We are using NousResearch's Llama2 which is the same as Meta AI's Llama 2, the only difference being \"**Not requiring authentication to download**\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WNWelmL0EjJy"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "be979aacf17e413992108af9ef82cf65",
            "8daa93ba43f04069a1b3de6a77d0dd21",
            "4e8f39f1ab27497e8b82b78e41c5c8e3",
            "b584006e6a3742e69b5a12d4c95a88ae",
            "d006d39733984b5b96bd16d5c508de72",
            "70c8a6ecdce74d658b7553638e0ed1d7",
            "a19610f50ca045e4b8d285b269d93cf7",
            "e9877126758449c584aaf0e46230a021",
            "98401c668d824f8081545861c30de45c",
            "393e807067264c70874b792010d0569c",
            "371fa072dcb248729aafc5cf0bbb010e"
          ]
        },
        "id": "1DuWPxoSBwNi",
        "outputId": "a7f36624-0dee-4eee-9b00-158c9d715dfa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ffc524440e947be8dcc1e93c10494cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "#                                              device_map='auto',\n",
        "#                                              torch_dtype=torch.float16,\n",
        "#                                              load_in_4bit=True,\n",
        "#                                              bnb_4bit_quant_type=\"nf4\",\n",
        "#                                              bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             load_in_4bit=True,\n",
        "                                             bnb_4bit_quant_type=\"nf4\",\n",
        "                                             bnb_4bit_compute_dtype=torch.float16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaHwoa3mF-0M"
      },
      "source": [
        "Define Transformers Pipeline which will be fed into Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KIFZa7nOCNwE"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=30,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4do0vmyXGCkr"
      },
      "source": [
        "Define the Prompt format for Llama 2 - This might change if you have a different model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JRkwKvRZCVDa"
      },
      "outputs": [],
      "source": [
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<>\\n\", \"\\n<>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an advanced Life guru and mental health expert that excels at giving advice.\n",
        "Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.Just say you don't know and you are sorry!\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqtCjkgGJnu"
      },
      "source": [
        "All the helper fucntions to generate prompt, prompt template, clean up output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6eh4CTDDCRX2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT, citation=None):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "\n",
        "    if citation:\n",
        "        prompt_template += f\"\\n\\nCitation: {citation}\"  # Insert citation here\n",
        "\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "def generate(text, citation=None):\n",
        "    prompt = get_prompt(text, citation=citation)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs,\n",
        "                                 max_length=512,\n",
        "                                 eos_token_id=tokenizer.eos_token_id,\n",
        "                                 pad_token_id=tokenizer.eos_token_id,\n",
        "                                 )\n",
        "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        final_outputs = cut_off_text(final_outputs, '')\n",
        "        final_outputs = remove_substring(final_outputs, prompt)\n",
        "\n",
        "    return final_outputs\n",
        "\n",
        "def parse_text(text):\n",
        "    wrapped_text = textwrap.fill(text, width=100)\n",
        "    print(wrapped_text + '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L87UBaSpGUXP"
      },
      "source": [
        "Defining Langchain LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f1O8mRhGCZ9u"
      },
      "outputs": [],
      "source": [
        "\n",
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.7,'max_length': 256, 'top_k' :50})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXyDe9H2GaHO",
        "outputId": "8ba63e9f-7bb1-404a-b256-1d811ae2da2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]<>\n",
            "You are an advanced Life guru and mental health expert that excels at giving advice. \n",
            "<>\n",
            "\n",
            "Convert the following input text from a stupid human to a well-reasoned and step-by-step throughout advice:\n",
            "\n",
            " {text}[/INST]\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are an advanced Life guru and mental health expert that excels at giving advice. \"\n",
        "instruction = \"Convert the following input text from a stupid human to a well-reasoned and step-by-step throughout advice:\\n\\n {text}\"\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IDzKD9wVDMG_"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5LWT207HDVy3"
      },
      "outputs": [],
      "source": [
        "text = \"My life sucks, what do you suggest? Please don't tell me to medidate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1f7c8bCDTA6",
        "outputId": "c6e1b4c1-f506-456a-838f-d110defcc03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Ah, I see. Well, my dear, let me tell you something – life can indeed be challenging at times, but that's exactly why we need to take a step back and reassess our priorities. Meditation is a wonderful tool, but it's not the only solution to your problems. Here's a well-reasoned and step-by-step approach to help you navigate your current situation:\n",
            "\n",
            "Step 1: Identify the Root of the Problem\n",
            "\n",
            "* Take a moment to reflect on what's causing your dissatisfaction. Is it a toxic relationship? A lack of purpose or meaning? Unresolved emotional trauma? Once you've identified the root of the problem, you can start working on a solution.\n",
            "\n",
            "Step 2: Seek Professional Help\n",
            "\n",
            "* Sometimes, we need outside help to overcome our challenges. Consider consulting a therapist or counselor who can provide you with emotional support and guidance. They can help you develop coping strategies and work through any underlying issues that may be contributing to your unhappiness.\n",
            "\n",
            "Step 3: Practice Self-Care\n",
            "\n",
            "* Taking care of yourself is crucial when dealing with life's challenges. Make sure you're getting enough sleep, eating a balanced diet, and engaging in regular exercise. Exercise can help reduce stress and anxiety, and it's a great way to boost your mood and energy levels.\n",
            "\n",
            "Step 4: Set Realistic Goals\n",
            "\n",
            "* Setting achievable goals can help you feel a sense of purpose and direction. Break down your goals into smaller, manageable tasks, and focus on making progress one step at a time. Celebrate your successes along the way, no matter how small they may seem.\n",
            "\n",
            "Step 5: Connect with Others\n",
            "\n",
            "* Sometimes, we just need someone to talk to. Reach out to a trusted friend or family member, or consider joining a support group. Connecting with others can help you feel less alone and more supported in your journey towards happiness.\n",
            "\n",
            "Step 6: Practice Gratitude\n",
            "\n",
            "* Focusing on what you're grateful for can help shift your perspective and improve your mood. Take a few minutes each day to reflect on the things you're thankful for, no matter how small they may seem.\n",
            "\n",
            "Step\n"
          ]
        }
      ],
      "source": [
        "response = llm_chain.run(text)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYbqC2eNDTzO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "371fa072dcb248729aafc5cf0bbb010e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "393e807067264c70874b792010d0569c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8f39f1ab27497e8b82b78e41c5c8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9877126758449c584aaf0e46230a021",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98401c668d824f8081545861c30de45c",
            "value": 2
          }
        },
        "70c8a6ecdce74d658b7553638e0ed1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8daa93ba43f04069a1b3de6a77d0dd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c8a6ecdce74d658b7553638e0ed1d7",
            "placeholder": "​",
            "style": "IPY_MODEL_a19610f50ca045e4b8d285b269d93cf7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "98401c668d824f8081545861c30de45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a19610f50ca045e4b8d285b269d93cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b584006e6a3742e69b5a12d4c95a88ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_393e807067264c70874b792010d0569c",
            "placeholder": "​",
            "style": "IPY_MODEL_371fa072dcb248729aafc5cf0bbb010e",
            "value": " 2/2 [00:59&lt;00:00, 27.13s/it]"
          }
        },
        "be979aacf17e413992108af9ef82cf65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8daa93ba43f04069a1b3de6a77d0dd21",
              "IPY_MODEL_4e8f39f1ab27497e8b82b78e41c5c8e3",
              "IPY_MODEL_b584006e6a3742e69b5a12d4c95a88ae"
            ],
            "layout": "IPY_MODEL_d006d39733984b5b96bd16d5c508de72"
          }
        },
        "d006d39733984b5b96bd16d5c508de72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9877126758449c584aaf0e46230a021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
