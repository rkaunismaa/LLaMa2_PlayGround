{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sunday, December 17, 2023\n",
    "\n",
    "[Tutorial: Text-To-Image Search Pipeline with Multimodal Retriever](https://haystack.deepset.ai/tutorials/19_text_to_image_search_pipeline_with_multimodal_retriever)\n",
    "\n",
    "This has been tweaked to run against the match images.\n",
    "\n",
    "Wow! I am stunned this actually worked!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CQyfa3akfIEZ"
   },
   "source": [
    "# Text-To-Image Search Pipeline with Multimodal Retriever\n",
    "\n",
    "**Level**: Intermediate\n",
    "\n",
    "**Time to complete**: 20 minutes\n",
    "\n",
    "**Prerequisites**: This tutorial assumes basic knowledge of Haystack Retrievers and Pipelines. If you want to learn about them, have a look at our tutorials on [Build Your First QA System](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/01_Basic_QA_Pipeline.ipynb) and [Fine-Tuning a Model on Your Own Data](https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/02_Finetune_a_model_on_your_data.ipynb).\n",
    "\n",
    "Prepare the Colab environment (see links below).\n",
    "\n",
    "**Nodes Used**: InMemoryDocumentStore, MultiModalRetriever\n",
    "\n",
    "**Goal**: After completing this tutorial, you will have built a search system that retrieves images as answers to a text query.\n",
    "\n",
    "**Description**: In this tutorial, you'll download a set of images that you'll then turn into embeddings using a transformers model, OpenAI CLIP. You'll then use the same model to embed the text query. Finally, you'll perform a nearest neighbor search to retrieve the images relevant to the text query.\n",
    "\n",
    "Let's build a text-to-image search pipeline using a small animal dataset!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fzn2uA1Be1Km"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tJU29jj0fX5m"
   },
   "source": [
    "## Installing Haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hl92D-ZlycPh"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install --upgrade pip\n",
    "# pip install farm-haystack[colab,inference]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Telemetry \n",
    "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running(19)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KkVAG7FdXsEU"
   },
   "source": [
    "## Initializing the DocumentStore\n",
    "\n",
    "A DocumentStore stores references to the images that Haystack will compare with your query. But before it can do that, you need to initialize it. In this tutorial, you'll use the InMemoryDocumentStore.\n",
    "\n",
    "If you want to learn more, see [DocumentStore](https://docs.haystack.deepset.ai/docs/document_store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dK86aFlSYQXv"
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "# Here Here we initialize the DocumentStore to store 512 dim image embeddings\n",
    "# obtained using OpenAI CLIP model\n",
    "document_store = InMemoryDocumentStore(embedding_dim=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oGNwBu0yYcDq"
   },
   "source": [
    "## Downloading Data\n",
    "\n",
    "Download 18 sample images of different animals and store it. You can find them in data/tutorial19/spirit-animals/ as a set of .jpg files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7yk_Prp3yYUa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "# doc_dir = \"data/tutorial19\"\n",
    "\n",
    "# fetch_archive_from_http(\n",
    "#     url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/spirit-animals.zip\",\n",
    "#     output_dir=doc_dir,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from haystack import Document\n",
    "\n",
    "# images = [\n",
    "#     Document(content=f\"./{doc_dir}/spirit-animals/{filename}\", content_type=\"image\")\n",
    "#     for filename in os.listdir(f\"./{doc_dir}/spirit-animals/\")\n",
    "# ]\n",
    "\n",
    "# document_store.write_documents(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imageFolder = \"/tf/Data/Documents/Github/rkaunismaa/NLP4HTML/express/express-match/public/images\"\n",
    "\n",
    "imageFiles = []\n",
    "for root, dirs, files in os.walk(imageFolder):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')):\n",
    "            imageFiles.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15755"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imageFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "\n",
    "imageDocuments = []\n",
    "for image in imageFiles:\n",
    "    doc = Document(content=image, content_type=\"image\")\n",
    "    imageDocuments.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(imageDocuments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oOJC6m8cqzCl"
   },
   "source": [
    "Add the images you just downloaded into Haystack Document objects and write them into the DocumentStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3pdDsSVp40vr"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from haystack import Document\n",
    "\n",
    "# images = [\n",
    "#     Document(content=f\"./{doc_dir}/spirit-animals/{filename}\", content_type=\"image\")\n",
    "#     for filename in os.listdir(f\"./{doc_dir}/spirit-animals/\")\n",
    "# ]\n",
    "\n",
    "# document_store.write_documents(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_U-RlPJWHMjO"
   },
   "source": [
    "You have successfully stored your images in the DocumentStore.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nfmeRIE9wz9o"
   },
   "source": [
    "## Initializing the Retriever\n",
    "\n",
    "Retrievers sift through all the images and return only those that are relevant to the query. To run a search on images, you'll use the MultiModalRetriever with the [OpenAI CLIP model](https://github.com/openai/CLIP/blob/main/model-card.md). \n",
    "\n",
    "For more details on supported modalities, see [MultiModalRetriever](https://docs.haystack.deepset.ai/docs/retriever#multimodal-retrieval).\n",
    "\n",
    "Before adding the Retriever to your pipeline, let's configure its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xuL1mtq6qx0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes.retriever.multimodal import MultiModalRetriever\n",
    "\n",
    "retriever_text_to_image = MultiModalRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"sentence-transformers/clip-ViT-B-32\",\n",
    "    query_type=\"text\",\n",
    "    document_embedding_models={\"image\": \"sentence-transformers/clip-ViT-B-32\"},\n",
    ")\n",
    "\n",
    "# # Now let's turn our images into embeddings and store them in the DocumentStore.\n",
    "# document_store.update_embeddings(retriever=retriever_text_to_image)\n",
    "\n",
    "# just embed\n",
    "# 4.0s\n",
    "\n",
    "# Download and embed\n",
    "# 10m 1.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 20000 docs [03:40, 90.58 docs/s]                        \n"
     ]
    }
   ],
   "source": [
    "# Now let's turn our images into embeddings and store them in the DocumentStore.\n",
    "document_store.update_embeddings(retriever=retriever_text_to_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mcopKii2MBCd"
   },
   "source": [
    " Your retriever is now ready for search!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vMNYvDjd9sqY"
   },
   "source": [
    "## Creating the MultiModal Search Pipeline\n",
    "\n",
    "We are populating a pipeline with a MultiModalRetriever node. This search pipeline queries the image database with text and returns the most relevant images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-a6ltABP40vs"
   },
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_node(component=retriever_text_to_image, name=\"retriever_text_to_image\", inputs=[\"Query\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wbEkh5oSMJbq"
   },
   "source": [
    "Now, you have a pipeline that uses the MultiModalRetriever and takes a text query as input. Let's try it out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TSjizWzAF6T9"
   },
   "source": [
    "## Searching Through the Images\n",
    "\n",
    "Use the pipeline `run()` method to query the images in the DocumentStore. The query argument is where you type your text query. Additionally, you can set the number of images you want the MultiModalRetriever to return using the `top-k` parameter. To learn more about setting arguments, see [Pipeline Arguments](https://docs.haystack.deepset.ai/docs/pipelines#arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRH5UbIdF7CW"
   },
   "outputs": [],
   "source": [
    "# results = pipeline.run(query=\"Animal that lives in the water\", params={\"retriever_text_to_image\": {\"top_k\": 3}})\n",
    "matchQuery = \"An attractive, young blonde woman with blue eyes and a smile\"\n",
    "# matchQuery = \"A bicycle\"\n",
    "\n",
    "results = pipeline.run(query=matchQuery, params={\"retriever_text_to_image\": {\"top_k\": 50}})\n",
    "\n",
    "# Sort the results based on the scores\n",
    "results = sorted(results[\"documents\"], key=lambda d: d.score, reverse=True)\n",
    "\n",
    "for doc in results:\n",
    "    print(doc.score, doc.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FoEt2cHHTdIZ"
   },
   "source": [
    "Here are some more query strings you could try out:\n",
    "\n",
    "1.   King of the Jungle\n",
    "2.   Fastest animal\n",
    "3.   Bird that can see clearly even in the dark\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KBukVUVVU0if"
   },
   "source": [
    "You can also easily vizualize these images together with their score using this code:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qSjZHuv68Hut",
    "outputId": "0cb0d794-3dc2-4034-e0c9-277ca318f23f"
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "\n",
    "def display_img_array(ima, score):\n",
    "    im = Image.open(ima)\n",
    "    img_with_border = ImageOps.expand(im, border=20, fill=\"white\")\n",
    "\n",
    "    # Add Text to an image\n",
    "    img = ImageDraw.Draw(img_with_border)\n",
    "    img.text((20, 0), f\"Score: {score},    Path: {ima}\", fill=(0, 0, 0))\n",
    "\n",
    "    bio = BytesIO()\n",
    "    img_with_border.save(bio, format=\"png\")\n",
    "    display(IPImage(bio.getvalue(), format=\"png\"))\n",
    "\n",
    "\n",
    "images_array = [doc.content for doc in results]\n",
    "scores = [doc.score for doc in results]\n",
    "for ima, score in zip(images_array, scores):\n",
    "    display_img_array(ima, score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
