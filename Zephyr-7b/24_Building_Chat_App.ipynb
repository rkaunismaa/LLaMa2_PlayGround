{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, December 16, 2023\n",
    "\n",
    "This notebook does not work because the HuggingFace Inference API is not working.\n",
    "\n",
    "#### Friday, December 15, 2023\n",
    "\n",
    "[Tutorial: Building a Conversational Chat App](https://haystack.deepset.ai/tutorials/24_building_chat_app)\n",
    "\n",
    "https://colab.research.google.com/github/deepset-ai/haystack-tutorials/blob/main/tutorials/24_Building_Chat_App.ipynb\n",
    "\n",
    "This is my first venture into working with Haystack!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEebQqubxa4G"
   },
   "source": [
    "# Tutorial: Building a Conversational Chat App\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 10 minutes\n",
    "- **Nodes Used**: `PromptNode`, `ConversationalAgent` and `ConversationSummaryMemory`\n",
    "- **Goal**: After completing this tutorial, you will have learned how to use ConversationalAgent to build a conversational chat application\n",
    "- **Prerequisites**: A [Hugging Face API Key](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQLWWW1Yy_Ta"
   },
   "source": [
    "## Overview\n",
    "\n",
    "A [ConversationalAgent](https://docs.haystack.deepset.ai/docs/agent#conversational-agent) is a type of Agent that is specifically implemented to create chat applications easily. With its memory integration, the ConversationalAgent enables human-like conversation with large language models (LLMs).\n",
    "\n",
    "This tutorial introduces you to the ConversationalAgent, ConversationSummaryMemory and explains how you can create your conversational chat application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obaSuZBHy8PF"
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_nC2XoPzDVh"
   },
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiZktTKoaHp5"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install --upgrade pip\n",
    "# pip install farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrPgO_1vzWb6"
   },
   "source": [
    "\n",
    "### Enabling Telemetry\n",
    "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product, but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6YZjKAvOzZRq"
   },
   "outputs": [],
   "source": [
    "from haystack.telemetry import tutorial_running\n",
    "\n",
    "tutorial_running(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0pnO7S6tbyW"
   },
   "source": [
    "## Initializing the ConversationalAgent\n",
    "\n",
    "To initialize a ConversationalAgent, you'll first need to create a PromptNode to define the LLM that your chat application will use. Then, you'll add a memory to enable the application to store previous conversation and use this memory to make the interaction more human-like.\n",
    "\n",
    "Now, create necessary components for a ConversationalAgent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Omji8PK_675"
   },
   "source": [
    "### 1) Provide a Hugging Face API Key\n",
    "\n",
    "Hugging Face offers [a hosted Inference API](https://huggingface.co/docs/api-inference/index) which you can use to access machine learning models using simple HTTP requests. This way, you don't need to download models from the hub. To use the service, you need to provide an [API key](https://huggingface.co/settings/tokens) from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qWuRxFWGcAL4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "model_api_key = os.getenv(\"HF_API_KEY\", None) or getpass(\"Enter HF API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4LI3vzH7Xvo"
   },
   "source": [
    "### 2) Create a PromptNode\n",
    "\n",
    "You'll initialize a PromptNode with the `model_name_or_path`, `api_key`, and `max_length` to control the output length of the model. In this tutorial, you'll use [HuggingFaceH4/zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), an open source chat Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RckAPBT3bSoh"
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=\"HuggingFaceH4/zephyr-7b-beta\", api_key=model_api_key, max_length=256, stop_words=[\"Human\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1e15LLV8ULF"
   },
   "source": [
    "### 3) Create a ConversationSummaryMemory\n",
    "\n",
    "To have a chat application closer to a human interaction, you need to provide [memory](https://docs.haystack.deepset.ai/docs/agent#conversational-agent-memory) to the ConversationalAgent. There are two types of memory options in Haystack:\n",
    "\n",
    "1.   **ConversationMemory**: stores the conversation history (default).\n",
    "2.   **ConversationSummaryMemory**: stores the conversation history and periodically generates summaries.\n",
    "\n",
    "These memory nodes inject the conversation history into the prompt for the large language model with every run. Instead of using the full conversation history, you'll use ConversationSummaryMemory that sums up the conversation without losing important information, thus saving the token limit.\n",
    "\n",
    "You can use the same PromptNode in ConversationSummaryMemory, so the same `HuggingFaceH4/zephyr-7b-beta` model generates chat summaries. By default, ConversationSummaryMemory summarizes the chat with every `3` runs using the predefined [`conversational-summary`](https://prompthub.deepset.ai/?prompt=deepset%2Fconversational-summary) PromptTemplate on PromptHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iBisS_dI8kan"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/prompt_template.py:444: UserWarning: You're using a legacy prompt template 'conversational-summary', we strongly suggest you use prompts from the official Haystack PromptHub: https://prompthub.deepset.ai/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.agents.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(prompt_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpeKtIsSSNSh"
   },
   "source": [
    "> Optionally, you can define a separate PromptNode with another LLM and PromptTemplate for generating conversation summary and use it in the ConversationSummaryMemory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiGGpDW98XjM"
   },
   "source": [
    "### 4) Create a ConversationalAgent\n",
    "\n",
    "Now that you have all the necessary components, you can initialize the ConversationalAgent. If you don't provide any tools, the ConversationalAgent uses the [`conversational-agent-without-tools`](https://prompthub.deepset.ai/?prompt=deepset%2Fconversational-agent-without-tools) prompt by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gt2DqNzllPQ",
    "outputId": "bbbb2aba-d524-4d32-d081-432e43c9b26a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConversationalAgent is created without tools\n",
      "/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/prompt_template.py:444: UserWarning: You're using a legacy prompt template 'conversational-agent-without-tools', we strongly suggest you use prompts from the official Haystack PromptHub: https://prompthub.deepset.ai/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.agents.conversational import ConversationalAgent\n",
    "\n",
    "conversational_agent = ConversationalAgent(prompt_node=prompt_node, memory=summary_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brdbkCGryKe0"
   },
   "source": [
    "> You can add tools to your chat application using `tools` params of the ConversationalAgent:\n",
    "> ```python\n",
    "> conversational_agent = ConversationalAgent(\n",
    ">    prompt_node=prompt_node,\n",
    ">    memory=summary_memory,\n",
    ">    tools=[search_tool]\n",
    ">)\n",
    ">```\n",
    ">To learn how to create tools, check out [Haystack documentation](https://docs.haystack.deepset.ai/docs/agent#tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mgeb6nlsGRTy"
   },
   "source": [
    "Now, your conversational agent is ready to chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq_yBlEXGe18"
   },
   "source": [
    "## Trying Out a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gAi6DN-LySIH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent conversational-agent-without-tools started with {'query': 'Tell me three most interesting things about Istanbul, Turkey', 'params': None}\n"
     ]
    },
    {
     "ename": "HuggingFaceInferenceError",
     "evalue": "HuggingFace Inference returned an error.\nStatus code: 503\nResponse body: {\"error\":\"Model HuggingFaceH4/zephyr-7b-beta is currently loading\",\"estimated_time\":579.3385620117188}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:237\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_codes_to_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattempts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/utils/requests_utils.py:93\u001b[0m, in \u001b[0;36mrequest_with_retry\u001b[0;34m(attempts, status_codes_to_retry, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# We raise here too in case the request failed with a status code that\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# won't trigger a retry, this way the call will still cause an explicit exception\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversational_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me three most interesting things about Istanbul, Turkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/agents/base.py:363\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, query, max_steps, params)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m agent_step\u001b[38;5;241m.\u001b[39mis_last():\n\u001b[0;32m--> 363\u001b[0m         agent_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_agent_finish(agent_step)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/agents/base.py:372\u001b[0m, in \u001b[0;36mAgent._step\u001b[0;34m(self, query, current_step, params)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, current_step: AgentStep, params: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# plan next step using the LLM\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     prompt_node_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# from the LLM response, create the next step\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     next_step \u001b[38;5;241m=\u001b[39m current_step\u001b[38;5;241m.\u001b[39mcreate_next_step(prompt_node_response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/agents/base.py:397\u001b[0m, in \u001b[0;36mAgent._plan\u001b[0;34m(self, query, current_step)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_prompt_template(template_params)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# invoke via prompt node\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m prompt_node_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAgentTokenStreamingHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_node_response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/prompt_node.py:169\u001b[0m, in \u001b[0;36mPromptNode.prompt\u001b[0;34m(self, prompt_template, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     prompt_collector\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m    168\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt being sent to LLM with prompt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prompt, kwargs_copy)\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(output)\n\u001b[1;32m    172\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_collector\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/prompt_model.py:129\u001b[0m, in \u001b[0;36mPromptModel.invoke\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Takes in a prompt and returns a list of responses using the underlying invocation layer.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :return: A list of model-generated responses for the prompt or prompts.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_invocation_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:173\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer.invoke\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# see https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\u001b[39;00m\n\u001b[1;32m    155\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs_with_defaults\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatermark\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    172\u001b[0m }\n\u001b[0;32m--> 173\u001b[0m response: requests\u001b[38;5;241m.\u001b[39mResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    177\u001b[0m     handler: TokenStreamingHandler \u001b[38;5;241m=\u001b[39m kwargs_with_defaults\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, DefaultTokenStreamingHandler())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/haystack/nodes/prompt/invocation_layer/hugging_face_inference.py:254\u001b[0m, in \u001b[0;36mHFInferenceEndpointInvocationLayer._post\u001b[0;34m(self, data, stream, attempts, status_codes_to_retry, timeout)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceUnauthorizedError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI key is invalid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingFaceInferenceError(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFace Inference returned an error.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStatus code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mstatus_code,  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     )\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mHuggingFaceInferenceError\u001b[0m: HuggingFace Inference returned an error.\nStatus code: 503\nResponse body: {\"error\":\"Model HuggingFaceH4/zephyr-7b-beta is currently loading\",\"estimated_time\":579.3385620117188}"
     ]
    }
   ],
   "source": [
    "conversational_agent.run(\"Tell me three most interesting things about Istanbul, Turkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBwcAcbgMTQN"
   },
   "outputs": [],
   "source": [
    "conversational_agent.run(\"Can you elaborate on the second item?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wl8dcWpMmNv"
   },
   "outputs": [],
   "source": [
    "conversational_agent.run(\"Can you turn this info into a twitter thread?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhrNe3xKIVsx"
   },
   "source": [
    "* At any point during the chat, you can use `load()` function to check the chat summary:\n",
    "\n",
    "```python\n",
    "print(conversational_agent.memory.load())\n",
    "```\n",
    "\n",
    "* To delete the whole chat history, call `clear()` method:\n",
    "\n",
    "```python\n",
    "conversational_agent.memory.clear()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGu3QLQXJo-z"
   },
   "source": [
    "Congratulations! ðŸŽ‰ You've learned how to use ConversationalAgent to create a chat application with a summarized memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQyYPjmAG2gJ"
   },
   "source": [
    "## ðŸ’¬ Example Application\n",
    "\n",
    "To take the chat experience to another level, check out this example application. Run the code cell below and use the textarea to interact with the conversational agent. Use the buttons on the right to load or delete the chat summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEcpPCLKKasg"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "## Text Input\n",
    "user_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Type your prompt here\",\n",
    "    disabled=False,\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100%\"),\n",
    ")\n",
    "\n",
    "## Submit Button\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Submit\", button_style=\"success\", layout=widgets.Layout(width=\"100%\", height=\"80%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    user_prompt = user_input.value\n",
    "    user_input.value = \"\"\n",
    "    print(\"\\nUser:\\n\", user_prompt)\n",
    "    conversational_agent.run(user_prompt)\n",
    "\n",
    "\n",
    "submit_button.on_click(on_button_clicked)\n",
    "\n",
    "## Show Memory Button\n",
    "memory_button = widgets.Button(\n",
    "    description=\"Show Memory\", button_style=\"info\", layout=widgets.Layout(width=\"100%\", height=\"100%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_memory_button_clicked(b):\n",
    "    memory = conversational_agent.memory.load()\n",
    "    if len(memory):\n",
    "        print(\"\\nMemory:\\n\", memory)\n",
    "    else:\n",
    "        print(\"Memory is empty\")\n",
    "\n",
    "\n",
    "memory_button.on_click(on_memory_button_clicked)\n",
    "\n",
    "## Clear Memory Button\n",
    "clear_button = widgets.Button(\n",
    "    description=\"Clear Memory\", button_style=\"warning\", layout=widgets.Layout(width=\"100%\", height=\"100%\")\n",
    ")\n",
    "\n",
    "\n",
    "def on_clear_button_button_clicked(b):\n",
    "    conversational_agent.memory.clear()\n",
    "    print(\"\\nMemory is cleared\\n\")\n",
    "\n",
    "\n",
    "clear_button.on_click(on_clear_button_button_clicked)\n",
    "\n",
    "## Layout\n",
    "grid = widgets.GridspecLayout(3, 3, height=\"200px\", width=\"800px\", grid_gap=\"10px\")\n",
    "grid[0, 2] = clear_button\n",
    "grid[0:2, 0:2] = user_input\n",
    "grid[2, 0:] = submit_button\n",
    "grid[1, 2] = memory_button\n",
    "display(grid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
