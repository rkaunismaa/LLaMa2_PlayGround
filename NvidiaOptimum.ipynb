{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday, December 6, 2023\n",
    "\n",
    "Looking into the following HuggingFace blog article ...\n",
    "\n",
    "[Optimum-NVIDIA - Unlock blazingly fast LLM inference in just 1 line of code](https://huggingface.co/blog/optimum-nvidia)\n",
    "\n",
    "https://github.com/huggingface/optimum-nvidia\n",
    "\n",
    "Dammit ... no pip package yet and \"docker pull huggingface/optimum-nvidia\" fails ... guess I need to wait a bit before I can give this a go ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum.nvidia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/NvidiaOptimum.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f686670745f4f63743238227d/tf/All/Data/Documents/Github/rkaunismaa/LLaMa2_PlayGround/NvidiaOptimum.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnvidia\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimum.nvidia'"
     ]
    }
   ],
   "source": [
    "from optimum.nvidia.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Llama 2 is here - get it on Hugging Face](https://huggingface.co/blog/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "model = \"TheBloke/Llama-2-13b-Chat-GPTQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--TheBloke--Python-Code-33B-GPTQ\n",
      "models--facebook--blenderbot-1B-distill\n",
      "models--google--flan-t5-large\n",
      "models--gpt2-medium\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "tmpcjh0h7gn\n",
      "tmpfcrmwgx2\n",
      "tmpzafytbf_\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker cp /home/rob/Data3/huggingface/transformers/models--meta-llama--Llama-2-13b-chat-hf c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 26GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "\n",
    "# docker cp /home/rob/Data3/huggingface/transformers/models--TheBloke--Llama-2-13b-Chat-GPTQ c9b676310ea0://home/rob/Data2/huggingface/transformers\n",
    "# Successfully copied 7.26GB to c9b676310ea0://home/rob/Data2/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--TheBloke--Llama-2-13B-chat-GPTQ\n",
    "# !rm -rf /home/rob/Data2/huggingface/transformers/models--TheBloke--Llama-2-13b-Chat-GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--TheBloke--CodeLlama-34B-Instruct-GPTQ\n",
      "models--TheBloke--Llama-2-13b-Chat-GPTQ\n",
      "models--TheBloke--Python-Code-33B-GPTQ\n",
      "models--facebook--blenderbot-1B-distill\n",
      "models--google--flan-t5-large\n",
      "models--gpt2-medium\n",
      "models--meta-llama--Llama-2-13b-chat-hf\n",
      "models--meta-llama--Llama-2-13b-hf\n",
      "tmp0m77pc0x\n",
      "tmpcjh0h7gn\n",
      "tmpfcrmwgx2\n",
      "tmpzafytbf_\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data2/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have activated exllama backend. Note that you can get better inference\n",
      "                    speed using exllamav2 kernel by setting `use_exllama_v2=True`.`disable_exllama` will be deprecated\n",
      "                    in future version.\n"
     ]
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# 9.4s\n",
    "# 12936 MiB VRAM\n",
    "\n",
    "# \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "# 6.9s\n",
    "# 8610 MiB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/transformers/src/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "\n",
    "# \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "# 6.0s\n",
    "\n",
    "# \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# 7m 0.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the output from every model for comparison purposes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "  * Yes, I can definitely recommend some shows that you might enjoy! Based on your interest in \"Breaking Bad\" and \"Band of Brothers\", I would suggest checking out the following shows:\n",
      "\n",
      "1. \"The Sopranos\" - This HBO series is a crime drama that explores the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
      "2. \"The Wire\" - This HBO series is a gritty and intense drama that explores the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians.\n",
      "3. \"Narcos\" - This Netflix series tells the true story of Pablo Escobar, the infam\n"
     ]
    }
   ],
   "source": [
    "# \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "I understand that you have different tastes and preferences, but I can provide some recommendations based on the shows you've enjoyed. Here are a few suggestions:\n",
      "\n",
      "1. \"The Sopranos\" - This critically acclaimed series is about a New Jersey mob boss, Tony Soprano, and his family. It explores themes of crime, family, and psychology.\n",
      "2. \"The Wire\" - This gritty HBO series is set in Baltimore and explores the drug trade and its impact on the city and its residents. It features a talented ensemble cast and is known for its complex characters and storylines.\n",
      "3. \"Narcos\" - This Netflix series tells the true story of Pablo Escobar and the Medell√≠n cartel, and expl\n"
     ]
    }
   ],
   "source": [
    "# \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
